{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import pickle\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.set_random_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP = 23\n",
    "LSTM_SIZE = 10\n",
    "N_INPUTS = 3\n",
    "N_OUTPUTS = 1\n",
    "# BATCH_SIZE = 20\n",
    "\n",
    "learning_rate = 0.01\n",
    "iterations = 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = np.load('./dataX.npy')\n",
    "target_batch = np.load('./dataY.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerfile = 'scalerY.sav'\n",
    "scalerY = pickle.load(open(scalerfile, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810\n",
      "203\n"
     ]
    }
   ],
   "source": [
    "# train/test split\n",
    "train_size = int(len(target_batch) * 0.8)\n",
    "test_size = len(target_batch) - train_size\n",
    "\n",
    "trainX, testX = np.array(input_batch[0:train_size]), np.array(\n",
    "    input_batch[train_size:len(input_batch)])\n",
    "trainY, testY = np.array(target_batch[0:train_size]), np.array(\n",
    "    target_batch[train_size:len(target_batch)])\n",
    "\n",
    "print(len(trainX))\n",
    "print(len(testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, TIME_STEP, N_INPUTS])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "lstm_cell = rnn.BasicLSTMCell(LSTM_SIZE, forget_bias=1.0)\n",
    "outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "outputs = outputs[:, -1]\n",
    "\n",
    "weight = tf.Variable(tf.random_normal([LSTM_SIZE, N_OUTPUTS]))\n",
    "bias = tf.Variable(tf.random_normal([N_OUTPUTS]))\n",
    "Y_pred = tf.matmul(outputs, weight) + bias\n",
    "\n",
    "loss = tf.losses.mean_squared_error(Y, Y_pred)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 0.04363582655787468\n",
      "[step: 1] loss: 0.02515999786555767\n",
      "[step: 2] loss: 0.01976954936981201\n",
      "[step: 3] loss: 0.022735049948096275\n",
      "[step: 4] loss: 0.021629946306347847\n",
      "[step: 5] loss: 0.017047526314854622\n",
      "[step: 6] loss: 0.013776522129774094\n",
      "[step: 7] loss: 0.013876388780772686\n",
      "[step: 8] loss: 0.01539591420441866\n",
      "[step: 9] loss: 0.015409722924232483\n",
      "[step: 10] loss: 0.014225848950445652\n",
      "[step: 11] loss: 0.013387506827712059\n",
      "[step: 12] loss: 0.013546936213970184\n",
      "[step: 13] loss: 0.014013308100402355\n",
      "[step: 14] loss: 0.013952262699604034\n",
      "[step: 15] loss: 0.013368991203606129\n",
      "[step: 16] loss: 0.012757358141243458\n",
      "[step: 17] loss: 0.012465005740523338\n",
      "[step: 18] loss: 0.012507550418376923\n",
      "[step: 19] loss: 0.012704415246844292\n",
      "[step: 20] loss: 0.012863206677138805\n",
      "[step: 21] loss: 0.012885565869510174\n",
      "[step: 22] loss: 0.012782578356564045\n",
      "[step: 23] loss: 0.01263255625963211\n",
      "[step: 24] loss: 0.012519956566393375\n",
      "[step: 25] loss: 0.012485917657613754\n",
      "[step: 26] loss: 0.01251060701906681\n",
      "[step: 27] loss: 0.012534200213849545\n",
      "[step: 28] loss: 0.012502718716859818\n",
      "[step: 29] loss: 0.012407300062477589\n",
      "[step: 30] loss: 0.01228870078921318\n",
      "[step: 31] loss: 0.012205208651721478\n",
      "[step: 32] loss: 0.012189343571662903\n",
      "[step: 33] loss: 0.012225503101944923\n",
      "[step: 34] loss: 0.012264585122466087\n",
      "[step: 35] loss: 0.012263884767889977\n",
      "[step: 36] loss: 0.012219666503369808\n",
      "[step: 37] loss: 0.012165057472884655\n",
      "[step: 38] loss: 0.012136494740843773\n",
      "[step: 39] loss: 0.012140458449721336\n",
      "[step: 40] loss: 0.012151064351201057\n",
      "[step: 41] loss: 0.012138409540057182\n",
      "[step: 42] loss: 0.012097565457224846\n",
      "[step: 43] loss: 0.012050286866724491\n",
      "[step: 44] loss: 0.012021781876683235\n",
      "[step: 45] loss: 0.012018048204481602\n",
      "[step: 46] loss: 0.012024176307022572\n",
      "[step: 47] loss: 0.012021275237202644\n",
      "[step: 48] loss: 0.012003728188574314\n",
      "[step: 49] loss: 0.011981337331235409\n",
      "[step: 50] loss: 0.011967034079134464\n",
      "[step: 51] loss: 0.01196370180696249\n",
      "[step: 52] loss: 0.011962546035647392\n",
      "[step: 53] loss: 0.011953243054449558\n",
      "[step: 54] loss: 0.011934800073504448\n",
      "[step: 55] loss: 0.011916080489754677\n",
      "[step: 56] loss: 0.011906026862561703\n",
      "[step: 57] loss: 0.011904357001185417\n",
      "[step: 58] loss: 0.011902835220098495\n",
      "[step: 59] loss: 0.011895325966179371\n",
      "[step: 60] loss: 0.011884578503668308\n",
      "[step: 61] loss: 0.011877666227519512\n",
      "[step: 62] loss: 0.011876499280333519\n",
      "[step: 63] loss: 0.011875798925757408\n",
      "[step: 64] loss: 0.011870463378727436\n",
      "[step: 65] loss: 0.011862010695040226\n",
      "[step: 66] loss: 0.011855844408273697\n",
      "[step: 67] loss: 0.011853947304189205\n",
      "[step: 68] loss: 0.01185302622616291\n",
      "[step: 69] loss: 0.011849812231957912\n",
      "[step: 70] loss: 0.011845401488244534\n",
      "[step: 71] loss: 0.01184296514838934\n",
      "[step: 72] loss: 0.011842920444905758\n",
      "[step: 73] loss: 0.011842605657875538\n",
      "[step: 74] loss: 0.011840233579277992\n",
      "[step: 75] loss: 0.011837145313620567\n",
      "[step: 76] loss: 0.0118354931473732\n",
      "[step: 77] loss: 0.011835134588181973\n",
      "[step: 78] loss: 0.01183423399925232\n",
      "[step: 79] loss: 0.011832265183329582\n",
      "[step: 80] loss: 0.011830644682049751\n",
      "[step: 81] loss: 0.011830274946987629\n",
      "[step: 82] loss: 0.011830175295472145\n",
      "[step: 83] loss: 0.01182917132973671\n",
      "[step: 84] loss: 0.011827651411294937\n",
      "[step: 85] loss: 0.011826658621430397\n",
      "[step: 86] loss: 0.011826143600046635\n",
      "[step: 87] loss: 0.01182525884360075\n",
      "[step: 88] loss: 0.011823889799416065\n",
      "[step: 89] loss: 0.011822748929262161\n",
      "[step: 90] loss: 0.011822124011814594\n",
      "[step: 91] loss: 0.011821472086012363\n",
      "[step: 92] loss: 0.011820410378277302\n",
      "[step: 93] loss: 0.011819275096058846\n",
      "[step: 94] loss: 0.01181840430945158\n",
      "[step: 95] loss: 0.011817597784101963\n",
      "[step: 96] loss: 0.011816531419754028\n",
      "[step: 97] loss: 0.011815342120826244\n",
      "[step: 98] loss: 0.011814344674348831\n",
      "[step: 99] loss: 0.011813502758741379\n",
      "[step: 100] loss: 0.01181255467236042\n",
      "[step: 101] loss: 0.011811480857431889\n",
      "[step: 102] loss: 0.011810495518147945\n",
      "[step: 103] loss: 0.011809613555669785\n",
      "[step: 104] loss: 0.011808663606643677\n",
      "[step: 105] loss: 0.011807614006102085\n",
      "[step: 106] loss: 0.011806599795818329\n",
      "[step: 107] loss: 0.011805688962340355\n",
      "[step: 108] loss: 0.011804763227701187\n",
      "[step: 109] loss: 0.011803784407675266\n",
      "[step: 110] loss: 0.011802825145423412\n",
      "[step: 111] loss: 0.011801937595009804\n",
      "[step: 112] loss: 0.0118010388687253\n",
      "[step: 113] loss: 0.011800093576312065\n",
      "[step: 114] loss: 0.011799152940511703\n",
      "[step: 115] loss: 0.011798249557614326\n",
      "[step: 116] loss: 0.01179735828191042\n",
      "[step: 117] loss: 0.011796439997851849\n",
      "[step: 118] loss: 0.011795531027019024\n",
      "[step: 119] loss: 0.011794650927186012\n",
      "[step: 120] loss: 0.011793779209256172\n",
      "[step: 121] loss: 0.011792883276939392\n",
      "[step: 122] loss: 0.011791987344622612\n",
      "[step: 123] loss: 0.011791114695370197\n",
      "[step: 124] loss: 0.011790241114795208\n",
      "[step: 125] loss: 0.011789361014962196\n",
      "[step: 126] loss: 0.011788478121161461\n",
      "[step: 127] loss: 0.011787616647779942\n",
      "[step: 128] loss: 0.011786753311753273\n",
      "[step: 129] loss: 0.011785885319113731\n",
      "[step: 130] loss: 0.011785016395151615\n",
      "[step: 131] loss: 0.011784160509705544\n",
      "[step: 132] loss: 0.011783299967646599\n",
      "[step: 133] loss: 0.011782439425587654\n",
      "[step: 134] loss: 0.011781577952206135\n",
      "[step: 135] loss: 0.011780728586018085\n",
      "[step: 136] loss: 0.011779878288507462\n",
      "[step: 137] loss: 0.011779030784964561\n",
      "[step: 138] loss: 0.011778182350099087\n",
      "[step: 139] loss: 0.01177733950316906\n",
      "[step: 140] loss: 0.011776499450206757\n",
      "[step: 141] loss: 0.01177565474063158\n",
      "[step: 142] loss: 0.01177481934428215\n",
      "[step: 143] loss: 0.011773986741900444\n",
      "[step: 144] loss: 0.011773154139518738\n",
      "[step: 145] loss: 0.011772321537137032\n",
      "[step: 146] loss: 0.011771495454013348\n",
      "[step: 147] loss: 0.011770671233534813\n",
      "[step: 148] loss: 0.011769848875701427\n",
      "[step: 149] loss: 0.011769031174480915\n",
      "[step: 150] loss: 0.011768211610615253\n",
      "[step: 151] loss: 0.011767394840717316\n",
      "[step: 152] loss: 0.011766581796109676\n",
      "[step: 153] loss: 0.011765772476792336\n",
      "[step: 154] loss: 0.011764968745410442\n",
      "[step: 155] loss: 0.011764162220060825\n",
      "[step: 156] loss: 0.011763359420001507\n",
      "[step: 157] loss: 0.011762553825974464\n",
      "[step: 158] loss: 0.011761755682528019\n",
      "[step: 159] loss: 0.011760961264371872\n",
      "[step: 160] loss: 0.011760164983570576\n",
      "[step: 161] loss: 0.011759374290704727\n",
      "[step: 162] loss: 0.011758589185774326\n",
      "[step: 163] loss: 0.011757800355553627\n",
      "[step: 164] loss: 0.011757012456655502\n",
      "[step: 165] loss: 0.011756230145692825\n",
      "[step: 166] loss: 0.011755452491343021\n",
      "[step: 167] loss: 0.011754673905670643\n",
      "[step: 168] loss: 0.011753899045288563\n",
      "[step: 169] loss: 0.011753123253583908\n",
      "[step: 170] loss: 0.01175235491245985\n",
      "[step: 171] loss: 0.011751585640013218\n",
      "[step: 172] loss: 0.01175081729888916\n",
      "[step: 173] loss: 0.0117500526830554\n",
      "[step: 174] loss: 0.011749288998544216\n",
      "[step: 175] loss: 0.011748528108000755\n",
      "[step: 176] loss: 0.011747769080102444\n",
      "[step: 177] loss: 0.011747011914849281\n",
      "[step: 178] loss: 0.011746259406208992\n",
      "[step: 179] loss: 0.011745505034923553\n",
      "[step: 180] loss: 0.011744757182896137\n",
      "[step: 181] loss: 0.011744008399546146\n",
      "[step: 182] loss: 0.01174326054751873\n",
      "[step: 183] loss: 0.011742514558136463\n",
      "[step: 184] loss: 0.011741775088012218\n",
      "[step: 185] loss: 0.01174103282392025\n",
      "[step: 186] loss: 0.011740295216441154\n",
      "[step: 187] loss: 0.01173955574631691\n",
      "[step: 188] loss: 0.011738821864128113\n",
      "[step: 189] loss: 0.01173809077590704\n",
      "[step: 190] loss: 0.011737360619008541\n",
      "[step: 191] loss: 0.011736632324755192\n",
      "[step: 192] loss: 0.011735906824469566\n",
      "[step: 193] loss: 0.011735180392861366\n",
      "[step: 194] loss: 0.011734453961253166\n",
      "[step: 195] loss: 0.011733733117580414\n",
      "[step: 196] loss: 0.011733012273907661\n",
      "[step: 197] loss: 0.011732292361557484\n",
      "[step: 198] loss: 0.011731576174497604\n",
      "[step: 199] loss: 0.011730862781405449\n",
      "[step: 200] loss: 0.011730149388313293\n",
      "[step: 201] loss: 0.011729439720511436\n",
      "[step: 202] loss: 0.011728726327419281\n",
      "[step: 203] loss: 0.011728019453585148\n",
      "[step: 204] loss: 0.01172731164842844\n",
      "[step: 205] loss: 0.011726606637239456\n",
      "[step: 206] loss: 0.01172590535134077\n",
      "[step: 207] loss: 0.01172520313411951\n",
      "[step: 208] loss: 0.011724499985575676\n",
      "[step: 209] loss: 0.011723799630999565\n",
      "[step: 210] loss: 0.011723104864358902\n",
      "[step: 211] loss: 0.011722407303750515\n",
      "[step: 212] loss: 0.011721712537109852\n",
      "[step: 213] loss: 0.011721016839146614\n",
      "[step: 214] loss: 0.011720324866473675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 215] loss: 0.011719636619091034\n",
      "[step: 216] loss: 0.01171894557774067\n",
      "[step: 217] loss: 0.011718260124325752\n",
      "[step: 218] loss: 0.011717570014297962\n",
      "[step: 219] loss: 0.01171688362956047\n",
      "[step: 220] loss: 0.011716198176145554\n",
      "[step: 221] loss: 0.01171551551669836\n",
      "[step: 222] loss: 0.011714835651218891\n",
      "[step: 223] loss: 0.011714154854416847\n",
      "[step: 224] loss: 0.011713477782905102\n",
      "[step: 225] loss: 0.011712799780070782\n",
      "[step: 226] loss: 0.011712119914591312\n",
      "[step: 227] loss: 0.011711444705724716\n",
      "[step: 228] loss: 0.01171077135950327\n",
      "[step: 229] loss: 0.011710097081959248\n",
      "[step: 230] loss: 0.0117094237357378\n",
      "[step: 231] loss: 0.011708753183484077\n",
      "[step: 232] loss: 0.01170808169990778\n",
      "[step: 233] loss: 0.011707412078976631\n",
      "[step: 234] loss: 0.011706741526722908\n",
      "[step: 235] loss: 0.011706072837114334\n",
      "[step: 236] loss: 0.011705406941473484\n",
      "[step: 237] loss: 0.011704741977155209\n",
      "[step: 238] loss: 0.011704077944159508\n",
      "[step: 239] loss: 0.011703410185873508\n",
      "[step: 240] loss: 0.011702747084200382\n",
      "[step: 241] loss: 0.01170208677649498\n",
      "[step: 242] loss: 0.011701422743499279\n",
      "[step: 243] loss: 0.011700763367116451\n",
      "[step: 244] loss: 0.01170009933412075\n",
      "[step: 245] loss: 0.011699439026415348\n",
      "[step: 246] loss: 0.01169877965003252\n",
      "[step: 247] loss: 0.011698121204972267\n",
      "[step: 248] loss: 0.011697462759912014\n",
      "[step: 249] loss: 0.01169680617749691\n",
      "[step: 250] loss: 0.011696148663759232\n",
      "[step: 251] loss: 0.011695494875311852\n",
      "[step: 252] loss: 0.011694836430251598\n",
      "[step: 253] loss: 0.011694181710481644\n",
      "[step: 254] loss: 0.011693529784679413\n",
      "[step: 255] loss: 0.011692873202264309\n",
      "[step: 256] loss: 0.01169221755117178\n",
      "[step: 257] loss: 0.011691565625369549\n",
      "[step: 258] loss: 0.011690913699567318\n",
      "[step: 259] loss: 0.011690259911119938\n",
      "[step: 260] loss: 0.011689606122672558\n",
      "[step: 261] loss: 0.0116889588534832\n",
      "[step: 262] loss: 0.01168830320239067\n",
      "[step: 263] loss: 0.011687654070556164\n",
      "[step: 264] loss: 0.011687003076076508\n",
      "[step: 265] loss: 0.011686351150274277\n",
      "[step: 266] loss: 0.011685701087117195\n",
      "[step: 267] loss: 0.011685050092637539\n",
      "[step: 268] loss: 0.011684400029480457\n",
      "[step: 269] loss: 0.011683749035000801\n",
      "[step: 270] loss: 0.01168309897184372\n",
      "[step: 271] loss: 0.011682448908686638\n",
      "[step: 272] loss: 0.01168180163949728\n",
      "[step: 273] loss: 0.011681150645017624\n",
      "[step: 274] loss: 0.011680500581860542\n",
      "[step: 275] loss: 0.011679849587380886\n",
      "[step: 276] loss: 0.011679200455546379\n",
      "[step: 277] loss: 0.011678552255034447\n",
      "[step: 278] loss: 0.011677899397909641\n",
      "[step: 279] loss: 0.011677250266075134\n",
      "[step: 280] loss: 0.011676600202918053\n",
      "[step: 281] loss: 0.011675945483148098\n",
      "[step: 282] loss: 0.01167529821395874\n",
      "[step: 283] loss: 0.011674648150801659\n",
      "[step: 284] loss: 0.011673995293676853\n",
      "[step: 285] loss: 0.011673340573906898\n",
      "[step: 286] loss: 0.01167269330471754\n",
      "[step: 287] loss: 0.01167204137891531\n",
      "[step: 288] loss: 0.011671386659145355\n",
      "[step: 289] loss: 0.01167073380202055\n",
      "[step: 290] loss: 0.011670080944895744\n",
      "[step: 291] loss: 0.01166942622512579\n",
      "[step: 292] loss: 0.011668774299323559\n",
      "[step: 293] loss: 0.011668117716908455\n",
      "[step: 294] loss: 0.011667462065815926\n",
      "[step: 295] loss: 0.011666806414723396\n",
      "[step: 296] loss: 0.011666150763630867\n",
      "[step: 297] loss: 0.011665494181215763\n",
      "[step: 298] loss: 0.01166483573615551\n",
      "[step: 299] loss: 0.011664178222417831\n",
      "[step: 300] loss: 0.011663518846035004\n",
      "[step: 301] loss: 0.011662861332297325\n",
      "[step: 302] loss: 0.011662198230624199\n",
      "[step: 303] loss: 0.011661538854241371\n",
      "[step: 304] loss: 0.011660875752568245\n",
      "[step: 305] loss: 0.011660214513540268\n",
      "[step: 306] loss: 0.011659547686576843\n",
      "[step: 307] loss: 0.011658882722258568\n",
      "[step: 308] loss: 0.011658217757940292\n",
      "[step: 309] loss: 0.011657550930976868\n",
      "[step: 310] loss: 0.011656885035336018\n",
      "[step: 311] loss: 0.011656212620437145\n",
      "[step: 312] loss: 0.011655543930828571\n",
      "[step: 313] loss: 0.011654872447252274\n",
      "[step: 314] loss: 0.011654200032353401\n",
      "[step: 315] loss: 0.011653528548777103\n",
      "[step: 316] loss: 0.011652854271233082\n",
      "[step: 317] loss: 0.011652176268398762\n",
      "[step: 318] loss: 0.01165150199085474\n",
      "[step: 319] loss: 0.011650819331407547\n",
      "[step: 320] loss: 0.011650139465928078\n",
      "[step: 321] loss: 0.011649456806480885\n",
      "[step: 322] loss: 0.011648774147033691\n",
      "[step: 323] loss: 0.011648089624941349\n",
      "[step: 324] loss: 0.011647406034171581\n",
      "[step: 325] loss: 0.011646716855466366\n",
      "[step: 326] loss: 0.011646026745438576\n",
      "[step: 327] loss: 0.011645336635410786\n",
      "[step: 328] loss: 0.011644644662737846\n",
      "[step: 329] loss: 0.011643953621387482\n",
      "[step: 330] loss: 0.011643254198133945\n",
      "[step: 331] loss: 0.011642557568848133\n",
      "[step: 332] loss: 0.011641855351626873\n",
      "[step: 333] loss: 0.011641154997050762\n",
      "[step: 334] loss: 0.011640449985861778\n",
      "[step: 335] loss: 0.011639745905995369\n",
      "[step: 336] loss: 0.011639039032161236\n",
      "[step: 337] loss: 0.011638329364359379\n",
      "[step: 338] loss: 0.0116376131772995\n",
      "[step: 339] loss: 0.011636902578175068\n",
      "[step: 340] loss: 0.011636186391115189\n",
      "[step: 341] loss: 0.011635467410087585\n",
      "[step: 342] loss: 0.011634745635092258\n",
      "[step: 343] loss: 0.011634021066129208\n",
      "[step: 344] loss: 0.011633297428488731\n",
      "[step: 345] loss: 0.011632568202912807\n",
      "[step: 346] loss: 0.01163183618336916\n",
      "[step: 347] loss: 0.011631104163825512\n",
      "[step: 348] loss: 0.011630366556346416\n",
      "[step: 349] loss: 0.011629628948867321\n",
      "[step: 350] loss: 0.011628885753452778\n",
      "[step: 351] loss: 0.01162814162671566\n",
      "[step: 352] loss: 0.011627394706010818\n",
      "[step: 353] loss: 0.011626645922660828\n",
      "[step: 354] loss: 0.011625890620052814\n",
      "[step: 355] loss: 0.011625134386122227\n",
      "[step: 356] loss: 0.011624375358223915\n",
      "[step: 357] loss: 0.01162361353635788\n",
      "[step: 358] loss: 0.011622847057878971\n",
      "[step: 359] loss: 0.011622077785432339\n",
      "[step: 360] loss: 0.011621305719017982\n",
      "[step: 361] loss: 0.011620530858635902\n",
      "[step: 362] loss: 0.0116197494789958\n",
      "[step: 363] loss: 0.011618965305387974\n",
      "[step: 364] loss: 0.011618176475167274\n",
      "[step: 365] loss: 0.011617385782301426\n",
      "[step: 366] loss: 0.01161658763885498\n",
      "[step: 367] loss: 0.011615792289376259\n",
      "[step: 368] loss: 0.01161498948931694\n",
      "[step: 369] loss: 0.011614184826612473\n",
      "[step: 370] loss: 0.011613369919359684\n",
      "[step: 371] loss: 0.011612556874752045\n",
      "[step: 372] loss: 0.011611739173531532\n",
      "[step: 373] loss: 0.011610913090407848\n",
      "[step: 374] loss: 0.011610083281993866\n",
      "[step: 375] loss: 0.011609249748289585\n",
      "[step: 376] loss: 0.011608412489295006\n",
      "[step: 377] loss: 0.011607569642364979\n",
      "[step: 378] loss: 0.011606721207499504\n",
      "[step: 379] loss: 0.011605869978666306\n",
      "[step: 380] loss: 0.011605014093220234\n",
      "[step: 381] loss: 0.01160414982587099\n",
      "[step: 382] loss: 0.0116032799705863\n",
      "[step: 383] loss: 0.011602410115301609\n",
      "[step: 384] loss: 0.011601529084146023\n",
      "[step: 385] loss: 0.011600644327700138\n",
      "[step: 386] loss: 0.011599753983318806\n",
      "[step: 387] loss: 0.011598857119679451\n",
      "[step: 388] loss: 0.011597955599427223\n",
      "[step: 389] loss: 0.01159704476594925\n",
      "[step: 390] loss: 0.011596128344535828\n",
      "[step: 391] loss: 0.011595208197832108\n",
      "[step: 392] loss: 0.011594281531870365\n",
      "[step: 393] loss: 0.011593344621360302\n",
      "[step: 394] loss: 0.01159240584820509\n",
      "[step: 395] loss: 0.011591456830501556\n",
      "[step: 396] loss: 0.011590499430894852\n",
      "[step: 397] loss: 0.0115895364433527\n",
      "[step: 398] loss: 0.0115885678678751\n",
      "[step: 399] loss: 0.01158758532255888\n",
      "[step: 400] loss: 0.011586600914597511\n",
      "[step: 401] loss: 0.011585602536797523\n",
      "[step: 402] loss: 0.011584604158997536\n",
      "[step: 403] loss: 0.011583592742681503\n",
      "[step: 404] loss: 0.011582574807107449\n",
      "[step: 405] loss: 0.011581548489630222\n",
      "[step: 406] loss: 0.011580508202314377\n",
      "[step: 407] loss: 0.011579465121030807\n",
      "[step: 408] loss: 0.011578409932553768\n",
      "[step: 409] loss: 0.011577344499528408\n",
      "[step: 410] loss: 0.011576268821954727\n",
      "[step: 411] loss: 0.01157518569380045\n",
      "[step: 412] loss: 0.011574088595807552\n",
      "[step: 413] loss: 0.011572984047234058\n",
      "[step: 414] loss: 0.01157186646014452\n",
      "[step: 415] loss: 0.011570737697184086\n",
      "[step: 416] loss: 0.011569599620997906\n",
      "[step: 417] loss: 0.01156845223158598\n",
      "[step: 418] loss: 0.011567286215722561\n",
      "[step: 419] loss: 0.011566112749278545\n",
      "[step: 420] loss: 0.011564924381673336\n",
      "[step: 421] loss: 0.011563725769519806\n",
      "[step: 422] loss: 0.011562513187527657\n",
      "[step: 423] loss: 0.011561285704374313\n",
      "[step: 424] loss: 0.011560044251382351\n",
      "[step: 425] loss: 0.011558787897229195\n",
      "[step: 426] loss: 0.011557516641914845\n",
      "[step: 427] loss: 0.01155623234808445\n",
      "[step: 428] loss: 0.011554930359125137\n",
      "[step: 429] loss: 0.01155361719429493\n",
      "[step: 430] loss: 0.011552284471690655\n",
      "[step: 431] loss: 0.011550933122634888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 432] loss: 0.011549565009772778\n",
      "[step: 433] loss: 0.011548181064426899\n",
      "[step: 434] loss: 0.011546777561306953\n",
      "[step: 435] loss: 0.011545354500412941\n",
      "[step: 436] loss: 0.011543911881744862\n",
      "[step: 437] loss: 0.01154244877398014\n",
      "[step: 438] loss: 0.011540965177118778\n",
      "[step: 439] loss: 0.011539464816451073\n",
      "[step: 440] loss: 0.011537935584783554\n",
      "[step: 441] loss: 0.011536385864019394\n",
      "[step: 442] loss: 0.011534812860190868\n",
      "[step: 443] loss: 0.011533215641975403\n",
      "[step: 444] loss: 0.011531593278050423\n",
      "[step: 445] loss: 0.011529947631061077\n",
      "[step: 446] loss: 0.01152826938778162\n",
      "[step: 447] loss: 0.011526566930115223\n",
      "[step: 448] loss: 0.011524834670126438\n",
      "[step: 449] loss: 0.011523072607815266\n",
      "[step: 450] loss: 0.01152127981185913\n",
      "[step: 451] loss: 0.011519454419612885\n",
      "[step: 452] loss: 0.01151759922504425\n",
      "[step: 453] loss: 0.011515705846250057\n",
      "[step: 454] loss: 0.011513780802488327\n",
      "[step: 455] loss: 0.011511814780533314\n",
      "[step: 456] loss: 0.011509811505675316\n",
      "[step: 457] loss: 0.011507769115269184\n",
      "[step: 458] loss: 0.011505684815347195\n",
      "[step: 459] loss: 0.011503556743264198\n",
      "[step: 460] loss: 0.011501383036375046\n",
      "[step: 461] loss: 0.011499160900712013\n",
      "[step: 462] loss: 0.011496894992887974\n",
      "[step: 463] loss: 0.011494574137032032\n",
      "[step: 464] loss: 0.011492199264466763\n",
      "[step: 465] loss: 0.01148977130651474\n",
      "[step: 466] loss: 0.011487284675240517\n",
      "[step: 467] loss: 0.011484737507998943\n",
      "[step: 468] loss: 0.011482130736112595\n",
      "[step: 469] loss: 0.011479449458420277\n",
      "[step: 470] loss: 0.011476708576083183\n",
      "[step: 471] loss: 0.011473935097455978\n",
      "[step: 472] loss: 0.011471539735794067\n",
      "[step: 473] loss: 0.011474783532321453\n",
      "[step: 474] loss: 0.011557396501302719\n",
      "[step: 475] loss: 0.012803973630070686\n",
      "[step: 476] loss: 0.023980166763067245\n",
      "[step: 477] loss: 0.014961483888328075\n",
      "[step: 478] loss: 0.017204925417900085\n",
      "[step: 479] loss: 0.01277373731136322\n",
      "[step: 480] loss: 0.013432778418064117\n",
      "[step: 481] loss: 0.014080354012548923\n",
      "[step: 482] loss: 0.012680567800998688\n",
      "[step: 483] loss: 0.01172033790498972\n",
      "[step: 484] loss: 0.012952541932463646\n",
      "[step: 485] loss: 0.013385096564888954\n",
      "[step: 486] loss: 0.01212287787348032\n",
      "[step: 487] loss: 0.011859636753797531\n",
      "[step: 488] loss: 0.012496840208768845\n",
      "[step: 489] loss: 0.01264120265841484\n",
      "[step: 490] loss: 0.012136030942201614\n",
      "[step: 491] loss: 0.01180208008736372\n",
      "[step: 492] loss: 0.012071681208908558\n",
      "[step: 493] loss: 0.01238177064806223\n",
      "[step: 494] loss: 0.012165519408881664\n",
      "[step: 495] loss: 0.011788485571742058\n",
      "[step: 496] loss: 0.011774267069995403\n",
      "[step: 497] loss: 0.011991781182587147\n",
      "[step: 498] loss: 0.01202149223536253\n",
      "[step: 499] loss: 0.011807282455265522\n",
      "[step: 500] loss: 0.011678919196128845\n",
      "[step: 501] loss: 0.011805186048150063\n",
      "[step: 502] loss: 0.011915148235857487\n",
      "[step: 503] loss: 0.011796705424785614\n",
      "[step: 504] loss: 0.011654592119157314\n",
      "[step: 505] loss: 0.011681980453431606\n",
      "[step: 506] loss: 0.011766795068979263\n",
      "[step: 507] loss: 0.011747590266168118\n",
      "[step: 508] loss: 0.011658985167741776\n",
      "[step: 509] loss: 0.011636207811534405\n",
      "[step: 510] loss: 0.011693837121129036\n",
      "[step: 511] loss: 0.01171752531081438\n",
      "[step: 512] loss: 0.011662408709526062\n",
      "[step: 513] loss: 0.011615106835961342\n",
      "[step: 514] loss: 0.011634448543190956\n",
      "[step: 515] loss: 0.011666747741401196\n",
      "[step: 516] loss: 0.011651091277599335\n",
      "[step: 517] loss: 0.011615280993282795\n",
      "[step: 518] loss: 0.011617161333560944\n",
      "[step: 519] loss: 0.011644061654806137\n",
      "[step: 520] loss: 0.011642396450042725\n",
      "[step: 521] loss: 0.011614812538027763\n",
      "[step: 522] loss: 0.01160613913089037\n",
      "[step: 523] loss: 0.011621559970080853\n",
      "[step: 524] loss: 0.01162775419652462\n",
      "[step: 525] loss: 0.011614270508289337\n",
      "[step: 526] loss: 0.011604754254221916\n",
      "[step: 527] loss: 0.01161216851323843\n",
      "[step: 528] loss: 0.01161915808916092\n",
      "[step: 529] loss: 0.01161150448024273\n",
      "[step: 530] loss: 0.011601309292018414\n",
      "[step: 531] loss: 0.011602705344557762\n",
      "[step: 532] loss: 0.011608653701841831\n",
      "[step: 533] loss: 0.011606569401919842\n",
      "[step: 534] loss: 0.011599717661738396\n",
      "[step: 535] loss: 0.011598717421293259\n",
      "[step: 536] loss: 0.011602441780269146\n",
      "[step: 537] loss: 0.011601834557950497\n",
      "[step: 538] loss: 0.011596618220210075\n",
      "[step: 539] loss: 0.011594285257160664\n",
      "[step: 540] loss: 0.011596296913921833\n",
      "[step: 541] loss: 0.011596977710723877\n",
      "[step: 542] loss: 0.011594139039516449\n",
      "[step: 543] loss: 0.011591816321015358\n",
      "[step: 544] loss: 0.011592422612011433\n",
      "[step: 545] loss: 0.01159301120787859\n",
      "[step: 546] loss: 0.011591154150664806\n",
      "[step: 547] loss: 0.011588940396904945\n",
      "[step: 548] loss: 0.011588751338422298\n",
      "[step: 549] loss: 0.01158928032964468\n",
      "[step: 550] loss: 0.011588376946747303\n",
      "[step: 551] loss: 0.011586696840822697\n",
      "[step: 552] loss: 0.011586061678826809\n",
      "[step: 553] loss: 0.011586172506213188\n",
      "[step: 554] loss: 0.011585499159991741\n",
      "[step: 555] loss: 0.011584117077291012\n",
      "[step: 556] loss: 0.011583308689296246\n",
      "[step: 557] loss: 0.011583194136619568\n",
      "[step: 558] loss: 0.011582748964428902\n",
      "[step: 559] loss: 0.01158170960843563\n",
      "[step: 560] loss: 0.01158084999769926\n",
      "[step: 561] loss: 0.011580475606024265\n",
      "[step: 562] loss: 0.011580005288124084\n",
      "[step: 563] loss: 0.011579113081097603\n",
      "[step: 564] loss: 0.011578270234167576\n",
      "[step: 565] loss: 0.011577790603041649\n",
      "[step: 566] loss: 0.011577333323657513\n",
      "[step: 567] loss: 0.011576584540307522\n",
      "[step: 568] loss: 0.011575783602893353\n",
      "[step: 569] loss: 0.011575205251574516\n",
      "[step: 570] loss: 0.011574679985642433\n",
      "[step: 571] loss: 0.011573979631066322\n",
      "[step: 572] loss: 0.011573225259780884\n",
      "[step: 573] loss: 0.011572619900107384\n",
      "[step: 574] loss: 0.011572074145078659\n",
      "[step: 575] loss: 0.011571412906050682\n",
      "[step: 576] loss: 0.011570693925023079\n",
      "[step: 577] loss: 0.011570055969059467\n",
      "[step: 578] loss: 0.011569470167160034\n",
      "[step: 579] loss: 0.011568818241357803\n",
      "[step: 580] loss: 0.011568117886781693\n",
      "[step: 581] loss: 0.011567476205527782\n",
      "[step: 582] loss: 0.011566873639822006\n",
      "[step: 583] loss: 0.011566227301955223\n",
      "[step: 584] loss: 0.011565539054572582\n",
      "[step: 585] loss: 0.011564884334802628\n",
      "[step: 586] loss: 0.011564258486032486\n",
      "[step: 587] loss: 0.01156360562890768\n",
      "[step: 588] loss: 0.01156292762607336\n",
      "[step: 589] loss: 0.011562267318367958\n",
      "[step: 590] loss: 0.011561629362404346\n",
      "[step: 591] loss: 0.011560971848666668\n",
      "[step: 592] loss: 0.011560293845832348\n",
      "[step: 593] loss: 0.011559627950191498\n",
      "[step: 594] loss: 0.01155897043645382\n",
      "[step: 595] loss: 0.01155830454081297\n",
      "[step: 596] loss: 0.011557623744010925\n",
      "[step: 597] loss: 0.011556951329112053\n",
      "[step: 598] loss: 0.011556283570826054\n",
      "[step: 599] loss: 0.011555606499314308\n",
      "[step: 600] loss: 0.01155492290854454\n",
      "[step: 601] loss: 0.011554237455129623\n",
      "[step: 602] loss: 0.011553560383617878\n",
      "[step: 603] loss: 0.011552873067557812\n",
      "[step: 604] loss: 0.011552182957530022\n",
      "[step: 605] loss: 0.011551492847502232\n",
      "[step: 606] loss: 0.011550801806151867\n",
      "[step: 607] loss: 0.011550107039511204\n",
      "[step: 608] loss: 0.011549402959644794\n",
      "[step: 609] loss: 0.011548700742423534\n",
      "[step: 610] loss: 0.011547998525202274\n",
      "[step: 611] loss: 0.011547290720045567\n",
      "[step: 612] loss: 0.01154657918959856\n",
      "[step: 613] loss: 0.011545867659151554\n",
      "[step: 614] loss: 0.011545156128704548\n",
      "[step: 615] loss: 0.01154443621635437\n",
      "[step: 616] loss: 0.011543710716068745\n",
      "[step: 617] loss: 0.011542988941073418\n",
      "[step: 618] loss: 0.011542262509465218\n",
      "[step: 619] loss: 0.011541531421244144\n",
      "[step: 620] loss: 0.011540797539055347\n",
      "[step: 621] loss: 0.011540057137608528\n",
      "[step: 622] loss: 0.011539319530129433\n",
      "[step: 623] loss: 0.01153857633471489\n",
      "[step: 624] loss: 0.011537828482687473\n",
      "[step: 625] loss: 0.011537077836692333\n",
      "[step: 626] loss: 0.011536321602761745\n",
      "[step: 627] loss: 0.011535565368831158\n",
      "[step: 628] loss: 0.011534806340932846\n",
      "[step: 629] loss: 0.011534038931131363\n",
      "[step: 630] loss: 0.011533272452652454\n",
      "[step: 631] loss: 0.011532501317560673\n",
      "[step: 632] loss: 0.011531723663210869\n",
      "[step: 633] loss: 0.011530943214893341\n",
      "[step: 634] loss: 0.01153015810996294\n",
      "[step: 635] loss: 0.01152937300503254\n",
      "[step: 636] loss: 0.011528581380844116\n",
      "[step: 637] loss: 0.01152778323739767\n",
      "[step: 638] loss: 0.01152698416262865\n",
      "[step: 639] loss: 0.011526179499924183\n",
      "[step: 640] loss: 0.01152537390589714\n",
      "[step: 641] loss: 0.011524558067321777\n",
      "[step: 642] loss: 0.011523738503456116\n",
      "[step: 643] loss: 0.011522919870913029\n",
      "[step: 644] loss: 0.011522097513079643\n",
      "[step: 645] loss: 0.011521263979375362\n",
      "[step: 646] loss: 0.011520426720380783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 647] loss: 0.011519589461386204\n",
      "[step: 648] loss: 0.011518742889165878\n",
      "[step: 649] loss: 0.011517893522977829\n",
      "[step: 650] loss: 0.011517038568854332\n",
      "[step: 651] loss: 0.011516179889440536\n",
      "[step: 652] loss: 0.011515315622091293\n",
      "[step: 653] loss: 0.011514446698129177\n",
      "[step: 654] loss: 0.011513572186231613\n",
      "[step: 655] loss: 0.01151269394904375\n",
      "[step: 656] loss: 0.011511807329952717\n",
      "[step: 657] loss: 0.011510918848216534\n",
      "[step: 658] loss: 0.01151002012193203\n",
      "[step: 659] loss: 0.011509118601679802\n",
      "[step: 660] loss: 0.011508212424814701\n",
      "[step: 661] loss: 0.011507301591336727\n",
      "[step: 662] loss: 0.011506379581987858\n",
      "[step: 663] loss: 0.011505459435284138\n",
      "[step: 664] loss: 0.011504526250064373\n",
      "[step: 665] loss: 0.011503591202199459\n",
      "[step: 666] loss: 0.011502647772431374\n",
      "[step: 667] loss: 0.01150170061737299\n",
      "[step: 668] loss: 0.011500743217766285\n",
      "[step: 669] loss: 0.011499783955514431\n",
      "[step: 670] loss: 0.011498816311359406\n",
      "[step: 671] loss: 0.011497837491333485\n",
      "[step: 672] loss: 0.011496859602630138\n",
      "[step: 673] loss: 0.011495872400701046\n",
      "[step: 674] loss: 0.011494877748191357\n",
      "[step: 675] loss: 0.01149387750774622\n",
      "[step: 676] loss: 0.011492868885397911\n",
      "[step: 677] loss: 0.011491854675114155\n",
      "[step: 678] loss: 0.011490833945572376\n",
      "[step: 679] loss: 0.011489800177514553\n",
      "[step: 680] loss: 0.01148876454681158\n",
      "[step: 681] loss: 0.011487720534205437\n",
      "[step: 682] loss: 0.011486668139696121\n",
      "[step: 683] loss: 0.01148560643196106\n",
      "[step: 684] loss: 0.011484537273645401\n",
      "[step: 685] loss: 0.011483462527394295\n",
      "[step: 686] loss: 0.011482378467917442\n",
      "[step: 687] loss: 0.011481285095214844\n",
      "[step: 688] loss: 0.011480187065899372\n",
      "[step: 689] loss: 0.011479075998067856\n",
      "[step: 690] loss: 0.011477959342300892\n",
      "[step: 691] loss: 0.011476834304630756\n",
      "[step: 692] loss: 0.011475697159767151\n",
      "[step: 693] loss: 0.011474554426968098\n",
      "[step: 694] loss: 0.011473400518298149\n",
      "[step: 695] loss: 0.011472237296402454\n",
      "[step: 696] loss: 0.011471066623926163\n",
      "[step: 697] loss: 0.011469886638224125\n",
      "[step: 698] loss: 0.011468697339296341\n",
      "[step: 699] loss: 0.011467494070529938\n",
      "[step: 700] loss: 0.011466281488537788\n",
      "[step: 701] loss: 0.011465064249932766\n",
      "[step: 702] loss: 0.011463834904134274\n",
      "[step: 703] loss: 0.011462594382464886\n",
      "[step: 704] loss: 0.011461339890956879\n",
      "[step: 705] loss: 0.011460080742835999\n",
      "[step: 706] loss: 0.0114588076248765\n",
      "[step: 707] loss: 0.011457519605755806\n",
      "[step: 708] loss: 0.01145622506737709\n",
      "[step: 709] loss: 0.01145491935312748\n",
      "[step: 710] loss: 0.011453600600361824\n",
      "[step: 711] loss: 0.011452272534370422\n",
      "[step: 712] loss: 0.011450927704572678\n",
      "[step: 713] loss: 0.011449574492871761\n",
      "[step: 714] loss: 0.0114482082426548\n",
      "[step: 715] loss: 0.011446830816566944\n",
      "[step: 716] loss: 0.011445438489317894\n",
      "[step: 717] loss: 0.011444036848843098\n",
      "[step: 718] loss: 0.011442617513239384\n",
      "[step: 719] loss: 0.011441187933087349\n",
      "[step: 720] loss: 0.011439742520451546\n",
      "[step: 721] loss: 0.011438285000622272\n",
      "[step: 722] loss: 0.01143681164830923\n",
      "[step: 723] loss: 0.011435326188802719\n",
      "[step: 724] loss: 0.011433823965489864\n",
      "[step: 725] loss: 0.01143230777233839\n",
      "[step: 726] loss: 0.011430775746703148\n",
      "[step: 727] loss: 0.01142923068255186\n",
      "[step: 728] loss: 0.01142766885459423\n",
      "[step: 729] loss: 0.011426092125475407\n",
      "[step: 730] loss: 0.011424495838582516\n",
      "[step: 731] loss: 0.011422884650528431\n",
      "[step: 732] loss: 0.01142125390470028\n",
      "[step: 733] loss: 0.011419608257710934\n",
      "[step: 734] loss: 0.011417943052947521\n",
      "[step: 735] loss: 0.011416262947022915\n",
      "[step: 736] loss: 0.011414563283324242\n",
      "[step: 737] loss: 0.011412842199206352\n",
      "[step: 738] loss: 0.011411105282604694\n",
      "[step: 739] loss: 0.01140934880822897\n",
      "[step: 740] loss: 0.011407569982111454\n",
      "[step: 741] loss: 0.011405771598219872\n",
      "[step: 742] loss: 0.011403952725231647\n",
      "[step: 743] loss: 0.011402110569179058\n",
      "[step: 744] loss: 0.011400249786674976\n",
      "[step: 745] loss: 0.01139836385846138\n",
      "[step: 746] loss: 0.011396453715860844\n",
      "[step: 747] loss: 0.011394522152841091\n",
      "[step: 748] loss: 0.0113925626501441\n",
      "[step: 749] loss: 0.011390580795705318\n",
      "[step: 750] loss: 0.011388576589524746\n",
      "[step: 751] loss: 0.011386540718376637\n",
      "[step: 752] loss: 0.011384483426809311\n",
      "[step: 753] loss: 0.011382391676306725\n",
      "[step: 754] loss: 0.011380275711417198\n",
      "[step: 755] loss: 0.011378128081560135\n",
      "[step: 756] loss: 0.011375951580703259\n",
      "[step: 757] loss: 0.011373746208846569\n",
      "[step: 758] loss: 0.011371507309377193\n",
      "[step: 759] loss: 0.011369232088327408\n",
      "[step: 760] loss: 0.01136692613363266\n",
      "[step: 761] loss: 0.0113645875826478\n",
      "[step: 762] loss: 0.011362208053469658\n",
      "[step: 763] loss: 0.011359798721969128\n",
      "[step: 764] loss: 0.011357341893017292\n",
      "[step: 765] loss: 0.01135485153645277\n",
      "[step: 766] loss: 0.01135232113301754\n",
      "[step: 767] loss: 0.011349747888743877\n",
      "[step: 768] loss: 0.011347129009664059\n",
      "[step: 769] loss: 0.011344465427100658\n",
      "[step: 770] loss: 0.011341756209731102\n",
      "[step: 771] loss: 0.01133899949491024\n",
      "[step: 772] loss: 0.011336191557347775\n",
      "[step: 773] loss: 0.011333336122334003\n",
      "[step: 774] loss: 0.011330422945320606\n",
      "[step: 775] loss: 0.011327456682920456\n",
      "[step: 776] loss: 0.01132443267852068\n",
      "[step: 777] loss: 0.011321351863443851\n",
      "[step: 778] loss: 0.01131820771843195\n",
      "[step: 779] loss: 0.011315002106130123\n",
      "[step: 780] loss: 0.0113117266446352\n",
      "[step: 781] loss: 0.011308384127914906\n",
      "[step: 782] loss: 0.01130497083067894\n",
      "[step: 783] loss: 0.011301481164991856\n",
      "[step: 784] loss: 0.011297916993498802\n",
      "[step: 785] loss: 0.011294272728264332\n",
      "[step: 786] loss: 0.01129054557532072\n",
      "[step: 787] loss: 0.011286729946732521\n",
      "[step: 788] loss: 0.011282823979854584\n",
      "[step: 789] loss: 0.01127882581204176\n",
      "[step: 790] loss: 0.011274728924036026\n",
      "[step: 791] loss: 0.011270531453192234\n",
      "[step: 792] loss: 0.011266226880252361\n",
      "[step: 793] loss: 0.01126181147992611\n",
      "[step: 794] loss: 0.011257283389568329\n",
      "[step: 795] loss: 0.01125263050198555\n",
      "[step: 796] loss: 0.011247851885855198\n",
      "[step: 797] loss: 0.011242943815886974\n",
      "[step: 798] loss: 0.011237903498113155\n",
      "[step: 799] loss: 0.011232717894017696\n",
      "[step: 800] loss: 0.011227392591536045\n",
      "[step: 801] loss: 0.01122190896421671\n",
      "[step: 802] loss: 0.011216270737349987\n",
      "[step: 803] loss: 0.01121047604829073\n",
      "[step: 804] loss: 0.01120451744645834\n",
      "[step: 805] loss: 0.011198388412594795\n",
      "[step: 806] loss: 0.011192094534635544\n",
      "[step: 807] loss: 0.01118563674390316\n",
      "[step: 808] loss: 0.011179017834365368\n",
      "[step: 809] loss: 0.011172241531312466\n",
      "[step: 810] loss: 0.011165321804583073\n",
      "[step: 811] loss: 0.01115826703608036\n",
      "[step: 812] loss: 0.011151095852255821\n",
      "[step: 813] loss: 0.01114393025636673\n",
      "[step: 814] loss: 0.011139770038425922\n",
      "[step: 815] loss: 0.011252818629145622\n",
      "[step: 816] loss: 0.015196636319160461\n",
      "[step: 817] loss: 0.0335676372051239\n",
      "[step: 818] loss: 0.018126195296645164\n",
      "[step: 819] loss: 0.027146728709340096\n",
      "[step: 820] loss: 0.019017057493329048\n",
      "[step: 821] loss: 0.012740879319608212\n",
      "[step: 822] loss: 0.015072096139192581\n",
      "[step: 823] loss: 0.019075525924563408\n",
      "[step: 824] loss: 0.017218641936779022\n",
      "[step: 825] loss: 0.013173112645745277\n",
      "[step: 826] loss: 0.012118333950638771\n",
      "[step: 827] loss: 0.013913756236433983\n",
      "[step: 828] loss: 0.01550407987087965\n",
      "[step: 829] loss: 0.014939061366021633\n",
      "[step: 830] loss: 0.013044762425124645\n",
      "[step: 831] loss: 0.011815223842859268\n",
      "[step: 832] loss: 0.012173032388091087\n",
      "[step: 833] loss: 0.013291969895362854\n",
      "[step: 834] loss: 0.013719084672629833\n",
      "[step: 835] loss: 0.013036506250500679\n",
      "[step: 836] loss: 0.01208080630749464\n",
      "[step: 837] loss: 0.01174962054938078\n",
      "[step: 838] loss: 0.012122666463255882\n",
      "[step: 839] loss: 0.01262171845883131\n",
      "[step: 840] loss: 0.012703769840300083\n",
      "[step: 841] loss: 0.01233237236738205\n",
      "[step: 842] loss: 0.011879980564117432\n",
      "[step: 843] loss: 0.011707260273396969\n",
      "[step: 844] loss: 0.011863221414387226\n",
      "[step: 845] loss: 0.012112737633287907\n",
      "[step: 846] loss: 0.012191819958388805\n",
      "[step: 847] loss: 0.012035560794174671\n",
      "[step: 848] loss: 0.011791849508881569\n",
      "[step: 849] loss: 0.011653722263872623\n",
      "[step: 850] loss: 0.011690203100442886\n",
      "[step: 851] loss: 0.01181329321116209\n",
      "[step: 852] loss: 0.011880113743245602\n",
      "[step: 853] loss: 0.011822939850389957\n",
      "[step: 854] loss: 0.011693763546645641\n",
      "[step: 855] loss: 0.011598875746130943\n",
      "[step: 856] loss: 0.011597840115427971\n",
      "[step: 857] loss: 0.011659935116767883\n",
      "[step: 858] loss: 0.011706218123435974\n",
      "[step: 859] loss: 0.011686691083014011\n",
      "[step: 860] loss: 0.011618828400969505\n",
      "[step: 861] loss: 0.011559964157640934\n",
      "[step: 862] loss: 0.011549703776836395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 863] loss: 0.011578492820262909\n",
      "[step: 864] loss: 0.011605053208768368\n",
      "[step: 865] loss: 0.011598557233810425\n",
      "[step: 866] loss: 0.011563854292035103\n",
      "[step: 867] loss: 0.011530501767992973\n",
      "[step: 868] loss: 0.011521940119564533\n",
      "[step: 869] loss: 0.01153587270528078\n",
      "[step: 870] loss: 0.011550940573215485\n",
      "[step: 871] loss: 0.011549031361937523\n",
      "[step: 872] loss: 0.011530720628798008\n",
      "[step: 873] loss: 0.011511447839438915\n",
      "[step: 874] loss: 0.011505084112286568\n",
      "[step: 875] loss: 0.011511673219501972\n",
      "[step: 876] loss: 0.01151973195374012\n",
      "[step: 877] loss: 0.011518718674778938\n",
      "[step: 878] loss: 0.011508552357554436\n",
      "[step: 879] loss: 0.011497861705720425\n",
      "[step: 880] loss: 0.011494405567646027\n",
      "[step: 881] loss: 0.011497960425913334\n",
      "[step: 882] loss: 0.011501935310661793\n",
      "[step: 883] loss: 0.01150063332170248\n",
      "[step: 884] loss: 0.011494439095258713\n",
      "[step: 885] loss: 0.0114883529022336\n",
      "[step: 886] loss: 0.011486422270536423\n",
      "[step: 887] loss: 0.011488126590847969\n",
      "[step: 888] loss: 0.011489651165902615\n",
      "[step: 889] loss: 0.011488137766718864\n",
      "[step: 890] loss: 0.011484194546937943\n",
      "[step: 891] loss: 0.011480677872896194\n",
      "[step: 892] loss: 0.011479556560516357\n",
      "[step: 893] loss: 0.011480160988867283\n",
      "[step: 894] loss: 0.011480328626930714\n",
      "[step: 895] loss: 0.0114787258207798\n",
      "[step: 896] loss: 0.011475980281829834\n",
      "[step: 897] loss: 0.011473721824586391\n",
      "[step: 898] loss: 0.011472825892269611\n",
      "[step: 899] loss: 0.011472707614302635\n",
      "[step: 900] loss: 0.01147214975208044\n",
      "[step: 901] loss: 0.011470599099993706\n",
      "[step: 902] loss: 0.01146859023720026\n",
      "[step: 903] loss: 0.011467010714113712\n",
      "[step: 904] loss: 0.011466176249086857\n",
      "[step: 905] loss: 0.011465620249509811\n",
      "[step: 906] loss: 0.011464688926935196\n",
      "[step: 907] loss: 0.011463228613138199\n",
      "[step: 908] loss: 0.011461636982858181\n",
      "[step: 909] loss: 0.011460376903414726\n",
      "[step: 910] loss: 0.011459500528872013\n",
      "[step: 911] loss: 0.011458677239716053\n",
      "[step: 912] loss: 0.011457590386271477\n",
      "[step: 913] loss: 0.01145624928176403\n",
      "[step: 914] loss: 0.011454915627837181\n",
      "[step: 915] loss: 0.011453798972070217\n",
      "[step: 916] loss: 0.011452851817011833\n",
      "[step: 917] loss: 0.01145187672227621\n",
      "[step: 918] loss: 0.011450731195509434\n",
      "[step: 919] loss: 0.011449473910033703\n",
      "[step: 920] loss: 0.011448273435235023\n",
      "[step: 921] loss: 0.011447199620306492\n",
      "[step: 922] loss: 0.011446190066635609\n",
      "[step: 923] loss: 0.011445124633610249\n",
      "[step: 924] loss: 0.011443967930972576\n",
      "[step: 925] loss: 0.01144277211278677\n",
      "[step: 926] loss: 0.011441631242632866\n",
      "[step: 927] loss: 0.011440552771091461\n",
      "[step: 928] loss: 0.011439485475420952\n",
      "[step: 929] loss: 0.011438368819653988\n",
      "[step: 930] loss: 0.011437206529080868\n",
      "[step: 931] loss: 0.011436042375862598\n",
      "[step: 932] loss: 0.011434909887611866\n",
      "[step: 933] loss: 0.011433806270360947\n",
      "[step: 934] loss: 0.011432691477239132\n",
      "[step: 935] loss: 0.011431544087827206\n",
      "[step: 936] loss: 0.011430371552705765\n",
      "[step: 937] loss: 0.011429207399487495\n",
      "[step: 938] loss: 0.011428063735365868\n",
      "[step: 939] loss: 0.011426922865211964\n",
      "[step: 940] loss: 0.011425767093896866\n",
      "[step: 941] loss: 0.011424589902162552\n",
      "[step: 942] loss: 0.011423405259847641\n",
      "[step: 943] loss: 0.011422228999435902\n",
      "[step: 944] loss: 0.011421049013733864\n",
      "[step: 945] loss: 0.011419876478612423\n",
      "[step: 946] loss: 0.011418680660426617\n",
      "[step: 947] loss: 0.011417476460337639\n",
      "[step: 948] loss: 0.011416266672313213\n",
      "[step: 949] loss: 0.011415062472224236\n",
      "[step: 950] loss: 0.011413851752877235\n",
      "[step: 951] loss: 0.011412633582949638\n",
      "[step: 952] loss: 0.011411407962441444\n",
      "[step: 953] loss: 0.011410171166062355\n",
      "[step: 954] loss: 0.011408933438360691\n",
      "[step: 955] loss: 0.011407694779336452\n",
      "[step: 956] loss: 0.011406444944441319\n",
      "[step: 957] loss: 0.01140519231557846\n",
      "[step: 958] loss: 0.011403927579522133\n",
      "[step: 959] loss: 0.011402660049498081\n",
      "[step: 960] loss: 0.011401386931538582\n",
      "[step: 961] loss: 0.011400108225643635\n",
      "[step: 962] loss: 0.011398823000490665\n",
      "[step: 963] loss: 0.011397531256079674\n",
      "[step: 964] loss: 0.011396230198442936\n",
      "[step: 965] loss: 0.01139492355287075\n",
      "[step: 966] loss: 0.011393615044653416\n",
      "[step: 967] loss: 0.01139229815453291\n",
      "[step: 968] loss: 0.011390970088541508\n",
      "[step: 969] loss: 0.011389639228582382\n",
      "[step: 970] loss: 0.011388297192752361\n",
      "[step: 971] loss: 0.01138695701956749\n",
      "[step: 972] loss: 0.011385605670511723\n",
      "[step: 973] loss: 0.011384245939552784\n",
      "[step: 974] loss: 0.011382881551980972\n",
      "[step: 975] loss: 0.011381509713828564\n",
      "[step: 976] loss: 0.011380132287740707\n",
      "[step: 977] loss: 0.011378745548427105\n",
      "[step: 978] loss: 0.011377349495887756\n",
      "[step: 979] loss: 0.011375952512025833\n",
      "[step: 980] loss: 0.011374544352293015\n",
      "[step: 981] loss: 0.011373132467269897\n",
      "[step: 982] loss: 0.01137171033769846\n",
      "[step: 983] loss: 0.011370283551514149\n",
      "[step: 984] loss: 0.011368848383426666\n",
      "[step: 985] loss: 0.011367406696081161\n",
      "[step: 986] loss: 0.011365960352122784\n",
      "[step: 987] loss: 0.011364500038325787\n",
      "[step: 988] loss: 0.011363034136593342\n",
      "[step: 989] loss: 0.011361564509570599\n",
      "[step: 990] loss: 0.011360088363289833\n",
      "[step: 991] loss: 0.011358603835105896\n",
      "[step: 992] loss: 0.011357106268405914\n",
      "[step: 993] loss: 0.011355612426996231\n",
      "[step: 994] loss: 0.011354103684425354\n",
      "[step: 995] loss: 0.01135258935391903\n",
      "[step: 996] loss: 0.011351068504154682\n",
      "[step: 997] loss: 0.011349539272487164\n",
      "[step: 998] loss: 0.011348006315529346\n",
      "[step: 999] loss: 0.011346460320055485\n",
      "[step: 1000] loss: 0.011344910599291325\n",
      "[step: 1001] loss: 0.011343350633978844\n",
      "[step: 1002] loss: 0.01134178414940834\n",
      "[step: 1003] loss: 0.011340215802192688\n",
      "[step: 1004] loss: 0.01133863627910614\n",
      "[step: 1005] loss: 0.01133705023676157\n",
      "[step: 1006] loss: 0.011335454881191254\n",
      "[step: 1007] loss: 0.011333853006362915\n",
      "[step: 1008] loss: 0.011332244612276554\n",
      "[step: 1009] loss: 0.011330630630254745\n",
      "[step: 1010] loss: 0.011329008266329765\n",
      "[step: 1011] loss: 0.011327379383146763\n",
      "[step: 1012] loss: 0.011325739324092865\n",
      "[step: 1013] loss: 0.011324097402393818\n",
      "[step: 1014] loss: 0.011322446167469025\n",
      "[step: 1015] loss: 0.011320788413286209\n",
      "[step: 1016] loss: 0.011319124139845371\n",
      "[step: 1017] loss: 0.011317450553178787\n",
      "[step: 1018] loss: 0.011315773241221905\n",
      "[step: 1019] loss: 0.011314088478684425\n",
      "[step: 1020] loss: 0.0113123944029212\n",
      "[step: 1021] loss: 0.011310694739222527\n",
      "[step: 1022] loss: 0.011308987624943256\n",
      "[step: 1023] loss: 0.011307276785373688\n",
      "[step: 1024] loss: 0.011305558495223522\n",
      "[step: 1025] loss: 0.01130383089184761\n",
      "[step: 1026] loss: 0.011302100494503975\n",
      "[step: 1027] loss: 0.011300359852612019\n",
      "[step: 1028] loss: 0.011298615485429764\n",
      "[step: 1029] loss: 0.011296863667666912\n",
      "[step: 1030] loss: 0.011295106261968613\n",
      "[step: 1031] loss: 0.011293343268334866\n",
      "[step: 1032] loss: 0.011291573755443096\n",
      "[step: 1033] loss: 0.011289798654615879\n",
      "[step: 1034] loss: 0.01128801517188549\n",
      "[step: 1035] loss: 0.011286227963864803\n",
      "[step: 1036] loss: 0.011284435167908669\n",
      "[step: 1037] loss: 0.011282636784017086\n",
      "[step: 1038] loss: 0.01128083374351263\n",
      "[step: 1039] loss: 0.011279022321105003\n",
      "[step: 1040] loss: 0.011277206242084503\n",
      "[step: 1041] loss: 0.011275386437773705\n",
      "[step: 1042] loss: 0.011273562908172607\n",
      "[step: 1043] loss: 0.011271730065345764\n",
      "[step: 1044] loss: 0.011269895359873772\n",
      "[step: 1045] loss: 0.011268055066466331\n",
      "[step: 1046] loss: 0.011266209185123444\n",
      "[step: 1047] loss: 0.011264361441135406\n",
      "[step: 1048] loss: 0.011262509040534496\n",
      "[step: 1049] loss: 0.01126064732670784\n",
      "[step: 1050] loss: 0.011258784681558609\n",
      "[step: 1051] loss: 0.01125691831111908\n",
      "[step: 1052] loss: 0.011255047284066677\n",
      "[step: 1053] loss: 0.0112531753256917\n",
      "[step: 1054] loss: 0.011251294985413551\n",
      "[step: 1055] loss: 0.011249413713812828\n",
      "[step: 1056] loss: 0.011247527785599232\n",
      "[step: 1057] loss: 0.011245639063417912\n",
      "[step: 1058] loss: 0.011243745684623718\n",
      "[step: 1059] loss: 0.011241854168474674\n",
      "[step: 1060] loss: 0.011239953339099884\n",
      "[step: 1061] loss: 0.011238053441047668\n",
      "[step: 1062] loss: 0.011236149817705154\n",
      "[step: 1063] loss: 0.01123424619436264\n",
      "[step: 1064] loss: 0.01123233512043953\n",
      "[step: 1065] loss: 0.011230424977838993\n",
      "[step: 1066] loss: 0.011228509247303009\n",
      "[step: 1067] loss: 0.011226595379412174\n",
      "[step: 1068] loss: 0.01122467964887619\n",
      "[step: 1069] loss: 0.011222758330404758\n",
      "[step: 1070] loss: 0.011220837943255901\n",
      "[step: 1071] loss: 0.011218912899494171\n",
      "[step: 1072] loss: 0.011216987855732441\n",
      "[step: 1073] loss: 0.011215059086680412\n",
      "[step: 1074] loss: 0.011213132180273533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1075] loss: 0.011211195960640907\n",
      "[step: 1076] loss: 0.011209266260266304\n",
      "[step: 1077] loss: 0.0112073365598917\n",
      "[step: 1078] loss: 0.011205402202904224\n",
      "[step: 1079] loss: 0.01120346412062645\n",
      "[step: 1080] loss: 0.0112015251070261\n",
      "[step: 1081] loss: 0.01119958609342575\n",
      "[step: 1082] loss: 0.011197646148502827\n",
      "[step: 1083] loss: 0.011195704340934753\n",
      "[step: 1084] loss: 0.011193760670721531\n",
      "[step: 1085] loss: 0.011191812343895435\n",
      "[step: 1086] loss: 0.011189867742359638\n",
      "[step: 1087] loss: 0.011187915690243244\n",
      "[step: 1088] loss: 0.011185965500772\n",
      "[step: 1089] loss: 0.01118401437997818\n",
      "[step: 1090] loss: 0.011182055808603764\n",
      "[step: 1091] loss: 0.011180100962519646\n",
      "[step: 1092] loss: 0.011178134009242058\n",
      "[step: 1093] loss: 0.011176171712577343\n",
      "[step: 1094] loss: 0.011174202896654606\n",
      "[step: 1095] loss: 0.011172231286764145\n",
      "[step: 1096] loss: 0.01117025874555111\n",
      "[step: 1097] loss: 0.011168279685080051\n",
      "[step: 1098] loss: 0.01116629596799612\n",
      "[step: 1099] loss: 0.011164305731654167\n",
      "[step: 1100] loss: 0.011162313632667065\n",
      "[step: 1101] loss: 0.011160315945744514\n",
      "[step: 1102] loss: 0.011158309876918793\n",
      "[step: 1103] loss: 0.011156300082802773\n",
      "[step: 1104] loss: 0.011154280975461006\n",
      "[step: 1105] loss: 0.011152254417538643\n",
      "[step: 1106] loss: 0.011150222271680832\n",
      "[step: 1107] loss: 0.0111481798812747\n",
      "[step: 1108] loss: 0.011146126314997673\n",
      "[step: 1109] loss: 0.0111440634354949\n",
      "[step: 1110] loss: 0.01114199124276638\n",
      "[step: 1111] loss: 0.011139906011521816\n",
      "[step: 1112] loss: 0.011137811467051506\n",
      "[step: 1113] loss: 0.011135699227452278\n",
      "[step: 1114] loss: 0.01113357674330473\n",
      "[step: 1115] loss: 0.011131436564028263\n",
      "[step: 1116] loss: 0.011129282414913177\n",
      "[step: 1117] loss: 0.0111271096393466\n",
      "[step: 1118] loss: 0.011124923825263977\n",
      "[step: 1119] loss: 0.011122716590762138\n",
      "[step: 1120] loss: 0.011120494455099106\n",
      "[step: 1121] loss: 0.011118246242403984\n",
      "[step: 1122] loss: 0.01111597940325737\n",
      "[step: 1123] loss: 0.01111368928104639\n",
      "[step: 1124] loss: 0.011111379601061344\n",
      "[step: 1125] loss: 0.011109043844044209\n",
      "[step: 1126] loss: 0.011106685735285282\n",
      "[step: 1127] loss: 0.011104298755526543\n",
      "[step: 1128] loss: 0.011101891286671162\n",
      "[step: 1129] loss: 0.011099453084170818\n",
      "[step: 1130] loss: 0.011096994392573833\n",
      "[step: 1131] loss: 0.011094513349235058\n",
      "[step: 1132] loss: 0.011092004366219044\n",
      "[step: 1133] loss: 0.011089473031461239\n",
      "[step: 1134] loss: 0.011086923070251942\n",
      "[step: 1135] loss: 0.011084355413913727\n",
      "[step: 1136] loss: 0.011081771925091743\n",
      "[step: 1137] loss: 0.011079172603785992\n",
      "[step: 1138] loss: 0.011076570488512516\n",
      "[step: 1139] loss: 0.011073959060013294\n",
      "[step: 1140] loss: 0.011071356013417244\n",
      "[step: 1141] loss: 0.011068757623434067\n",
      "[step: 1142] loss: 0.011066170409321785\n",
      "[step: 1143] loss: 0.011063606478273869\n",
      "[step: 1144] loss: 0.011061067692935467\n",
      "[step: 1145] loss: 0.011058556847274303\n",
      "[step: 1146] loss: 0.0110560841858387\n",
      "[step: 1147] loss: 0.011053650639951229\n",
      "[step: 1148] loss: 0.011051266454160213\n",
      "[step: 1149] loss: 0.01104892697185278\n",
      "[step: 1150] loss: 0.011046640574932098\n",
      "[step: 1151] loss: 0.01104440726339817\n",
      "[step: 1152] loss: 0.011042226105928421\n",
      "[step: 1153] loss: 0.011040099896490574\n",
      "[step: 1154] loss: 0.011038031429052353\n",
      "[step: 1155] loss: 0.011036017909646034\n",
      "[step: 1156] loss: 0.011034063994884491\n",
      "[step: 1157] loss: 0.011032171547412872\n",
      "[step: 1158] loss: 0.01103033497929573\n",
      "[step: 1159] loss: 0.01102855708450079\n",
      "[step: 1160] loss: 0.0110268360003829\n",
      "[step: 1161] loss: 0.011025164276361465\n",
      "[step: 1162] loss: 0.011023536324501038\n",
      "[step: 1163] loss: 0.011021949350833893\n",
      "[step: 1164] loss: 0.01102039311081171\n",
      "[step: 1165] loss: 0.011018863879144192\n",
      "[step: 1166] loss: 0.01101735420525074\n",
      "[step: 1167] loss: 0.011015856638550758\n",
      "[step: 1168] loss: 0.011014367453753948\n",
      "[step: 1169] loss: 0.011012888513505459\n",
      "[step: 1170] loss: 0.011011414229869843\n",
      "[step: 1171] loss: 0.011009950190782547\n",
      "[step: 1172] loss: 0.011008488945662975\n",
      "[step: 1173] loss: 0.011007032357156277\n",
      "[step: 1174] loss: 0.01100558415055275\n",
      "[step: 1175] loss: 0.011004136875271797\n",
      "[step: 1176] loss: 0.011002695187926292\n",
      "[step: 1177] loss: 0.011001265607774258\n",
      "[step: 1178] loss: 0.010999838821589947\n",
      "[step: 1179] loss: 0.010998426005244255\n",
      "[step: 1180] loss: 0.010997024364769459\n",
      "[step: 1181] loss: 0.010995636694133282\n",
      "[step: 1182] loss: 0.010994263924658298\n",
      "[step: 1183] loss: 0.010992910712957382\n",
      "[step: 1184] loss: 0.01099157240241766\n",
      "[step: 1185] loss: 0.010990249924361706\n",
      "[step: 1186] loss: 0.010988948866724968\n",
      "[step: 1187] loss: 0.010987666435539722\n",
      "[step: 1188] loss: 0.010986407287418842\n",
      "[step: 1189] loss: 0.010985172353684902\n",
      "[step: 1190] loss: 0.01098395325243473\n",
      "[step: 1191] loss: 0.010982763953506947\n",
      "[step: 1192] loss: 0.010981596074998379\n",
      "[step: 1193] loss: 0.010980453342199326\n",
      "[step: 1194] loss: 0.010979332029819489\n",
      "[step: 1195] loss: 0.01097824051976204\n",
      "[step: 1196] loss: 0.010977189056575298\n",
      "[step: 1197] loss: 0.010976297780871391\n",
      "[step: 1198] loss: 0.01097667682915926\n",
      "[step: 1199] loss: 0.01098934467881918\n",
      "[step: 1200] loss: 0.011131815612316132\n",
      "[step: 1201] loss: 0.012375091202557087\n",
      "[step: 1202] loss: 0.015709977596998215\n",
      "[step: 1203] loss: 0.011463913135230541\n",
      "[step: 1204] loss: 0.013031015172600746\n",
      "[step: 1205] loss: 0.011585524305701256\n",
      "[step: 1206] loss: 0.01126469299197197\n",
      "[step: 1207] loss: 0.012200969271361828\n",
      "[step: 1208] loss: 0.012123470194637775\n",
      "[step: 1209] loss: 0.011343657970428467\n",
      "[step: 1210] loss: 0.0112423375248909\n",
      "[step: 1211] loss: 0.011705823242664337\n",
      "[step: 1212] loss: 0.01175923366099596\n",
      "[step: 1213] loss: 0.011361625045537949\n",
      "[step: 1214] loss: 0.011191258206963539\n",
      "[step: 1215] loss: 0.011408543214201927\n",
      "[step: 1216] loss: 0.011577533558011055\n",
      "[step: 1217] loss: 0.011445121839642525\n",
      "[step: 1218] loss: 0.011241098865866661\n",
      "[step: 1219] loss: 0.011243204586207867\n",
      "[step: 1220] loss: 0.011386724188923836\n",
      "[step: 1221] loss: 0.011425288394093513\n",
      "[step: 1222] loss: 0.011307165026664734\n",
      "[step: 1223] loss: 0.011208602227270603\n",
      "[step: 1224] loss: 0.011246437206864357\n",
      "[step: 1225] loss: 0.011329183354973793\n",
      "[step: 1226] loss: 0.01132223941385746\n",
      "[step: 1227] loss: 0.011241146363317966\n",
      "[step: 1228] loss: 0.011201278306543827\n",
      "[step: 1229] loss: 0.011239685118198395\n",
      "[step: 1230] loss: 0.01127929612994194\n",
      "[step: 1231] loss: 0.011257119476795197\n",
      "[step: 1232] loss: 0.011207782663404942\n",
      "[step: 1233] loss: 0.011196810752153397\n",
      "[step: 1234] loss: 0.011225619353353977\n",
      "[step: 1235] loss: 0.011242265813052654\n",
      "[step: 1236] loss: 0.011221618391573429\n",
      "[step: 1237] loss: 0.011193810030817986\n",
      "[step: 1238] loss: 0.011193043552339077\n",
      "[step: 1239] loss: 0.011210531927645206\n",
      "[step: 1240] loss: 0.011214257217943668\n",
      "[step: 1241] loss: 0.011197037063539028\n",
      "[step: 1242] loss: 0.011182151734828949\n",
      "[step: 1243] loss: 0.01118583045899868\n",
      "[step: 1244] loss: 0.011195899918675423\n",
      "[step: 1245] loss: 0.011193690821528435\n",
      "[step: 1246] loss: 0.011181192472577095\n",
      "[step: 1247] loss: 0.011174561455845833\n",
      "[step: 1248] loss: 0.011178791522979736\n",
      "[step: 1249] loss: 0.011182728223502636\n",
      "[step: 1250] loss: 0.011177772656083107\n",
      "[step: 1251] loss: 0.011169631034135818\n",
      "[step: 1252] loss: 0.011167589575052261\n",
      "[step: 1253] loss: 0.011170647107064724\n",
      "[step: 1254] loss: 0.011170802637934685\n",
      "[step: 1255] loss: 0.011165696196258068\n",
      "[step: 1256] loss: 0.011160957626998425\n",
      "[step: 1257] loss: 0.011160730384290218\n",
      "[step: 1258] loss: 0.01116198766976595\n",
      "[step: 1259] loss: 0.011160185560584068\n",
      "[step: 1260] loss: 0.011156137101352215\n",
      "[step: 1261] loss: 0.011153873056173325\n",
      "[step: 1262] loss: 0.011154147796332836\n",
      "[step: 1263] loss: 0.011153897270560265\n",
      "[step: 1264] loss: 0.011151419021189213\n",
      "[step: 1265] loss: 0.01114862784743309\n",
      "[step: 1266] loss: 0.011147567071020603\n",
      "[step: 1267] loss: 0.011147387325763702\n",
      "[step: 1268] loss: 0.01114610768854618\n",
      "[step: 1269] loss: 0.011143798939883709\n",
      "[step: 1270] loss: 0.01114208996295929\n",
      "[step: 1271] loss: 0.011141437105834484\n",
      "[step: 1272] loss: 0.011140650138258934\n",
      "[step: 1273] loss: 0.011138970032334328\n",
      "[step: 1274] loss: 0.011137153021991253\n",
      "[step: 1275] loss: 0.011136040091514587\n",
      "[step: 1276] loss: 0.011135278269648552\n",
      "[step: 1277] loss: 0.011134060099720955\n",
      "[step: 1278] loss: 0.011132446117699146\n",
      "[step: 1279] loss: 0.0111311050131917\n",
      "[step: 1280] loss: 0.011130180209875107\n",
      "[step: 1281] loss: 0.011129152961075306\n",
      "[step: 1282] loss: 0.011127774603664875\n",
      "[step: 1283] loss: 0.011126394383609295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1284] loss: 0.011125320568680763\n",
      "[step: 1285] loss: 0.011124344542622566\n",
      "[step: 1286] loss: 0.011123152449727058\n",
      "[step: 1287] loss: 0.01112185325473547\n",
      "[step: 1288] loss: 0.011120710521936417\n",
      "[step: 1289] loss: 0.01111971028149128\n",
      "[step: 1290] loss: 0.011118625290691853\n",
      "[step: 1291] loss: 0.011117413640022278\n",
      "[step: 1292] loss: 0.01111625600606203\n",
      "[step: 1293] loss: 0.01111522875726223\n",
      "[step: 1294] loss: 0.011114188469946384\n",
      "[step: 1295] loss: 0.0111130615696311\n",
      "[step: 1296] loss: 0.011111928150057793\n",
      "[step: 1297] loss: 0.01111088041216135\n",
      "[step: 1298] loss: 0.011109855957329273\n",
      "[step: 1299] loss: 0.011108781211078167\n",
      "[step: 1300] loss: 0.011107676662504673\n",
      "[step: 1301] loss: 0.01110662892460823\n",
      "[step: 1302] loss: 0.011105614714324474\n",
      "[step: 1303] loss: 0.011104573495686054\n",
      "[step: 1304] loss: 0.01110350526869297\n",
      "[step: 1305] loss: 0.011102462187409401\n",
      "[step: 1306] loss: 0.011101447977125645\n",
      "[step: 1307] loss: 0.011100426316261292\n",
      "[step: 1308] loss: 0.011099383234977722\n",
      "[step: 1309] loss: 0.011098349466919899\n",
      "[step: 1310] loss: 0.011097338981926441\n",
      "[step: 1311] loss: 0.011096330359578133\n",
      "[step: 1312] loss: 0.011095304973423481\n",
      "[step: 1313] loss: 0.011094281449913979\n",
      "[step: 1314] loss: 0.011093276552855968\n",
      "[step: 1315] loss: 0.01109226979315281\n",
      "[step: 1316] loss: 0.011091255582869053\n",
      "[step: 1317] loss: 0.011090242303907871\n",
      "[step: 1318] loss: 0.01108923926949501\n",
      "[step: 1319] loss: 0.011088240891695023\n",
      "[step: 1320] loss: 0.011087233200669289\n",
      "[step: 1321] loss: 0.011086221784353256\n",
      "[step: 1322] loss: 0.01108521968126297\n",
      "[step: 1323] loss: 0.011084219440817833\n",
      "[step: 1324] loss: 0.011083214543759823\n",
      "[step: 1325] loss: 0.011082207784056664\n",
      "[step: 1326] loss: 0.011081203818321228\n",
      "[step: 1327] loss: 0.011080201715230942\n",
      "[step: 1328] loss: 0.011079198680818081\n",
      "[step: 1329] loss: 0.011078188195824623\n",
      "[step: 1330] loss: 0.011077184230089188\n",
      "[step: 1331] loss: 0.011076179333031178\n",
      "[step: 1332] loss: 0.011075172573328018\n",
      "[step: 1333] loss: 0.011074158363044262\n",
      "[step: 1334] loss: 0.011073144152760506\n",
      "[step: 1335] loss: 0.011072133667767048\n",
      "[step: 1336] loss: 0.011071120388805866\n",
      "[step: 1337] loss: 0.011070107109844685\n",
      "[step: 1338] loss: 0.01106908917427063\n",
      "[step: 1339] loss: 0.011068071238696575\n",
      "[step: 1340] loss: 0.011067051440477371\n",
      "[step: 1341] loss: 0.011066026985645294\n",
      "[step: 1342] loss: 0.011065001599490643\n",
      "[step: 1343] loss: 0.011063975282013416\n",
      "[step: 1344] loss: 0.01106294710189104\n",
      "[step: 1345] loss: 0.011061914265155792\n",
      "[step: 1346] loss: 0.011060879565775394\n",
      "[step: 1347] loss: 0.011059848591685295\n",
      "[step: 1348] loss: 0.011058807373046875\n",
      "[step: 1349] loss: 0.011057768948376179\n",
      "[step: 1350] loss: 0.011056727729737759\n",
      "[step: 1351] loss: 0.01105568464845419\n",
      "[step: 1352] loss: 0.011054633185267448\n",
      "[step: 1353] loss: 0.011053583584725857\n",
      "[step: 1354] loss: 0.011052533984184265\n",
      "[step: 1355] loss: 0.011051476001739502\n",
      "[step: 1356] loss: 0.011050419881939888\n",
      "[step: 1357] loss: 0.011049360036849976\n",
      "[step: 1358] loss: 0.011048294603824615\n",
      "[step: 1359] loss: 0.01104723010212183\n",
      "[step: 1360] loss: 0.011046163737773895\n",
      "[step: 1361] loss: 0.011045090854167938\n",
      "[step: 1362] loss: 0.011044020764529705\n",
      "[step: 1363] loss: 0.011042943224310875\n",
      "[step: 1364] loss: 0.011041868478059769\n",
      "[step: 1365] loss: 0.01104078907519579\n",
      "[step: 1366] loss: 0.01103970780968666\n",
      "[step: 1367] loss: 0.011038623750209808\n",
      "[step: 1368] loss: 0.011037533171474934\n",
      "[step: 1369] loss: 0.011036444455385208\n",
      "[step: 1370] loss: 0.011035353876650333\n",
      "[step: 1371] loss: 0.011034260503947735\n",
      "[step: 1372] loss: 0.011033166199922562\n",
      "[step: 1373] loss: 0.011032071895897388\n",
      "[step: 1374] loss: 0.011030971072614193\n",
      "[step: 1375] loss: 0.011029870249330997\n",
      "[step: 1376] loss: 0.011028767563402653\n",
      "[step: 1377] loss: 0.011027662083506584\n",
      "[step: 1378] loss: 0.011026556603610516\n",
      "[step: 1379] loss: 0.011025451123714447\n",
      "[step: 1380] loss: 0.011024338193237782\n",
      "[step: 1381] loss: 0.01102322805672884\n",
      "[step: 1382] loss: 0.011022117920219898\n",
      "[step: 1383] loss: 0.011021007783710957\n",
      "[step: 1384] loss: 0.01101989671587944\n",
      "[step: 1385] loss: 0.0110187828540802\n",
      "[step: 1386] loss: 0.011017666198313236\n",
      "[step: 1387] loss: 0.01101655513048172\n",
      "[step: 1388] loss: 0.011015440337359905\n",
      "[step: 1389] loss: 0.011014324612915516\n",
      "[step: 1390] loss: 0.011013210751116276\n",
      "[step: 1391] loss: 0.011012096889317036\n",
      "[step: 1392] loss: 0.01101098395884037\n",
      "[step: 1393] loss: 0.011009867303073406\n",
      "[step: 1394] loss: 0.01100875809788704\n",
      "[step: 1395] loss: 0.011007644236087799\n",
      "[step: 1396] loss: 0.011006535030901432\n",
      "[step: 1397] loss: 0.01100542675703764\n",
      "[step: 1398] loss: 0.011004313826560974\n",
      "[step: 1399] loss: 0.011003212071955204\n",
      "[step: 1400] loss: 0.011002111248672009\n",
      "[step: 1401] loss: 0.011001002974808216\n",
      "[step: 1402] loss: 0.010999903082847595\n",
      "[step: 1403] loss: 0.010998803190886974\n",
      "[step: 1404] loss: 0.01099771074950695\n",
      "[step: 1405] loss: 0.01099662110209465\n",
      "[step: 1406] loss: 0.010995530523359776\n",
      "[step: 1407] loss: 0.010994448326528072\n",
      "[step: 1408] loss: 0.01099336612969637\n",
      "[step: 1409] loss: 0.010992290452122688\n",
      "[step: 1410] loss: 0.010991218499839306\n",
      "[step: 1411] loss: 0.010990150272846222\n",
      "[step: 1412] loss: 0.010989083908498287\n",
      "[step: 1413] loss: 0.010988026857376099\n",
      "[step: 1414] loss: 0.010986973531544209\n",
      "[step: 1415] loss: 0.010985925793647766\n",
      "[step: 1416] loss: 0.010984880849719048\n",
      "[step: 1417] loss: 0.01098384615033865\n",
      "[step: 1418] loss: 0.0109828170388937\n",
      "[step: 1419] loss: 0.010981792584061623\n",
      "[step: 1420] loss: 0.010980774648487568\n",
      "[step: 1421] loss: 0.010979763232171535\n",
      "[step: 1422] loss: 0.010978764854371548\n",
      "[step: 1423] loss: 0.010977767407894135\n",
      "[step: 1424] loss: 0.010976781137287617\n",
      "[step: 1425] loss: 0.010975798591971397\n",
      "[step: 1426] loss: 0.01097482442855835\n",
      "[step: 1427] loss: 0.010973862372338772\n",
      "[step: 1428] loss: 0.010972904972732067\n",
      "[step: 1429] loss: 0.010971956886351109\n",
      "[step: 1430] loss: 0.010971017181873322\n",
      "[step: 1431] loss: 0.01097008865326643\n",
      "[step: 1432] loss: 0.010969169437885284\n",
      "[step: 1433] loss: 0.010968255810439587\n",
      "[step: 1434] loss: 0.01096735242754221\n",
      "[step: 1435] loss: 0.010966455563902855\n",
      "[step: 1436] loss: 0.010965575464069843\n",
      "[step: 1437] loss: 0.010964700020849705\n",
      "[step: 1438] loss: 0.010963834822177887\n",
      "[step: 1439] loss: 0.01096297986805439\n",
      "[step: 1440] loss: 0.010962133295834064\n",
      "[step: 1441] loss: 0.010961297899484634\n",
      "[step: 1442] loss: 0.010960472747683525\n",
      "[step: 1443] loss: 0.010959655046463013\n",
      "[step: 1444] loss: 0.010958850383758545\n",
      "[step: 1445] loss: 0.010958051308989525\n",
      "[step: 1446] loss: 0.010957262478768826\n",
      "[step: 1447] loss: 0.010956484824419022\n",
      "[step: 1448] loss: 0.01095571368932724\n",
      "[step: 1449] loss: 0.010954954661428928\n",
      "[step: 1450] loss: 0.010954206809401512\n",
      "[step: 1451] loss: 0.010953463613986969\n",
      "[step: 1452] loss: 0.010952730663120747\n",
      "[step: 1453] loss: 0.01095200888812542\n",
      "[step: 1454] loss: 0.010951291769742966\n",
      "[step: 1455] loss: 0.010950587689876556\n",
      "[step: 1456] loss: 0.010949889197945595\n",
      "[step: 1457] loss: 0.010949201881885529\n",
      "[step: 1458] loss: 0.01094852015376091\n",
      "[step: 1459] loss: 0.010947847738862038\n",
      "[step: 1460] loss: 0.010947181843221188\n",
      "[step: 1461] loss: 0.010946523398160934\n",
      "[step: 1462] loss: 0.010945876128971577\n",
      "[step: 1463] loss: 0.010945232585072517\n",
      "[step: 1464] loss: 0.010944598354399204\n",
      "[step: 1465] loss: 0.01094396598637104\n",
      "[step: 1466] loss: 0.010943346656858921\n",
      "[step: 1467] loss: 0.010942731983959675\n",
      "[step: 1468] loss: 0.010942124761641026\n",
      "[step: 1469] loss: 0.010941524989902973\n",
      "[step: 1470] loss: 0.010940929874777794\n",
      "[step: 1471] loss: 0.010940339416265488\n",
      "[step: 1472] loss: 0.010939759202301502\n",
      "[step: 1473] loss: 0.010939178988337517\n",
      "[step: 1474] loss: 0.01093861274421215\n",
      "[step: 1475] loss: 0.010938046500086784\n",
      "[step: 1476] loss: 0.01093748677521944\n",
      "[step: 1477] loss: 0.010936934500932693\n",
      "[step: 1478] loss: 0.010936385951936245\n",
      "[step: 1479] loss: 0.010935842990875244\n",
      "[step: 1480] loss: 0.010935306549072266\n",
      "[step: 1481] loss: 0.010934775695204735\n",
      "[step: 1482] loss: 0.010934249497950077\n",
      "[step: 1483] loss: 0.010933725163340569\n",
      "[step: 1484] loss: 0.010933207347989082\n",
      "[step: 1485] loss: 0.010932697914540768\n",
      "[step: 1486] loss: 0.010932188481092453\n",
      "[step: 1487] loss: 0.010931686498224735\n",
      "[step: 1488] loss: 0.010931192897260189\n",
      "[step: 1489] loss: 0.010930698364973068\n",
      "[step: 1490] loss: 0.010930212214589119\n",
      "[step: 1491] loss: 0.010929727926850319\n",
      "[step: 1492] loss: 0.010929247364401817\n",
      "[step: 1493] loss: 0.010928775183856487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1494] loss: 0.010928304865956306\n",
      "[step: 1495] loss: 0.010927839204668999\n",
      "[step: 1496] loss: 0.010927379131317139\n",
      "[step: 1497] loss: 0.010926920920610428\n",
      "[step: 1498] loss: 0.010926471091806889\n",
      "[step: 1499] loss: 0.010926024056971073\n",
      "[step: 1500] loss: 0.010925578884780407\n",
      "[step: 1501] loss: 0.010925138369202614\n",
      "[step: 1502] loss: 0.010924708098173141\n",
      "[step: 1503] loss: 0.010924271307885647\n",
      "[step: 1504] loss: 0.010923844762146473\n",
      "[step: 1505] loss: 0.010923421941697598\n",
      "[step: 1506] loss: 0.010923001915216446\n",
      "[step: 1507] loss: 0.010922586545348167\n",
      "[step: 1508] loss: 0.010922173038125038\n",
      "[step: 1509] loss: 0.010921766981482506\n",
      "[step: 1510] loss: 0.010921361856162548\n",
      "[step: 1511] loss: 0.010920960456132889\n",
      "[step: 1512] loss: 0.010920563712716103\n",
      "[step: 1513] loss: 0.01092017162591219\n",
      "[step: 1514] loss: 0.010919779539108276\n",
      "[step: 1515] loss: 0.010919395834207535\n",
      "[step: 1516] loss: 0.010919012129306793\n",
      "[step: 1517] loss: 0.0109186302870512\n",
      "[step: 1518] loss: 0.010918253101408482\n",
      "[step: 1519] loss: 0.010917877778410912\n",
      "[step: 1520] loss: 0.01091750804334879\n",
      "[step: 1521] loss: 0.01091714110225439\n",
      "[step: 1522] loss: 0.010916776023805141\n",
      "[step: 1523] loss: 0.010916415601968765\n",
      "[step: 1524] loss: 0.010916058905422688\n",
      "[step: 1525] loss: 0.010915703140199184\n",
      "[step: 1526] loss: 0.01091534923762083\n",
      "[step: 1527] loss: 0.01091499999165535\n",
      "[step: 1528] loss: 0.010914651677012444\n",
      "[step: 1529] loss: 0.010914308950304985\n",
      "[step: 1530] loss: 0.010913965292274952\n",
      "[step: 1531] loss: 0.010913625359535217\n",
      "[step: 1532] loss: 0.010913290083408356\n",
      "[step: 1533] loss: 0.010912956669926643\n",
      "[step: 1534] loss: 0.01091262698173523\n",
      "[step: 1535] loss: 0.01091229822486639\n",
      "[step: 1536] loss: 0.010911967605352402\n",
      "[step: 1537] loss: 0.010911641642451286\n",
      "[step: 1538] loss: 0.010911322198808193\n",
      "[step: 1539] loss: 0.01091100089251995\n",
      "[step: 1540] loss: 0.010910685174167156\n",
      "[step: 1541] loss: 0.010910369455814362\n",
      "[step: 1542] loss: 0.010910052806138992\n",
      "[step: 1543] loss: 0.010909740813076496\n",
      "[step: 1544] loss: 0.010909428820014\n",
      "[step: 1545] loss: 0.010909121483564377\n",
      "[step: 1546] loss: 0.010908816941082478\n",
      "[step: 1547] loss: 0.010908511467278004\n",
      "[step: 1548] loss: 0.010908210650086403\n",
      "[step: 1549] loss: 0.010907907038927078\n",
      "[step: 1550] loss: 0.010907609947025776\n",
      "[step: 1551] loss: 0.010907310992479324\n",
      "[step: 1552] loss: 0.010907016694545746\n",
      "[step: 1553] loss: 0.010906721465289593\n",
      "[step: 1554] loss: 0.010906429030001163\n",
      "[step: 1555] loss: 0.010906138457357883\n",
      "[step: 1556] loss: 0.010905846022069454\n",
      "[step: 1557] loss: 0.010905560106039047\n",
      "[step: 1558] loss: 0.01090527419000864\n",
      "[step: 1559] loss: 0.010904989205300808\n",
      "[step: 1560] loss: 0.010904707945883274\n",
      "[step: 1561] loss: 0.010904423892498016\n",
      "[step: 1562] loss: 0.010904142633080482\n",
      "[step: 1563] loss: 0.01090385764837265\n",
      "[step: 1564] loss: 0.01090358104556799\n",
      "[step: 1565] loss: 0.010903304442763329\n",
      "[step: 1566] loss: 0.010903027839958668\n",
      "[step: 1567] loss: 0.01090275403112173\n",
      "[step: 1568] loss: 0.010902481153607368\n",
      "[step: 1569] loss: 0.010902204550802708\n",
      "[step: 1570] loss: 0.01090193260461092\n",
      "[step: 1571] loss: 0.010901661589741707\n",
      "[step: 1572] loss: 0.010901392437517643\n",
      "[step: 1573] loss: 0.010901122353971004\n",
      "[step: 1574] loss: 0.010900854133069515\n",
      "[step: 1575] loss: 0.0109005868434906\n",
      "[step: 1576] loss: 0.010900315828621387\n",
      "[step: 1577] loss: 0.010900053195655346\n",
      "[step: 1578] loss: 0.010899784974753857\n",
      "[step: 1579] loss: 0.010899522341787815\n",
      "[step: 1580] loss: 0.010899258777499199\n",
      "[step: 1581] loss: 0.010898995213210583\n",
      "[step: 1582] loss: 0.010898733511567116\n",
      "[step: 1583] loss: 0.010898473672568798\n",
      "[step: 1584] loss: 0.010898211970925331\n",
      "[step: 1585] loss: 0.010897952131927013\n",
      "[step: 1586] loss: 0.010897691361606121\n",
      "[step: 1587] loss: 0.010897432453930378\n",
      "[step: 1588] loss: 0.010897173546254635\n",
      "[step: 1589] loss: 0.010896913707256317\n",
      "[step: 1590] loss: 0.010896657593548298\n",
      "[step: 1591] loss: 0.010896400548517704\n",
      "[step: 1592] loss: 0.010896142572164536\n",
      "[step: 1593] loss: 0.010895888321101665\n",
      "[step: 1594] loss: 0.010895632207393646\n",
      "[step: 1595] loss: 0.010895377025008202\n",
      "[step: 1596] loss: 0.010895121842622757\n",
      "[step: 1597] loss: 0.010894868522882462\n",
      "[step: 1598] loss: 0.010894613340497017\n",
      "[step: 1599] loss: 0.010894358158111572\n",
      "[step: 1600] loss: 0.010894106701016426\n",
      "[step: 1601] loss: 0.010893851518630981\n",
      "[step: 1602] loss: 0.01089359913021326\n",
      "[step: 1603] loss: 0.010893345810472965\n",
      "[step: 1604] loss: 0.010893091559410095\n",
      "[step: 1605] loss: 0.010892841033637524\n",
      "[step: 1606] loss: 0.010892590507864952\n",
      "[step: 1607] loss: 0.010892334394156933\n",
      "[step: 1608] loss: 0.010892082937061787\n",
      "[step: 1609] loss: 0.01089183334261179\n",
      "[step: 1610] loss: 0.010891580954194069\n",
      "[step: 1611] loss: 0.010891329497098923\n",
      "[step: 1612] loss: 0.010891079902648926\n",
      "[step: 1613] loss: 0.010890827514231205\n",
      "[step: 1614] loss: 0.010890576057136059\n",
      "[step: 1615] loss: 0.010890324600040913\n",
      "[step: 1616] loss: 0.010890074074268341\n",
      "[step: 1617] loss: 0.010889822617173195\n",
      "[step: 1618] loss: 0.010889571160078049\n",
      "[step: 1619] loss: 0.010889320634305477\n",
      "[step: 1620] loss: 0.010889067314565182\n",
      "[step: 1621] loss: 0.010888815857470036\n",
      "[step: 1622] loss: 0.010888566263020039\n",
      "[step: 1623] loss: 0.010888316668570042\n",
      "[step: 1624] loss: 0.010888063348829746\n",
      "[step: 1625] loss: 0.010887812823057175\n",
      "[step: 1626] loss: 0.010887560434639454\n",
      "[step: 1627] loss: 0.010887307114899158\n",
      "[step: 1628] loss: 0.010887057520449162\n",
      "[step: 1629] loss: 0.010886804200708866\n",
      "[step: 1630] loss: 0.010886549949645996\n",
      "[step: 1631] loss: 0.010886300355196\n",
      "[step: 1632] loss: 0.010886048898100853\n",
      "[step: 1633] loss: 0.010885795578360558\n",
      "[step: 1634] loss: 0.010885541327297688\n",
      "[step: 1635] loss: 0.010885288938879967\n",
      "[step: 1636] loss: 0.01088503748178482\n",
      "[step: 1637] loss: 0.010884780436754227\n",
      "[step: 1638] loss: 0.010884526185691357\n",
      "[step: 1639] loss: 0.010884271934628487\n",
      "[step: 1640] loss: 0.01088402047753334\n",
      "[step: 1641] loss: 0.010883764363825321\n",
      "[step: 1642] loss: 0.010883510112762451\n",
      "[step: 1643] loss: 0.010883254930377007\n",
      "[step: 1644] loss: 0.010882997885346413\n",
      "[step: 1645] loss: 0.010882742702960968\n",
      "[step: 1646] loss: 0.010882488451898098\n",
      "[step: 1647] loss: 0.010882227681577206\n",
      "[step: 1648] loss: 0.010881970636546612\n",
      "[step: 1649] loss: 0.010881714522838593\n",
      "[step: 1650] loss: 0.01088145561516285\n",
      "[step: 1651] loss: 0.010881197638809681\n",
      "[step: 1652] loss: 0.010880939662456512\n",
      "[step: 1653] loss: 0.01088068075478077\n",
      "[step: 1654] loss: 0.01088042464107275\n",
      "[step: 1655] loss: 0.01088017225265503\n",
      "[step: 1656] loss: 0.010879935696721077\n",
      "[step: 1657] loss: 0.010879738256335258\n",
      "[step: 1658] loss: 0.010879633948206902\n",
      "[step: 1659] loss: 0.010879816487431526\n",
      "[step: 1660] loss: 0.010880732908844948\n",
      "[step: 1661] loss: 0.010883919894695282\n",
      "[step: 1662] loss: 0.010892475955188274\n",
      "[step: 1663] loss: 0.010920779779553413\n",
      "[step: 1664] loss: 0.01097885612398386\n",
      "[step: 1665] loss: 0.011188364587724209\n",
      "[step: 1666] loss: 0.01124723069369793\n",
      "[step: 1667] loss: 0.011522777378559113\n",
      "[step: 1668] loss: 0.010944817215204239\n",
      "[step: 1669] loss: 0.01108847837895155\n",
      "[step: 1670] loss: 0.011442977003753185\n",
      "[step: 1671] loss: 0.010887746699154377\n",
      "[step: 1672] loss: 0.011414092034101486\n",
      "[step: 1673] loss: 0.011175167746841908\n",
      "[step: 1674] loss: 0.011214589700102806\n",
      "[step: 1675] loss: 0.011048409156501293\n",
      "[step: 1676] loss: 0.011129206977784634\n",
      "[step: 1677] loss: 0.011020893231034279\n",
      "[step: 1678] loss: 0.011147547513246536\n",
      "[step: 1679] loss: 0.010925253853201866\n",
      "[step: 1680] loss: 0.011135163716971874\n",
      "[step: 1681] loss: 0.010931764729321003\n",
      "[step: 1682] loss: 0.01105534192174673\n",
      "[step: 1683] loss: 0.011002370156347752\n",
      "[step: 1684] loss: 0.010949589312076569\n",
      "[step: 1685] loss: 0.01104172132909298\n",
      "[step: 1686] loss: 0.010931896977126598\n",
      "[step: 1687] loss: 0.011001640930771828\n",
      "[step: 1688] loss: 0.01097334548830986\n",
      "[step: 1689] loss: 0.010941642336547375\n",
      "[step: 1690] loss: 0.010995374992489815\n",
      "[step: 1691] loss: 0.010934360325336456\n",
      "[step: 1692] loss: 0.01095999963581562\n",
      "[step: 1693] loss: 0.010963663458824158\n",
      "[step: 1694] loss: 0.010928327217698097\n",
      "[step: 1695] loss: 0.01096208207309246\n",
      "[step: 1696] loss: 0.010933025740087032\n",
      "[step: 1697] loss: 0.010935953818261623\n",
      "[step: 1698] loss: 0.010947507806122303\n",
      "[step: 1699] loss: 0.010920539498329163\n",
      "[step: 1700] loss: 0.010938291437923908\n",
      "[step: 1701] loss: 0.010926811024546623\n",
      "[step: 1702] loss: 0.010920966975390911\n",
      "[step: 1703] loss: 0.010930775664746761\n",
      "[step: 1704] loss: 0.010913282632827759\n",
      "[step: 1705] loss: 0.010922908782958984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1706] loss: 0.010915983468294144\n",
      "[step: 1707] loss: 0.010911202058196068\n",
      "[step: 1708] loss: 0.010916666127741337\n",
      "[step: 1709] loss: 0.010905635543167591\n",
      "[step: 1710] loss: 0.010911825112998486\n",
      "[step: 1711] loss: 0.010904518887400627\n",
      "[step: 1712] loss: 0.010904615744948387\n",
      "[step: 1713] loss: 0.010904692113399506\n",
      "[step: 1714] loss: 0.010898822918534279\n",
      "[step: 1715] loss: 0.010902376845479012\n",
      "[step: 1716] loss: 0.010895848274230957\n",
      "[step: 1717] loss: 0.010898620821535587\n",
      "[step: 1718] loss: 0.010894158855080605\n",
      "[step: 1719] loss: 0.010894405655562878\n",
      "[step: 1720] loss: 0.010892706923186779\n",
      "[step: 1721] loss: 0.010890917852520943\n",
      "[step: 1722] loss: 0.010890834964811802\n",
      "[step: 1723] loss: 0.01088805589824915\n",
      "[step: 1724] loss: 0.010888817720115185\n",
      "[step: 1725] loss: 0.010885748080909252\n",
      "[step: 1726] loss: 0.010886603966355324\n",
      "[step: 1727] loss: 0.010883820243179798\n",
      "[step: 1728] loss: 0.01088451687246561\n",
      "[step: 1729] loss: 0.010882000438869\n",
      "[step: 1730] loss: 0.010882539674639702\n",
      "[step: 1731] loss: 0.01088037621229887\n",
      "[step: 1732] loss: 0.010880689136683941\n",
      "[step: 1733] loss: 0.010878817178308964\n",
      "[step: 1734] loss: 0.010878999717533588\n",
      "[step: 1735] loss: 0.01087734941393137\n",
      "[step: 1736] loss: 0.010877379216253757\n",
      "[step: 1737] loss: 0.010876032523810863\n",
      "[step: 1738] loss: 0.010875817388296127\n",
      "[step: 1739] loss: 0.010874779894948006\n",
      "[step: 1740] loss: 0.01087432075291872\n",
      "[step: 1741] loss: 0.010873633436858654\n",
      "[step: 1742] loss: 0.010872880928218365\n",
      "[step: 1743] loss: 0.010872500948607922\n",
      "[step: 1744] loss: 0.01087157242000103\n",
      "[step: 1745] loss: 0.010871310718357563\n",
      "[step: 1746] loss: 0.010870411060750484\n",
      "[step: 1747] loss: 0.010870068334043026\n",
      "[step: 1748] loss: 0.010869373567402363\n",
      "[step: 1749] loss: 0.010868804529309273\n",
      "[step: 1750] loss: 0.010868372395634651\n",
      "[step: 1751] loss: 0.010867632925510406\n",
      "[step: 1752] loss: 0.010867276228964329\n",
      "[step: 1753] loss: 0.01086661871522665\n",
      "[step: 1754] loss: 0.01086611207574606\n",
      "[step: 1755] loss: 0.0108656520023942\n",
      "[step: 1756] loss: 0.01086501870304346\n",
      "[step: 1757] loss: 0.01086460705846548\n",
      "[step: 1758] loss: 0.010864043608307838\n",
      "[step: 1759] loss: 0.010863516479730606\n",
      "[step: 1760] loss: 0.010863084346055984\n",
      "[step: 1761] loss: 0.010862510651350021\n",
      "[step: 1762] loss: 0.010862045921385288\n",
      "[step: 1763] loss: 0.010861575603485107\n",
      "[step: 1764] loss: 0.010861029848456383\n",
      "[step: 1765] loss: 0.01086058933287859\n",
      "[step: 1766] loss: 0.010860104113817215\n",
      "[step: 1767] loss: 0.010859592817723751\n",
      "[step: 1768] loss: 0.010859151370823383\n",
      "[step: 1769] loss: 0.010858668014407158\n",
      "[step: 1770] loss: 0.010858173482120037\n",
      "[step: 1771] loss: 0.010857733897864819\n",
      "[step: 1772] loss: 0.010857255198061466\n",
      "[step: 1773] loss: 0.010856776498258114\n",
      "[step: 1774] loss: 0.010856331326067448\n",
      "[step: 1775] loss: 0.010855866596102715\n",
      "[step: 1776] loss: 0.010855394415557384\n",
      "[step: 1777] loss: 0.010854944586753845\n",
      "[step: 1778] loss: 0.010854491032660007\n",
      "[step: 1779] loss: 0.010854020714759827\n",
      "[step: 1780] loss: 0.010853570885956287\n",
      "[step: 1781] loss: 0.010853124782443047\n",
      "[step: 1782] loss: 0.010852660983800888\n",
      "[step: 1783] loss: 0.010852206498384476\n",
      "[step: 1784] loss: 0.010851757600903511\n",
      "[step: 1785] loss: 0.01085130125284195\n",
      "[step: 1786] loss: 0.010850838385522366\n",
      "[step: 1787] loss: 0.010850388556718826\n",
      "[step: 1788] loss: 0.010849938727915287\n",
      "[step: 1789] loss: 0.010849479585886002\n",
      "[step: 1790] loss: 0.010849015787243843\n",
      "[step: 1791] loss: 0.01084856130182743\n",
      "[step: 1792] loss: 0.01084810309112072\n",
      "[step: 1793] loss: 0.01084764115512371\n",
      "[step: 1794] loss: 0.010847180150449276\n",
      "[step: 1795] loss: 0.010846717283129692\n",
      "[step: 1796] loss: 0.010846252553164959\n",
      "[step: 1797] loss: 0.010845782235264778\n",
      "[step: 1798] loss: 0.010845311917364597\n",
      "[step: 1799] loss: 0.01084484439343214\n",
      "[step: 1800] loss: 0.010844368487596512\n",
      "[step: 1801] loss: 0.010843893513083458\n",
      "[step: 1802] loss: 0.010843412019312382\n",
      "[step: 1803] loss: 0.010842930525541306\n",
      "[step: 1804] loss: 0.01084244903177023\n",
      "[step: 1805] loss: 0.01084196101874113\n",
      "[step: 1806] loss: 0.010841473005712032\n",
      "[step: 1807] loss: 0.010840978473424911\n",
      "[step: 1808] loss: 0.010840479284524918\n",
      "[step: 1809] loss: 0.010839980095624924\n",
      "[step: 1810] loss: 0.010839477181434631\n",
      "[step: 1811] loss: 0.01083897240459919\n",
      "[step: 1812] loss: 0.010838459245860577\n",
      "[step: 1813] loss: 0.010837947018444538\n",
      "[step: 1814] loss: 0.010837421752512455\n",
      "[step: 1815] loss: 0.010836903005838394\n",
      "[step: 1816] loss: 0.010836374945938587\n",
      "[step: 1817] loss: 0.010835844092071056\n",
      "[step: 1818] loss: 0.010835305787622929\n",
      "[step: 1819] loss: 0.010834763757884502\n",
      "[step: 1820] loss: 0.010834220796823502\n",
      "[step: 1821] loss: 0.01083366945385933\n",
      "[step: 1822] loss: 0.010833111591637135\n",
      "[step: 1823] loss: 0.01083255186676979\n",
      "[step: 1824] loss: 0.010831983759999275\n",
      "[step: 1825] loss: 0.010831410065293312\n",
      "[step: 1826] loss: 0.010830833576619625\n",
      "[step: 1827] loss: 0.010830252431333065\n",
      "[step: 1828] loss: 0.010829661041498184\n",
      "[step: 1829] loss: 0.010829065926373005\n",
      "[step: 1830] loss: 0.010828464291989803\n",
      "[step: 1831] loss: 0.010827857069671154\n",
      "[step: 1832] loss: 0.010827244259417057\n",
      "[step: 1833] loss: 0.010826623998582363\n",
      "[step: 1834] loss: 0.010826002806425095\n",
      "[step: 1835] loss: 0.010825371369719505\n",
      "[step: 1836] loss: 0.010824743658304214\n",
      "[step: 1837] loss: 0.010824127122759819\n",
      "[step: 1838] loss: 0.010823535732924938\n",
      "[step: 1839] loss: 0.010823039337992668\n",
      "[step: 1840] loss: 0.010822759941220284\n",
      "[step: 1841] loss: 0.010823150165379047\n",
      "[step: 1842] loss: 0.010825148783624172\n",
      "[step: 1843] loss: 0.010832589119672775\n",
      "[step: 1844] loss: 0.010851923376321793\n",
      "[step: 1845] loss: 0.01092079933732748\n",
      "[step: 1846] loss: 0.01103692315518856\n",
      "[step: 1847] loss: 0.011467406526207924\n",
      "[step: 1848] loss: 0.011185088194906712\n",
      "[step: 1849] loss: 0.011011159978806973\n",
      "[step: 1850] loss: 0.01082850992679596\n",
      "[step: 1851] loss: 0.011045453138649464\n",
      "[step: 1852] loss: 0.011089347302913666\n",
      "[step: 1853] loss: 0.010855813510715961\n",
      "[step: 1854] loss: 0.011127429082989693\n",
      "[step: 1855] loss: 0.010915451683104038\n",
      "[step: 1856] loss: 0.010975881479680538\n",
      "[step: 1857] loss: 0.010933095589280128\n",
      "[step: 1858] loss: 0.010890129022300243\n",
      "[step: 1859] loss: 0.01093321107327938\n",
      "[step: 1860] loss: 0.010893343947827816\n",
      "[step: 1861] loss: 0.010904709808528423\n",
      "[step: 1862] loss: 0.010880816727876663\n",
      "[step: 1863] loss: 0.01089206151664257\n",
      "[step: 1864] loss: 0.010884877294301987\n",
      "[step: 1865] loss: 0.010870211757719517\n",
      "[step: 1866] loss: 0.010886728763580322\n",
      "[step: 1867] loss: 0.010857412591576576\n",
      "[step: 1868] loss: 0.010886071249842644\n",
      "[step: 1869] loss: 0.010847065597772598\n",
      "[step: 1870] loss: 0.01088019460439682\n",
      "[step: 1871] loss: 0.010844153352081776\n",
      "[step: 1872] loss: 0.010871224105358124\n",
      "[step: 1873] loss: 0.010841218754649162\n",
      "[step: 1874] loss: 0.01086375117301941\n",
      "[step: 1875] loss: 0.010838937014341354\n",
      "[step: 1876] loss: 0.010854038409888744\n",
      "[step: 1877] loss: 0.010838097892701626\n",
      "[step: 1878] loss: 0.0108463354408741\n",
      "[step: 1879] loss: 0.010834166780114174\n",
      "[step: 1880] loss: 0.010840248316526413\n",
      "[step: 1881] loss: 0.010830773040652275\n",
      "[step: 1882] loss: 0.010834826156497002\n",
      "[step: 1883] loss: 0.010826162993907928\n",
      "[step: 1884] loss: 0.010830129496753216\n",
      "[step: 1885] loss: 0.010821695439517498\n",
      "[step: 1886] loss: 0.010826574638485909\n",
      "[step: 1887] loss: 0.01081700250506401\n",
      "[step: 1888] loss: 0.010822239331901073\n",
      "[step: 1889] loss: 0.010813762433826923\n",
      "[step: 1890] loss: 0.010817603208124638\n",
      "[step: 1891] loss: 0.010812189429998398\n",
      "[step: 1892] loss: 0.01081169955432415\n",
      "[step: 1893] loss: 0.010811110027134418\n",
      "[step: 1894] loss: 0.010806435719132423\n",
      "[step: 1895] loss: 0.010808669030666351\n",
      "[step: 1896] loss: 0.010804037563502789\n",
      "[step: 1897] loss: 0.010803944431245327\n",
      "[step: 1898] loss: 0.010803771205246449\n",
      "[step: 1899] loss: 0.010799814015626907\n",
      "[step: 1900] loss: 0.01080094464123249\n",
      "[step: 1901] loss: 0.010799679905176163\n",
      "[step: 1902] loss: 0.010796458460390568\n",
      "[step: 1903] loss: 0.010797318071126938\n",
      "[step: 1904] loss: 0.010796600952744484\n",
      "[step: 1905] loss: 0.010793479159474373\n",
      "[step: 1906] loss: 0.010793238878250122\n",
      "[step: 1907] loss: 0.010793627239763737\n",
      "[step: 1908] loss: 0.010791435837745667\n",
      "[step: 1909] loss: 0.010789357125759125\n",
      "[step: 1910] loss: 0.010789139196276665\n",
      "[step: 1911] loss: 0.010789068415760994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1912] loss: 0.010787748731672764\n",
      "[step: 1913] loss: 0.010785697028040886\n",
      "[step: 1914] loss: 0.010784291662275791\n",
      "[step: 1915] loss: 0.010783775709569454\n",
      "[step: 1916] loss: 0.010783481411635876\n",
      "[step: 1917] loss: 0.01078294962644577\n",
      "[step: 1918] loss: 0.010781862773001194\n",
      "[step: 1919] loss: 0.010780591517686844\n",
      "[step: 1920] loss: 0.010779176838696003\n",
      "[step: 1921] loss: 0.010777896270155907\n",
      "[step: 1922] loss: 0.010776694864034653\n",
      "[step: 1923] loss: 0.010775696486234665\n",
      "[step: 1924] loss: 0.010774904862046242\n",
      "[step: 1925] loss: 0.01077474094927311\n",
      "[step: 1926] loss: 0.010775787755846977\n",
      "[step: 1927] loss: 0.010781166143715382\n",
      "[step: 1928] loss: 0.010796026326715946\n",
      "[step: 1929] loss: 0.010851504281163216\n",
      "[step: 1930] loss: 0.010960404761135578\n",
      "[step: 1931] loss: 0.011364634148776531\n",
      "[step: 1932] loss: 0.011124525219202042\n",
      "[step: 1933] loss: 0.010975005105137825\n",
      "[step: 1934] loss: 0.01078720297664404\n",
      "[step: 1935] loss: 0.011012655682861805\n",
      "[step: 1936] loss: 0.010994571261107922\n",
      "[step: 1937] loss: 0.010834244079887867\n",
      "[step: 1938] loss: 0.011045639403164387\n",
      "[step: 1939] loss: 0.010819329880177975\n",
      "[step: 1940] loss: 0.010928712785243988\n",
      "[step: 1941] loss: 0.010824373923242092\n",
      "[step: 1942] loss: 0.01088529173284769\n",
      "[step: 1943] loss: 0.010843122377991676\n",
      "[step: 1944] loss: 0.010869249701499939\n",
      "[step: 1945] loss: 0.01082642562687397\n",
      "[step: 1946] loss: 0.010857822373509407\n",
      "[step: 1947] loss: 0.010822768323123455\n",
      "[step: 1948] loss: 0.010852006264030933\n",
      "[step: 1949] loss: 0.01080804131925106\n",
      "[step: 1950] loss: 0.010846561752259731\n",
      "[step: 1951] loss: 0.010804436169564724\n",
      "[step: 1952] loss: 0.010837385430932045\n",
      "[step: 1953] loss: 0.010797700844705105\n",
      "[step: 1954] loss: 0.010830072686076164\n",
      "[step: 1955] loss: 0.010793234221637249\n",
      "[step: 1956] loss: 0.010819096118211746\n",
      "[step: 1957] loss: 0.010788243263959885\n",
      "[step: 1958] loss: 0.010810577310621738\n",
      "[step: 1959] loss: 0.01078428141772747\n",
      "[step: 1960] loss: 0.010801061056554317\n",
      "[step: 1961] loss: 0.010779952630400658\n",
      "[step: 1962] loss: 0.010793921537697315\n",
      "[step: 1963] loss: 0.010777215473353863\n",
      "[step: 1964] loss: 0.010785386897623539\n",
      "[step: 1965] loss: 0.010774520225822926\n",
      "[step: 1966] loss: 0.010776416398584843\n",
      "[step: 1967] loss: 0.010774376802146435\n",
      "[step: 1968] loss: 0.010767824947834015\n",
      "[step: 1969] loss: 0.01077342126518488\n",
      "[step: 1970] loss: 0.01076192781329155\n",
      "[step: 1971] loss: 0.010767335072159767\n",
      "[step: 1972] loss: 0.010761834681034088\n",
      "[step: 1973] loss: 0.01075818482786417\n",
      "[step: 1974] loss: 0.010761925019323826\n",
      "[step: 1975] loss: 0.010754826478660107\n",
      "[step: 1976] loss: 0.010754403658211231\n",
      "[step: 1977] loss: 0.01075657270848751\n",
      "[step: 1978] loss: 0.010750374756753445\n",
      "[step: 1979] loss: 0.010748826898634434\n",
      "[step: 1980] loss: 0.010751082561910152\n",
      "[step: 1981] loss: 0.010747704654932022\n",
      "[step: 1982] loss: 0.010743347927927971\n",
      "[step: 1983] loss: 0.010743007063865662\n",
      "[step: 1984] loss: 0.010743916966021061\n",
      "[step: 1985] loss: 0.010742584243416786\n",
      "[step: 1986] loss: 0.01073882170021534\n",
      "[step: 1987] loss: 0.010735712945461273\n",
      "[step: 1988] loss: 0.01073436625301838\n",
      "[step: 1989] loss: 0.010734198614954948\n",
      "[step: 1990] loss: 0.01073487289249897\n",
      "[step: 1991] loss: 0.010735824704170227\n",
      "[step: 1992] loss: 0.010739637538790703\n",
      "[step: 1993] loss: 0.010746973566710949\n",
      "[step: 1994] loss: 0.010772861540317535\n",
      "[step: 1995] loss: 0.010819636285305023\n",
      "[step: 1996] loss: 0.01098555140197277\n",
      "[step: 1997] loss: 0.011057301424443722\n",
      "[step: 1998] loss: 0.011283568106591702\n",
      "[step: 1999] loss: 0.010789101012051105\n",
      "[step: 2000] loss: 0.010883359238505363\n",
      "[step: 2001] loss: 0.011106675490736961\n",
      "[step: 2002] loss: 0.010759233497083187\n",
      "[step: 2003] loss: 0.011083913967013359\n",
      "[step: 2004] loss: 0.010901798494160175\n",
      "[step: 2005] loss: 0.010902956128120422\n",
      "[step: 2006] loss: 0.01090456172823906\n",
      "[step: 2007] loss: 0.010815011337399483\n",
      "[step: 2008] loss: 0.010893833823502064\n",
      "[step: 2009] loss: 0.010818752460181713\n",
      "[step: 2010] loss: 0.010859142057597637\n",
      "[step: 2011] loss: 0.010807925835251808\n",
      "[step: 2012] loss: 0.010833436623215675\n",
      "[step: 2013] loss: 0.010814462788403034\n",
      "[step: 2014] loss: 0.010801983065903187\n",
      "[step: 2015] loss: 0.010813004337251186\n",
      "[step: 2016] loss: 0.010783813893795013\n",
      "[step: 2017] loss: 0.010810303501784801\n",
      "[step: 2018] loss: 0.010766010731458664\n",
      "[step: 2019] loss: 0.010803877376019955\n",
      "[step: 2020] loss: 0.010755516588687897\n",
      "[step: 2021] loss: 0.010791828855872154\n",
      "[step: 2022] loss: 0.010746948421001434\n",
      "[step: 2023] loss: 0.010779804550111294\n",
      "[step: 2024] loss: 0.010741584934294224\n",
      "[step: 2025] loss: 0.010766533203423023\n",
      "[step: 2026] loss: 0.010736928321421146\n",
      "[step: 2027] loss: 0.010755101218819618\n",
      "[step: 2028] loss: 0.01073428150266409\n",
      "[step: 2029] loss: 0.010742790065705776\n",
      "[step: 2030] loss: 0.01073185820132494\n",
      "[step: 2031] loss: 0.010729920119047165\n",
      "[step: 2032] loss: 0.010731163434684277\n",
      "[step: 2033] loss: 0.010719634592533112\n",
      "[step: 2034] loss: 0.010728406719863415\n",
      "[step: 2035] loss: 0.010715336538851261\n",
      "[step: 2036] loss: 0.010716794058680534\n",
      "[step: 2037] loss: 0.010716022923588753\n",
      "[step: 2038] loss: 0.010706024244427681\n",
      "[step: 2039] loss: 0.010708197951316833\n",
      "[step: 2040] loss: 0.010705447755753994\n",
      "[step: 2041] loss: 0.010697667486965656\n",
      "[step: 2042] loss: 0.010697508230805397\n",
      "[step: 2043] loss: 0.010697824880480766\n",
      "[step: 2044] loss: 0.010690550319850445\n",
      "[step: 2045] loss: 0.010687743313610554\n",
      "[step: 2046] loss: 0.01068678218871355\n",
      "[step: 2047] loss: 0.010687604546546936\n",
      "[step: 2048] loss: 0.010681988671422005\n",
      "[step: 2049] loss: 0.010677240788936615\n",
      "[step: 2050] loss: 0.010674281977117062\n",
      "[step: 2051] loss: 0.01067238487303257\n",
      "[step: 2052] loss: 0.010673252865672112\n",
      "[step: 2053] loss: 0.0106730367988348\n",
      "[step: 2054] loss: 0.010676872916519642\n",
      "[step: 2055] loss: 0.010685943067073822\n",
      "[step: 2056] loss: 0.010720420628786087\n",
      "[step: 2057] loss: 0.010792192071676254\n",
      "[step: 2058] loss: 0.011066680774092674\n",
      "[step: 2059] loss: 0.011074092239141464\n",
      "[step: 2060] loss: 0.011184677481651306\n",
      "[step: 2061] loss: 0.010727038607001305\n",
      "[step: 2062] loss: 0.011000953614711761\n",
      "[step: 2063] loss: 0.011100449599325657\n",
      "[step: 2064] loss: 0.010861705057322979\n",
      "[step: 2065] loss: 0.010930508375167847\n",
      "[step: 2066] loss: 0.010906743817031384\n",
      "[step: 2067] loss: 0.010801741853356361\n",
      "[step: 2068] loss: 0.010920609347522259\n",
      "[step: 2069] loss: 0.010752574540674686\n",
      "[step: 2070] loss: 0.010884332470595837\n",
      "[step: 2071] loss: 0.010738999582827091\n",
      "[step: 2072] loss: 0.010858213528990746\n",
      "[step: 2073] loss: 0.010712859220802784\n",
      "[step: 2074] loss: 0.010821372270584106\n",
      "[step: 2075] loss: 0.010704346932470798\n",
      "[step: 2076] loss: 0.010802214965224266\n",
      "[step: 2077] loss: 0.01068426575511694\n",
      "[step: 2078] loss: 0.01077296957373619\n",
      "[step: 2079] loss: 0.010680602863430977\n",
      "[step: 2080] loss: 0.010752814821898937\n",
      "[step: 2081] loss: 0.010698645375669003\n",
      "[step: 2082] loss: 0.010713876225054264\n",
      "[step: 2083] loss: 0.010719917714595795\n",
      "[step: 2084] loss: 0.010691852308809757\n",
      "[step: 2085] loss: 0.010681980289518833\n",
      "[step: 2086] loss: 0.010697053745388985\n",
      "[step: 2087] loss: 0.010657488368451595\n",
      "[step: 2088] loss: 0.010686011053621769\n",
      "[step: 2089] loss: 0.010661979205906391\n",
      "[step: 2090] loss: 0.010660562664270401\n",
      "[step: 2091] loss: 0.010660518892109394\n",
      "[step: 2092] loss: 0.010648926720023155\n",
      "[step: 2093] loss: 0.010648191906511784\n",
      "[step: 2094] loss: 0.010647416114807129\n",
      "[step: 2095] loss: 0.010639266110956669\n",
      "[step: 2096] loss: 0.010630928911268711\n",
      "[step: 2097] loss: 0.010640889406204224\n",
      "[step: 2098] loss: 0.010625286027789116\n",
      "[step: 2099] loss: 0.010618279688060284\n",
      "[step: 2100] loss: 0.010613773949444294\n",
      "[step: 2101] loss: 0.010607452131807804\n",
      "[step: 2102] loss: 0.010622547008097172\n",
      "[step: 2103] loss: 0.010695083998143673\n",
      "[step: 2104] loss: 0.011226185597479343\n",
      "[step: 2105] loss: 0.010821141302585602\n",
      "[step: 2106] loss: 0.0108150914311409\n",
      "[step: 2107] loss: 0.011033949442207813\n",
      "[step: 2108] loss: 0.010841493494808674\n",
      "[step: 2109] loss: 0.010854793712496758\n",
      "[step: 2110] loss: 0.01092788390815258\n",
      "[step: 2111] loss: 0.010796431452035904\n",
      "[step: 2112] loss: 0.01077110506594181\n",
      "[step: 2113] loss: 0.010755832307040691\n",
      "[step: 2114] loss: 0.01084235031157732\n",
      "[step: 2115] loss: 0.010655983351171017\n",
      "[step: 2116] loss: 0.01080295816063881\n",
      "[step: 2117] loss: 0.010653209872543812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2118] loss: 0.010735406540334225\n",
      "[step: 2119] loss: 0.0106695257127285\n",
      "[step: 2120] loss: 0.01071641594171524\n",
      "[step: 2121] loss: 0.010623665526509285\n",
      "[step: 2122] loss: 0.010700320824980736\n",
      "[step: 2123] loss: 0.010610477067530155\n",
      "[step: 2124] loss: 0.010663049295544624\n",
      "[step: 2125] loss: 0.010625344701111317\n",
      "[step: 2126] loss: 0.010616067796945572\n",
      "[step: 2127] loss: 0.010638716630637646\n",
      "[step: 2128] loss: 0.010585419833660126\n",
      "[step: 2129] loss: 0.010624030604958534\n",
      "[step: 2130] loss: 0.010622600093483925\n",
      "[step: 2131] loss: 0.011077099479734898\n",
      "[step: 2132] loss: 0.01089561264961958\n",
      "[step: 2133] loss: 0.01081861648708582\n",
      "[step: 2134] loss: 0.010765762999653816\n",
      "[step: 2135] loss: 0.010690567083656788\n",
      "[step: 2136] loss: 0.010787270031869411\n",
      "[step: 2137] loss: 0.01073940098285675\n",
      "[step: 2138] loss: 0.01067804079502821\n",
      "[step: 2139] loss: 0.010775547474622726\n",
      "[step: 2140] loss: 0.01062986720353365\n",
      "[step: 2141] loss: 0.010702951811254025\n",
      "[step: 2142] loss: 0.010713736526668072\n",
      "[step: 2143] loss: 0.010610798373818398\n",
      "[step: 2144] loss: 0.010633482597768307\n",
      "[step: 2145] loss: 0.01062793005257845\n",
      "[step: 2146] loss: 0.010693219490349293\n",
      "[step: 2147] loss: 0.010907975025475025\n",
      "[step: 2148] loss: 0.01066771149635315\n",
      "[step: 2149] loss: 0.010743831284344196\n",
      "[step: 2150] loss: 0.011154425330460072\n",
      "[step: 2151] loss: 0.011119186878204346\n",
      "[step: 2152] loss: 0.010592040605843067\n",
      "[step: 2153] loss: 0.010948632843792439\n",
      "[step: 2154] loss: 0.010775383561849594\n",
      "[step: 2155] loss: 0.010664374567568302\n",
      "[step: 2156] loss: 0.01076265424489975\n",
      "[step: 2157] loss: 0.010584985837340355\n",
      "[step: 2158] loss: 0.010791703127324581\n",
      "[step: 2159] loss: 0.010715044103562832\n",
      "[step: 2160] loss: 0.010691316798329353\n",
      "[step: 2161] loss: 0.010710973292589188\n",
      "[step: 2162] loss: 0.010601147077977657\n",
      "[step: 2163] loss: 0.01069837249815464\n",
      "[step: 2164] loss: 0.010585134848952293\n",
      "[step: 2165] loss: 0.01065017282962799\n",
      "[step: 2166] loss: 0.010583791881799698\n",
      "[step: 2167] loss: 0.010579089634120464\n",
      "[step: 2168] loss: 0.010590420104563236\n",
      "[step: 2169] loss: 0.01067296415567398\n",
      "[step: 2170] loss: 0.010521656833589077\n",
      "[step: 2171] loss: 0.01078207977116108\n",
      "[step: 2172] loss: 0.011610380373895168\n",
      "[step: 2173] loss: 0.011087636463344097\n",
      "[step: 2174] loss: 0.010954467579722404\n",
      "[step: 2175] loss: 0.011079501360654831\n",
      "[step: 2176] loss: 0.011114264838397503\n",
      "[step: 2177] loss: 0.011062901467084885\n",
      "[step: 2178] loss: 0.010691341012716293\n",
      "[step: 2179] loss: 0.010993651114404202\n",
      "[step: 2180] loss: 0.010889606550335884\n",
      "[step: 2181] loss: 0.010853794403374195\n",
      "[step: 2182] loss: 0.010639334097504616\n",
      "[step: 2183] loss: 0.01090200338512659\n",
      "[step: 2184] loss: 0.010699856095016003\n",
      "[step: 2185] loss: 0.01064564660191536\n",
      "[step: 2186] loss: 0.01074281707406044\n",
      "[step: 2187] loss: 0.010712901130318642\n",
      "[step: 2188] loss: 0.010594813153147697\n",
      "[step: 2189] loss: 0.010697981342673302\n",
      "[step: 2190] loss: 0.010627969168126583\n",
      "[step: 2191] loss: 0.01063085999339819\n",
      "[step: 2192] loss: 0.010587076656520367\n",
      "[step: 2193] loss: 0.010645536705851555\n",
      "[step: 2194] loss: 0.01053185947239399\n",
      "[step: 2195] loss: 0.010578753426671028\n",
      "[step: 2196] loss: 0.010556777007877827\n",
      "[step: 2197] loss: 0.010490043088793755\n",
      "[step: 2198] loss: 0.010551944375038147\n",
      "[step: 2199] loss: 0.010474654845893383\n",
      "[step: 2200] loss: 0.010468696244060993\n",
      "[step: 2201] loss: 0.01049735676497221\n",
      "[step: 2202] loss: 0.01041385903954506\n",
      "[step: 2203] loss: 0.010421891696751118\n",
      "[step: 2204] loss: 0.010359077714383602\n",
      "[step: 2205] loss: 0.010375026613473892\n",
      "[step: 2206] loss: 0.010471516288816929\n",
      "[step: 2207] loss: 0.011494508013129234\n",
      "[step: 2208] loss: 0.018222708255052567\n",
      "[step: 2209] loss: 0.01315411739051342\n",
      "[step: 2210] loss: 0.015849998220801353\n",
      "[step: 2211] loss: 0.012261644937098026\n",
      "[step: 2212] loss: 0.015033258125185966\n",
      "[step: 2213] loss: 0.013277867808938026\n",
      "[step: 2214] loss: 0.013165648095309734\n",
      "[step: 2215] loss: 0.01357275154441595\n",
      "[step: 2216] loss: 0.011970320716500282\n",
      "[step: 2217] loss: 0.011993235908448696\n",
      "[step: 2218] loss: 0.012988808564841747\n",
      "[step: 2219] loss: 0.011814350262284279\n",
      "[step: 2220] loss: 0.0117870531976223\n",
      "[step: 2221] loss: 0.012328642420470715\n",
      "[step: 2222] loss: 0.011632299982011318\n",
      "[step: 2223] loss: 0.011594906449317932\n",
      "[step: 2224] loss: 0.01212200429290533\n",
      "[step: 2225] loss: 0.011633116751909256\n",
      "[step: 2226] loss: 0.011409711092710495\n",
      "[step: 2227] loss: 0.011710279621183872\n",
      "[step: 2228] loss: 0.011443662457168102\n",
      "[step: 2229] loss: 0.011141589842736721\n",
      "[step: 2230] loss: 0.011390368454158306\n",
      "[step: 2231] loss: 0.011362774297595024\n",
      "[step: 2232] loss: 0.01112451683729887\n",
      "[step: 2233] loss: 0.011240978725254536\n",
      "[step: 2234] loss: 0.011282378807663918\n",
      "[step: 2235] loss: 0.01110164262354374\n",
      "[step: 2236] loss: 0.011114232242107391\n",
      "[step: 2237] loss: 0.011228881776332855\n",
      "[step: 2238] loss: 0.011138993315398693\n",
      "[step: 2239] loss: 0.01104977447539568\n",
      "[step: 2240] loss: 0.011106613092124462\n",
      "[step: 2241] loss: 0.011095653288066387\n",
      "[step: 2242] loss: 0.011005314067006111\n",
      "[step: 2243] loss: 0.01102233212441206\n",
      "[step: 2244] loss: 0.011061521247029305\n",
      "[step: 2245] loss: 0.010998453013598919\n",
      "[step: 2246] loss: 0.010966930538415909\n",
      "[step: 2247] loss: 0.011002597399055958\n",
      "[step: 2248] loss: 0.010989508591592312\n",
      "[step: 2249] loss: 0.01094775926321745\n",
      "[step: 2250] loss: 0.010952756740152836\n",
      "[step: 2251] loss: 0.010958320461213589\n",
      "[step: 2252] loss: 0.010931777767837048\n",
      "[step: 2253] loss: 0.010927447117865086\n",
      "[step: 2254] loss: 0.010940264910459518\n",
      "[step: 2255] loss: 0.010924561880528927\n",
      "[step: 2256] loss: 0.010906655341386795\n",
      "[step: 2257] loss: 0.010914545506238937\n",
      "[step: 2258] loss: 0.010914048179984093\n",
      "[step: 2259] loss: 0.010892738588154316\n",
      "[step: 2260] loss: 0.010884891264140606\n",
      "[step: 2261] loss: 0.010890341363847256\n",
      "[step: 2262] loss: 0.010880834423005581\n",
      "[step: 2263] loss: 0.010867898352444172\n",
      "[step: 2264] loss: 0.010869228281080723\n",
      "[step: 2265] loss: 0.010867581702768803\n",
      "[step: 2266] loss: 0.010856861248612404\n",
      "[step: 2267] loss: 0.010853004641830921\n",
      "[step: 2268] loss: 0.010852835141122341\n",
      "[step: 2269] loss: 0.010845203883945942\n",
      "[step: 2270] loss: 0.01083899475634098\n",
      "[step: 2271] loss: 0.010838926769793034\n",
      "[step: 2272] loss: 0.010834719054400921\n",
      "[step: 2273] loss: 0.010827689431607723\n",
      "[step: 2274] loss: 0.010825809091329575\n",
      "[step: 2275] loss: 0.01082372572273016\n",
      "[step: 2276] loss: 0.010817278176546097\n",
      "[step: 2277] loss: 0.010813063010573387\n",
      "[step: 2278] loss: 0.010811399668455124\n",
      "[step: 2279] loss: 0.010806967504322529\n",
      "[step: 2280] loss: 0.01080233883112669\n",
      "[step: 2281] loss: 0.010800326243042946\n",
      "[step: 2282] loss: 0.010797466151416302\n",
      "[step: 2283] loss: 0.010793396271765232\n",
      "[step: 2284] loss: 0.01079058088362217\n",
      "[step: 2285] loss: 0.010787940584123135\n",
      "[step: 2286] loss: 0.010784639976918697\n",
      "[step: 2287] loss: 0.01078173890709877\n",
      "[step: 2288] loss: 0.010779041796922684\n",
      "[step: 2289] loss: 0.01077635120600462\n",
      "[step: 2290] loss: 0.010773802176117897\n",
      "[step: 2291] loss: 0.010770966298878193\n",
      "[step: 2292] loss: 0.01076833251863718\n",
      "[step: 2293] loss: 0.010766074061393738\n",
      "[step: 2294] loss: 0.01076335646212101\n",
      "[step: 2295] loss: 0.010760879144072533\n",
      "[step: 2296] loss: 0.010758957825601101\n",
      "[step: 2297] loss: 0.01075652800500393\n",
      "[step: 2298] loss: 0.010754176415503025\n",
      "[step: 2299] loss: 0.01075231283903122\n",
      "[step: 2300] loss: 0.010750088840723038\n",
      "[step: 2301] loss: 0.0107479402795434\n",
      "[step: 2302] loss: 0.010746117681264877\n",
      "[step: 2303] loss: 0.010744091123342514\n",
      "[step: 2304] loss: 0.010742179118096828\n",
      "[step: 2305] loss: 0.010740338824689388\n",
      "[step: 2306] loss: 0.010738417506217957\n",
      "[step: 2307] loss: 0.010736683383584023\n",
      "[step: 2308] loss: 0.010734870098531246\n",
      "[step: 2309] loss: 0.01073310524225235\n",
      "[step: 2310] loss: 0.010731495916843414\n",
      "[step: 2311] loss: 0.010729732923209667\n",
      "[step: 2312] loss: 0.010728088207542896\n",
      "[step: 2313] loss: 0.010726489126682281\n",
      "[step: 2314] loss: 0.010724819265305996\n",
      "[step: 2315] loss: 0.010723249055445194\n",
      "[step: 2316] loss: 0.010721652768552303\n",
      "[step: 2317] loss: 0.010720090940594673\n",
      "[step: 2318] loss: 0.01071851421147585\n",
      "[step: 2319] loss: 0.010716934688389301\n",
      "[step: 2320] loss: 0.010715408250689507\n",
      "[step: 2321] loss: 0.010713824070990086\n",
      "[step: 2322] loss: 0.010712294839322567\n",
      "[step: 2323] loss: 0.01071073953062296\n",
      "[step: 2324] loss: 0.010709177702665329\n",
      "[step: 2325] loss: 0.010707622393965721\n",
      "[step: 2326] loss: 0.010706046596169472\n",
      "[step: 2327] loss: 0.010704489424824715\n",
      "[step: 2328] loss: 0.010702891275286674\n",
      "[step: 2329] loss: 0.010701307095587254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2330] loss: 0.01069969404488802\n",
      "[step: 2331] loss: 0.010698074474930763\n",
      "[step: 2332] loss: 0.010696442797780037\n",
      "[step: 2333] loss: 0.010694792494177818\n",
      "[step: 2334] loss: 0.010693122632801533\n",
      "[step: 2335] loss: 0.010691428557038307\n",
      "[step: 2336] loss: 0.010689725168049335\n",
      "[step: 2337] loss: 0.01068798452615738\n",
      "[step: 2338] loss: 0.010686229914426804\n",
      "[step: 2339] loss: 0.010684450156986713\n",
      "[step: 2340] loss: 0.01068264152854681\n",
      "[step: 2341] loss: 0.010680796578526497\n",
      "[step: 2342] loss: 0.01067893672734499\n",
      "[step: 2343] loss: 0.01067703403532505\n",
      "[step: 2344] loss: 0.010675103403627872\n",
      "[step: 2345] loss: 0.01067313365638256\n",
      "[step: 2346] loss: 0.010671130381524563\n",
      "[step: 2347] loss: 0.01066907960921526\n",
      "[step: 2348] loss: 0.010666999965906143\n",
      "[step: 2349] loss: 0.010664865374565125\n",
      "[step: 2350] loss: 0.010662690736353397\n",
      "[step: 2351] loss: 0.010660468600690365\n",
      "[step: 2352] loss: 0.010658198967576027\n",
      "[step: 2353] loss: 0.010655872523784637\n",
      "[step: 2354] loss: 0.01065349206328392\n",
      "[step: 2355] loss: 0.010651053860783577\n",
      "[step: 2356] loss: 0.01064855232834816\n",
      "[step: 2357] loss: 0.010645988397300243\n",
      "[step: 2358] loss: 0.010643357411026955\n",
      "[step: 2359] loss: 0.010640655644237995\n",
      "[step: 2360] loss: 0.010637874715030193\n",
      "[step: 2361] loss: 0.010635016486048698\n",
      "[step: 2362] loss: 0.01063208281993866\n",
      "[step: 2363] loss: 0.010629053227603436\n",
      "[step: 2364] loss: 0.010625932365655899\n",
      "[step: 2365] loss: 0.01062270998954773\n",
      "[step: 2366] loss: 0.010619387961924076\n",
      "[step: 2367] loss: 0.010615954175591469\n",
      "[step: 2368] loss: 0.010612402111291885\n",
      "[step: 2369] loss: 0.010608725249767303\n",
      "[step: 2370] loss: 0.010604911483824253\n",
      "[step: 2371] loss: 0.010600961744785309\n",
      "[step: 2372] loss: 0.010596854612231255\n",
      "[step: 2373] loss: 0.010592586360871792\n",
      "[step: 2374] loss: 0.010588139295578003\n",
      "[step: 2375] loss: 0.01058350782841444\n",
      "[step: 2376] loss: 0.010578671470284462\n",
      "[step: 2377] loss: 0.010573618113994598\n",
      "[step: 2378] loss: 0.010568329133093357\n",
      "[step: 2379] loss: 0.010562782175838947\n",
      "[step: 2380] loss: 0.010556962341070175\n",
      "[step: 2381] loss: 0.010550843551754951\n",
      "[step: 2382] loss: 0.010544398799538612\n",
      "[step: 2383] loss: 0.01053761038929224\n",
      "[step: 2384] loss: 0.010530439205467701\n",
      "[step: 2385] loss: 0.010522879660129547\n",
      "[step: 2386] loss: 0.010514927096664906\n",
      "[step: 2387] loss: 0.010506830178201199\n",
      "[step: 2388] loss: 0.010500047355890274\n",
      "[step: 2389] loss: 0.010503752157092094\n",
      "[step: 2390] loss: 0.010589317418634892\n",
      "[step: 2391] loss: 0.010907592251896858\n",
      "[step: 2392] loss: 0.012073177844285965\n",
      "[step: 2393] loss: 0.011374812573194504\n",
      "[step: 2394] loss: 0.011419250629842281\n",
      "[step: 2395] loss: 0.011427401565015316\n",
      "[step: 2396] loss: 0.011339724063873291\n",
      "[step: 2397] loss: 0.011283013969659805\n",
      "[step: 2398] loss: 0.011412753723561764\n",
      "[step: 2399] loss: 0.011307033710181713\n",
      "[step: 2400] loss: 0.010954978875815868\n",
      "[step: 2401] loss: 0.011064572259783745\n",
      "[step: 2402] loss: 0.011276068165898323\n",
      "[step: 2403] loss: 0.010979697108268738\n",
      "[step: 2404] loss: 0.010919149033725262\n",
      "[step: 2405] loss: 0.011009540408849716\n",
      "[step: 2406] loss: 0.010944748297333717\n",
      "[step: 2407] loss: 0.010744981467723846\n",
      "[step: 2408] loss: 0.010862471535801888\n",
      "[step: 2409] loss: 0.01093758549541235\n",
      "[step: 2410] loss: 0.010775716044008732\n",
      "[step: 2411] loss: 0.010729352943599224\n",
      "[step: 2412] loss: 0.010796936228871346\n",
      "[step: 2413] loss: 0.010749719105660915\n",
      "[step: 2414] loss: 0.010682650841772556\n",
      "[step: 2415] loss: 0.010747469030320644\n",
      "[step: 2416] loss: 0.010736413300037384\n",
      "[step: 2417] loss: 0.010653209872543812\n",
      "[step: 2418] loss: 0.010673677548766136\n",
      "[step: 2419] loss: 0.010696928016841412\n",
      "[step: 2420] loss: 0.010642004199326038\n",
      "[step: 2421] loss: 0.010638650506734848\n",
      "[step: 2422] loss: 0.010651021264493465\n",
      "[step: 2423] loss: 0.010602773167192936\n",
      "[step: 2424] loss: 0.010597708635032177\n",
      "[step: 2425] loss: 0.010615124367177486\n",
      "[step: 2426] loss: 0.010582287795841694\n",
      "[step: 2427] loss: 0.010559198446571827\n",
      "[step: 2428] loss: 0.010566114448010921\n",
      "[step: 2429] loss: 0.010558385401964188\n",
      "[step: 2430] loss: 0.010533350519835949\n",
      "[step: 2431] loss: 0.010538778267800808\n",
      "[step: 2432] loss: 0.010522205382585526\n",
      "[step: 2433] loss: 0.010499052703380585\n",
      "[step: 2434] loss: 0.010504513047635555\n",
      "[step: 2435] loss: 0.010492509230971336\n",
      "[step: 2436] loss: 0.01046917773783207\n",
      "[step: 2437] loss: 0.010468769818544388\n",
      "[step: 2438] loss: 0.010462851263582706\n",
      "[step: 2439] loss: 0.010445491410791874\n",
      "[step: 2440] loss: 0.010442785918712616\n",
      "[step: 2441] loss: 0.010437451303005219\n",
      "[step: 2442] loss: 0.010422702878713608\n",
      "[step: 2443] loss: 0.010417974554002285\n",
      "[step: 2444] loss: 0.01041344553232193\n",
      "[step: 2445] loss: 0.010406441986560822\n",
      "[step: 2446] loss: 0.010408682748675346\n",
      "[step: 2447] loss: 0.01041407510638237\n",
      "[step: 2448] loss: 0.010428939945995808\n",
      "[step: 2449] loss: 0.010525036603212357\n",
      "[step: 2450] loss: 0.011036590673029423\n",
      "[step: 2451] loss: 0.011853056959807873\n",
      "[step: 2452] loss: 0.011400644667446613\n",
      "[step: 2453] loss: 0.011378618888556957\n",
      "[step: 2454] loss: 0.010777963325381279\n",
      "[step: 2455] loss: 0.011447838507592678\n",
      "[step: 2456] loss: 0.010693199001252651\n",
      "[step: 2457] loss: 0.011187641881406307\n",
      "[step: 2458] loss: 0.010927442461252213\n",
      "[step: 2459] loss: 0.01074440311640501\n",
      "[step: 2460] loss: 0.011320658028125763\n",
      "[step: 2461] loss: 0.011008708737790585\n",
      "[step: 2462] loss: 0.011063453741371632\n",
      "[step: 2463] loss: 0.011232836171984673\n",
      "[step: 2464] loss: 0.011033500544726849\n",
      "[step: 2465] loss: 0.010971506126224995\n",
      "[step: 2466] loss: 0.011097310110926628\n",
      "[step: 2467] loss: 0.011008387431502342\n",
      "[step: 2468] loss: 0.01093374751508236\n",
      "[step: 2469] loss: 0.010960078798234463\n",
      "[step: 2470] loss: 0.010978524573147297\n",
      "[step: 2471] loss: 0.010900765657424927\n",
      "[step: 2472] loss: 0.010868849232792854\n",
      "[step: 2473] loss: 0.010939991101622581\n",
      "[step: 2474] loss: 0.010892446152865887\n",
      "[step: 2475] loss: 0.01084526814520359\n",
      "[step: 2476] loss: 0.010873901657760143\n",
      "[step: 2477] loss: 0.01085958257317543\n",
      "[step: 2478] loss: 0.010819501243531704\n",
      "[step: 2479] loss: 0.010808377526700497\n",
      "[step: 2480] loss: 0.010818356648087502\n",
      "[step: 2481] loss: 0.010785641148686409\n",
      "[step: 2482] loss: 0.01074869092553854\n",
      "[step: 2483] loss: 0.010757028125226498\n",
      "[step: 2484] loss: 0.010736304335296154\n",
      "[step: 2485] loss: 0.010706508532166481\n",
      "[step: 2486] loss: 0.010710746049880981\n",
      "[step: 2487] loss: 0.010690271854400635\n",
      "[step: 2488] loss: 0.01066830474883318\n",
      "[step: 2489] loss: 0.010668228380382061\n",
      "[step: 2490] loss: 0.01065471675246954\n",
      "[step: 2491] loss: 0.010633313097059727\n",
      "[step: 2492] loss: 0.010622928850352764\n",
      "[step: 2493] loss: 0.010610421188175678\n",
      "[step: 2494] loss: 0.010588408447802067\n",
      "[step: 2495] loss: 0.010574587620794773\n",
      "[step: 2496] loss: 0.01056231465190649\n",
      "[step: 2497] loss: 0.010538313537836075\n",
      "[step: 2498] loss: 0.010526925325393677\n",
      "[step: 2499] loss: 0.010504213161766529\n",
      "[step: 2500] loss: 0.010479061864316463\n",
      "[step: 2501] loss: 0.010451991111040115\n",
      "[step: 2502] loss: 0.010423880070447922\n",
      "[step: 2503] loss: 0.010399279184639454\n",
      "[step: 2504] loss: 0.010384662076830864\n",
      "[step: 2505] loss: 0.010626665316522121\n",
      "[step: 2506] loss: 0.010743840597569942\n",
      "[step: 2507] loss: 0.010873368009924889\n",
      "[step: 2508] loss: 0.01084969099611044\n",
      "[step: 2509] loss: 0.010736692696809769\n",
      "[step: 2510] loss: 0.010571187362074852\n",
      "[step: 2511] loss: 0.011081777513027191\n",
      "[step: 2512] loss: 0.010693560354411602\n",
      "[step: 2513] loss: 0.01090532448142767\n",
      "[step: 2514] loss: 0.01035690400749445\n",
      "[step: 2515] loss: 0.01097510289400816\n",
      "[step: 2516] loss: 0.010772437788546085\n",
      "[step: 2517] loss: 0.011322726495563984\n",
      "[step: 2518] loss: 0.011323073878884315\n",
      "[step: 2519] loss: 0.011197664774954319\n",
      "[step: 2520] loss: 0.011072498746216297\n",
      "[step: 2521] loss: 0.010936064645648003\n",
      "[step: 2522] loss: 0.011010931804776192\n",
      "[step: 2523] loss: 0.011021437123417854\n",
      "[step: 2524] loss: 0.011003388091921806\n",
      "[step: 2525] loss: 0.011006997898221016\n",
      "[step: 2526] loss: 0.010937761515378952\n",
      "[step: 2527] loss: 0.010885813273489475\n",
      "[step: 2528] loss: 0.010885600000619888\n",
      "[step: 2529] loss: 0.010882492177188396\n",
      "[step: 2530] loss: 0.010878898203372955\n",
      "[step: 2531] loss: 0.010909505188465118\n",
      "[step: 2532] loss: 0.010926068760454655\n",
      "[step: 2533] loss: 0.010913319885730743\n",
      "[step: 2534] loss: 0.010894524864852428\n",
      "[step: 2535] loss: 0.010882408358156681\n",
      "[step: 2536] loss: 0.010867797769606113\n",
      "[step: 2537] loss: 0.010853026062250137\n",
      "[step: 2538] loss: 0.01084983628243208\n",
      "[step: 2539] loss: 0.010845989920198917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2540] loss: 0.010832546278834343\n",
      "[step: 2541] loss: 0.010823041200637817\n",
      "[step: 2542] loss: 0.010825990699231625\n",
      "[step: 2543] loss: 0.01082866732031107\n",
      "[step: 2544] loss: 0.010829484090209007\n",
      "[step: 2545] loss: 0.010831760242581367\n",
      "[step: 2546] loss: 0.010831165127456188\n",
      "[step: 2547] loss: 0.010828022845089436\n",
      "[step: 2548] loss: 0.010825751349329948\n",
      "[step: 2549] loss: 0.010819880291819572\n",
      "[step: 2550] loss: 0.010813668370246887\n",
      "[step: 2551] loss: 0.010811935178935528\n",
      "[step: 2552] loss: 0.010809505358338356\n",
      "[step: 2553] loss: 0.010804397985339165\n",
      "[step: 2554] loss: 0.010800233110785484\n",
      "[step: 2555] loss: 0.010796796530485153\n",
      "[step: 2556] loss: 0.010793975554406643\n",
      "[step: 2557] loss: 0.010793226771056652\n",
      "[step: 2558] loss: 0.01079174317419529\n",
      "[step: 2559] loss: 0.010789124295115471\n",
      "[step: 2560] loss: 0.010787486098706722\n",
      "[step: 2561] loss: 0.010785036720335484\n",
      "[step: 2562] loss: 0.010781317949295044\n",
      "[step: 2563] loss: 0.010778062976896763\n",
      "[step: 2564] loss: 0.010774961672723293\n",
      "[step: 2565] loss: 0.010772138833999634\n",
      "[step: 2566] loss: 0.010769368149340153\n",
      "[step: 2567] loss: 0.010765586979687214\n",
      "[step: 2568] loss: 0.010762162506580353\n",
      "[step: 2569] loss: 0.010758706368505955\n",
      "[step: 2570] loss: 0.010754190385341644\n",
      "[step: 2571] loss: 0.010749125853180885\n",
      "[step: 2572] loss: 0.010742810554802418\n",
      "[step: 2573] loss: 0.010735525749623775\n",
      "[step: 2574] loss: 0.010727159678936005\n",
      "[step: 2575] loss: 0.01071762852370739\n",
      "[step: 2576] loss: 0.01071417797356844\n",
      "[step: 2577] loss: 0.010795388370752335\n",
      "[step: 2578] loss: 0.010954889468848705\n",
      "[step: 2579] loss: 0.01092854980379343\n",
      "[step: 2580] loss: 0.010925657115876675\n",
      "[step: 2581] loss: 0.01098388247191906\n",
      "[step: 2582] loss: 0.011030949652194977\n",
      "[step: 2583] loss: 0.01091491337865591\n",
      "[step: 2584] loss: 0.01085514947772026\n",
      "[step: 2585] loss: 0.010851600207388401\n",
      "[step: 2586] loss: 0.010878590866923332\n",
      "[step: 2587] loss: 0.01091917883604765\n",
      "[step: 2588] loss: 0.010891018435359001\n",
      "[step: 2589] loss: 0.010838322341442108\n",
      "[step: 2590] loss: 0.010841227136552334\n",
      "[step: 2591] loss: 0.010852315463125706\n",
      "[step: 2592] loss: 0.010861324146389961\n",
      "[step: 2593] loss: 0.010849719867110252\n",
      "[step: 2594] loss: 0.010841225273907185\n",
      "[step: 2595] loss: 0.010832359082996845\n",
      "[step: 2596] loss: 0.010813851840794086\n",
      "[step: 2597] loss: 0.010811078362166882\n",
      "[step: 2598] loss: 0.010812649503350258\n",
      "[step: 2599] loss: 0.010822108946740627\n",
      "[step: 2600] loss: 0.010814578272402287\n",
      "[step: 2601] loss: 0.010810285806655884\n",
      "[step: 2602] loss: 0.010803374461829662\n",
      "[step: 2603] loss: 0.010799907147884369\n",
      "[step: 2604] loss: 0.01079801470041275\n",
      "[step: 2605] loss: 0.010798441246151924\n",
      "[step: 2606] loss: 0.01079481840133667\n",
      "[step: 2607] loss: 0.010785779915750027\n",
      "[step: 2608] loss: 0.010780512355268002\n",
      "[step: 2609] loss: 0.010779875330626965\n",
      "[step: 2610] loss: 0.010779870674014091\n",
      "[step: 2611] loss: 0.010776283219456673\n",
      "[step: 2612] loss: 0.010774497874081135\n",
      "[step: 2613] loss: 0.010771578177809715\n",
      "[step: 2614] loss: 0.010768398642539978\n",
      "[step: 2615] loss: 0.010765427723526955\n",
      "[step: 2616] loss: 0.01076498068869114\n",
      "[step: 2617] loss: 0.010762552730739117\n",
      "[step: 2618] loss: 0.01075915340334177\n",
      "[step: 2619] loss: 0.010756234638392925\n",
      "[step: 2620] loss: 0.010753139853477478\n",
      "[step: 2621] loss: 0.010750540532171726\n",
      "[step: 2622] loss: 0.010748481377959251\n",
      "[step: 2623] loss: 0.010746638290584087\n",
      "[step: 2624] loss: 0.01074332743883133\n",
      "[step: 2625] loss: 0.010742039419710636\n",
      "[step: 2626] loss: 0.010739810764789581\n",
      "[step: 2627] loss: 0.010736360214650631\n",
      "[step: 2628] loss: 0.010734222829341888\n",
      "[step: 2629] loss: 0.010731457732617855\n",
      "[step: 2630] loss: 0.010728270746767521\n",
      "[step: 2631] loss: 0.010725528001785278\n",
      "[step: 2632] loss: 0.010722274892032146\n",
      "[step: 2633] loss: 0.01071925088763237\n",
      "[step: 2634] loss: 0.010715394280850887\n",
      "[step: 2635] loss: 0.010711900889873505\n",
      "[step: 2636] loss: 0.010707610286772251\n",
      "[step: 2637] loss: 0.010702786967158318\n",
      "[step: 2638] loss: 0.010698074474930763\n",
      "[step: 2639] loss: 0.010693109594285488\n",
      "[step: 2640] loss: 0.010702635161578655\n",
      "[step: 2641] loss: 0.010685835033655167\n",
      "[step: 2642] loss: 0.010697784833610058\n",
      "[step: 2643] loss: 0.010643037967383862\n",
      "[step: 2644] loss: 0.010670564137399197\n",
      "[step: 2645] loss: 0.010863788425922394\n",
      "[step: 2646] loss: 0.015160840004682541\n",
      "[step: 2647] loss: 0.015007397159934044\n",
      "[step: 2648] loss: 0.017319699749350548\n",
      "[step: 2649] loss: 0.016207562759518623\n",
      "[step: 2650] loss: 0.016151506453752518\n",
      "[step: 2651] loss: 0.016389736905694008\n",
      "[step: 2652] loss: 0.014746327884495258\n",
      "[step: 2653] loss: 0.013503503985702991\n",
      "[step: 2654] loss: 0.013625817373394966\n",
      "[step: 2655] loss: 0.01366721373051405\n",
      "[step: 2656] loss: 0.012269794009625912\n",
      "[step: 2657] loss: 0.011025436222553253\n",
      "[step: 2658] loss: 0.011124404147267342\n",
      "[step: 2659] loss: 0.011727938428521156\n",
      "[step: 2660] loss: 0.011674251407384872\n",
      "[step: 2661] loss: 0.011910540983080864\n",
      "[step: 2662] loss: 0.012076686136424541\n",
      "[step: 2663] loss: 0.011335914954543114\n",
      "[step: 2664] loss: 0.011149072088301182\n",
      "[step: 2665] loss: 0.011267375200986862\n",
      "[step: 2666] loss: 0.011036007665097713\n",
      "[step: 2667] loss: 0.010928078554570675\n",
      "[step: 2668] loss: 0.01101088710129261\n",
      "[step: 2669] loss: 0.011044983752071857\n",
      "[step: 2670] loss: 0.010967385023832321\n",
      "[step: 2671] loss: 0.010977423749864101\n",
      "[step: 2672] loss: 0.011063775978982449\n",
      "[step: 2673] loss: 0.011003528721630573\n",
      "[step: 2674] loss: 0.010803480632603168\n",
      "[step: 2675] loss: 0.01068906206637621\n",
      "[step: 2676] loss: 0.010701563209295273\n",
      "[step: 2677] loss: 0.01067350059747696\n",
      "[step: 2678] loss: 0.010616163723170757\n",
      "[step: 2679] loss: 0.010648543015122414\n",
      "[step: 2680] loss: 0.01067886222153902\n",
      "[step: 2681] loss: 0.01064229290932417\n",
      "[step: 2682] loss: 0.010606524534523487\n",
      "[step: 2683] loss: 0.010639138519763947\n",
      "[step: 2684] loss: 0.01064259372651577\n",
      "[step: 2685] loss: 0.010582955554127693\n",
      "[step: 2686] loss: 0.010551817715168\n",
      "[step: 2687] loss: 0.010539914481341839\n",
      "[step: 2688] loss: 0.010501829907298088\n",
      "[step: 2689] loss: 0.010469578206539154\n",
      "[step: 2690] loss: 0.010475002229213715\n",
      "[step: 2691] loss: 0.010474810376763344\n",
      "[step: 2692] loss: 0.010452846996486187\n",
      "[step: 2693] loss: 0.010455985553562641\n",
      "[step: 2694] loss: 0.010583385825157166\n",
      "[step: 2695] loss: 0.010491114109754562\n",
      "[step: 2696] loss: 0.01056820061057806\n",
      "[step: 2697] loss: 0.01047496311366558\n",
      "[step: 2698] loss: 0.010617103427648544\n",
      "[step: 2699] loss: 0.010393839329481125\n",
      "[step: 2700] loss: 0.01041040662676096\n",
      "[step: 2701] loss: 0.010806815698742867\n",
      "[step: 2702] loss: 0.010764260776340961\n",
      "[step: 2703] loss: 0.010631476528942585\n",
      "[step: 2704] loss: 0.010999210178852081\n",
      "[step: 2705] loss: 0.010466896928846836\n",
      "[step: 2706] loss: 0.011019095778465271\n",
      "[step: 2707] loss: 0.01117956917732954\n",
      "[step: 2708] loss: 0.011176537722349167\n",
      "[step: 2709] loss: 0.011305308900773525\n",
      "[step: 2710] loss: 0.011111327446997166\n",
      "[step: 2711] loss: 0.011064453981816769\n",
      "[step: 2712] loss: 0.010889571160078049\n",
      "[step: 2713] loss: 0.010500319302082062\n",
      "[step: 2714] loss: 0.011032435111701488\n",
      "[step: 2715] loss: 0.010785105638206005\n",
      "[step: 2716] loss: 0.010559085756540298\n",
      "[step: 2717] loss: 0.010777246206998825\n",
      "[step: 2718] loss: 0.01081925816833973\n",
      "[step: 2719] loss: 0.010733510367572308\n",
      "[step: 2720] loss: 0.010505672544240952\n",
      "[step: 2721] loss: 0.010525746271014214\n",
      "[step: 2722] loss: 0.010664408095180988\n",
      "[step: 2723] loss: 0.010417154058814049\n",
      "[step: 2724] loss: 0.010532792657613754\n",
      "[step: 2725] loss: 0.010575775988399982\n",
      "[step: 2726] loss: 0.010494641959667206\n",
      "[step: 2727] loss: 0.010421845130622387\n",
      "[step: 2728] loss: 0.01052027102559805\n",
      "[step: 2729] loss: 0.010438518598675728\n",
      "[step: 2730] loss: 0.010431202128529549\n",
      "[step: 2731] loss: 0.010472345165908337\n",
      "[step: 2732] loss: 0.010433325543999672\n",
      "[step: 2733] loss: 0.010397323407232761\n",
      "[step: 2734] loss: 0.010437479242682457\n",
      "[step: 2735] loss: 0.010399478487670422\n",
      "[step: 2736] loss: 0.010372471064329147\n",
      "[step: 2737] loss: 0.01040736585855484\n",
      "[step: 2738] loss: 0.0103729497641325\n",
      "[step: 2739] loss: 0.010341957211494446\n",
      "[step: 2740] loss: 0.010373761877417564\n",
      "[step: 2741] loss: 0.01033120695501566\n",
      "[step: 2742] loss: 0.010332642123103142\n",
      "[step: 2743] loss: 0.01032991148531437\n",
      "[step: 2744] loss: 0.010305134579539299\n",
      "[step: 2745] loss: 0.010316196829080582\n",
      "[step: 2746] loss: 0.010283032432198524\n",
      "[step: 2747] loss: 0.010296691209077835\n",
      "[step: 2748] loss: 0.010265136137604713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2749] loss: 0.010279444977641106\n",
      "[step: 2750] loss: 0.010249375365674496\n",
      "[step: 2751] loss: 0.010252044536173344\n",
      "[step: 2752] loss: 0.01023810263723135\n",
      "[step: 2753] loss: 0.010220078751444817\n",
      "[step: 2754] loss: 0.010225153528153896\n",
      "[step: 2755] loss: 0.010208318009972572\n",
      "[step: 2756] loss: 0.010192415677011013\n",
      "[step: 2757] loss: 0.01018612552434206\n",
      "[step: 2758] loss: 0.010192847810685635\n",
      "[step: 2759] loss: 0.010245617479085922\n",
      "[step: 2760] loss: 0.010244961827993393\n",
      "[step: 2761] loss: 0.01040511392056942\n",
      "[step: 2762] loss: 0.010205316357314587\n",
      "[step: 2763] loss: 0.011145560070872307\n",
      "[step: 2764] loss: 0.010403222404420376\n",
      "[step: 2765] loss: 0.010635219514369965\n",
      "[step: 2766] loss: 0.010371781885623932\n",
      "[step: 2767] loss: 0.010636752471327782\n",
      "[step: 2768] loss: 0.010505015030503273\n",
      "[step: 2769] loss: 0.01043907180428505\n",
      "[step: 2770] loss: 0.010592428036034107\n",
      "[step: 2771] loss: 0.010420356877148151\n",
      "[step: 2772] loss: 0.010353228077292442\n",
      "[step: 2773] loss: 0.010437707416713238\n",
      "[step: 2774] loss: 0.010227599181234837\n",
      "[step: 2775] loss: 0.010356346145272255\n",
      "[step: 2776] loss: 0.01027010940015316\n",
      "[step: 2777] loss: 0.010320333763957024\n",
      "[step: 2778] loss: 0.010249094106256962\n",
      "[step: 2779] loss: 0.010318274609744549\n",
      "[step: 2780] loss: 0.010280867107212543\n",
      "[step: 2781] loss: 0.010244261473417282\n",
      "[step: 2782] loss: 0.01022874191403389\n",
      "[step: 2783] loss: 0.01023613940924406\n",
      "[step: 2784] loss: 0.010228860192000866\n",
      "[step: 2785] loss: 0.010193025693297386\n",
      "[step: 2786] loss: 0.010223264805972576\n",
      "[step: 2787] loss: 0.01018782053142786\n",
      "[step: 2788] loss: 0.01021424401551485\n",
      "[step: 2789] loss: 0.010159624740481377\n",
      "[step: 2790] loss: 0.010193709284067154\n",
      "[step: 2791] loss: 0.010163618251681328\n",
      "[step: 2792] loss: 0.010157562792301178\n",
      "[step: 2793] loss: 0.010171487927436829\n",
      "[step: 2794] loss: 0.01012306660413742\n",
      "[step: 2795] loss: 0.010127834044396877\n",
      "[step: 2796] loss: 0.010138853453099728\n",
      "[step: 2797] loss: 0.010133013129234314\n",
      "[step: 2798] loss: 0.01008487306535244\n",
      "[step: 2799] loss: 0.010110084898769855\n",
      "[step: 2800] loss: 0.0101768234744668\n",
      "[step: 2801] loss: 0.01005980372428894\n",
      "[step: 2802] loss: 0.01018157135695219\n",
      "[step: 2803] loss: 0.010346141643822193\n",
      "[step: 2804] loss: 0.010322438552975655\n",
      "[step: 2805] loss: 0.010149074718356133\n",
      "[step: 2806] loss: 0.01005084440112114\n",
      "[step: 2807] loss: 0.010111289098858833\n",
      "[step: 2808] loss: 0.010096559301018715\n",
      "[step: 2809] loss: 0.010086621157824993\n",
      "[step: 2810] loss: 0.010026028379797935\n",
      "[step: 2811] loss: 0.010007809847593307\n",
      "[step: 2812] loss: 0.009995637461543083\n",
      "[step: 2813] loss: 0.009996457025408745\n",
      "[step: 2814] loss: 0.010092065669596195\n",
      "[step: 2815] loss: 0.010402080602943897\n",
      "[step: 2816] loss: 0.01068783737719059\n",
      "[step: 2817] loss: 0.01092731487005949\n",
      "[step: 2818] loss: 0.010867578908801079\n",
      "[step: 2819] loss: 0.010819325223565102\n",
      "[step: 2820] loss: 0.010876952670514584\n",
      "[step: 2821] loss: 0.010778043419122696\n",
      "[step: 2822] loss: 0.0107581140473485\n",
      "[step: 2823] loss: 0.01077617984265089\n",
      "[step: 2824] loss: 0.010758576914668083\n",
      "[step: 2825] loss: 0.010734240524470806\n",
      "[step: 2826] loss: 0.010699070058763027\n",
      "[step: 2827] loss: 0.010663134045898914\n",
      "[step: 2828] loss: 0.01062918920069933\n",
      "[step: 2829] loss: 0.010568935424089432\n",
      "[step: 2830] loss: 0.01045102160423994\n",
      "[step: 2831] loss: 0.01035256590694189\n",
      "[step: 2832] loss: 0.010315367951989174\n",
      "[step: 2833] loss: 0.010313304141163826\n",
      "[step: 2834] loss: 0.010375302284955978\n",
      "[step: 2835] loss: 0.01074754074215889\n",
      "[step: 2836] loss: 0.010574977844953537\n",
      "[step: 2837] loss: 0.010209912434220314\n",
      "[step: 2838] loss: 0.010799716226756573\n",
      "[step: 2839] loss: 0.010243221186101437\n",
      "[step: 2840] loss: 0.01052842941135168\n",
      "[step: 2841] loss: 0.01049282867461443\n",
      "[step: 2842] loss: 0.010504452511668205\n",
      "[step: 2843] loss: 0.010389483533799648\n",
      "[step: 2844] loss: 0.0105214174836874\n",
      "[step: 2845] loss: 0.010447475127875805\n",
      "[step: 2846] loss: 0.010546534322202206\n",
      "[step: 2847] loss: 0.010429595597088337\n",
      "[step: 2848] loss: 0.010501625947654247\n",
      "[step: 2849] loss: 0.010386008769273758\n",
      "[step: 2850] loss: 0.010448080487549305\n",
      "[step: 2851] loss: 0.010349499993026257\n",
      "[step: 2852] loss: 0.010447061620652676\n",
      "[step: 2853] loss: 0.010391183197498322\n",
      "[step: 2854] loss: 0.010425415821373463\n",
      "[step: 2855] loss: 0.010299568064510822\n",
      "[step: 2856] loss: 0.010534229688346386\n",
      "[step: 2857] loss: 0.010309201665222645\n",
      "[step: 2858] loss: 0.010375550016760826\n",
      "[step: 2859] loss: 0.010279275476932526\n",
      "[step: 2860] loss: 0.010411680676043034\n",
      "[step: 2861] loss: 0.010288181714713573\n",
      "[step: 2862] loss: 0.01033311802893877\n",
      "[step: 2863] loss: 0.010218844749033451\n",
      "[step: 2864] loss: 0.010334121063351631\n",
      "[step: 2865] loss: 0.01035305391997099\n",
      "[step: 2866] loss: 0.010403717868030071\n",
      "[step: 2867] loss: 0.010202497243881226\n",
      "[step: 2868] loss: 0.010525409132242203\n",
      "[step: 2869] loss: 0.010308429598808289\n",
      "[step: 2870] loss: 0.01044383179396391\n",
      "[step: 2871] loss: 0.010268131271004677\n",
      "[step: 2872] loss: 0.010428802110254765\n",
      "[step: 2873] loss: 0.010191353037953377\n",
      "[step: 2874] loss: 0.010293925181031227\n",
      "[step: 2875] loss: 0.010256437584757805\n",
      "[step: 2876] loss: 0.01020542811602354\n",
      "[step: 2877] loss: 0.010197672061622143\n",
      "[step: 2878] loss: 0.010218721814453602\n",
      "[step: 2879] loss: 0.010206599719822407\n",
      "[step: 2880] loss: 0.01019689068198204\n",
      "[step: 2881] loss: 0.010130527429282665\n",
      "[step: 2882] loss: 0.010167357511818409\n",
      "[step: 2883] loss: 0.010116376914083958\n",
      "[step: 2884] loss: 0.010139791294932365\n",
      "[step: 2885] loss: 0.010149159468710423\n",
      "[step: 2886] loss: 0.010098880156874657\n",
      "[step: 2887] loss: 0.010186953470110893\n",
      "[step: 2888] loss: 0.010163589380681515\n",
      "[step: 2889] loss: 0.010138721205294132\n",
      "[step: 2890] loss: 0.010175593197345734\n",
      "[step: 2891] loss: 0.010084574110805988\n",
      "[step: 2892] loss: 0.010078190825879574\n",
      "[step: 2893] loss: 0.010106123983860016\n",
      "[step: 2894] loss: 0.010065105743706226\n",
      "[step: 2895] loss: 0.01001620851457119\n",
      "[step: 2896] loss: 0.01015421748161316\n",
      "[step: 2897] loss: 0.010200800374150276\n",
      "[step: 2898] loss: 0.010206983424723148\n",
      "[step: 2899] loss: 0.009995857253670692\n",
      "[step: 2900] loss: 0.010045683942735195\n",
      "[step: 2901] loss: 0.01018117181956768\n",
      "[step: 2902] loss: 0.010174878872931004\n",
      "[step: 2903] loss: 0.00997637864202261\n",
      "[step: 2904] loss: 0.010146185755729675\n",
      "[step: 2905] loss: 0.010152352973818779\n",
      "[step: 2906] loss: 0.010284866206347942\n",
      "[step: 2907] loss: 0.00999512616544962\n",
      "[step: 2908] loss: 0.010471590794622898\n",
      "[step: 2909] loss: 0.010008311830461025\n",
      "[step: 2910] loss: 0.010225968435406685\n",
      "[step: 2911] loss: 0.010049165226519108\n",
      "[step: 2912] loss: 0.010186334140598774\n",
      "[step: 2913] loss: 0.010114622302353382\n",
      "[step: 2914] loss: 0.010122218169271946\n",
      "[step: 2915] loss: 0.010184738785028458\n",
      "[step: 2916] loss: 0.010010973550379276\n",
      "[step: 2917] loss: 0.0100605683401227\n",
      "[step: 2918] loss: 0.009982749819755554\n",
      "[step: 2919] loss: 0.010089186951518059\n",
      "[step: 2920] loss: 0.009997256100177765\n",
      "[step: 2921] loss: 0.010044426657259464\n",
      "[step: 2922] loss: 0.00994213204830885\n",
      "[step: 2923] loss: 0.010050992481410503\n",
      "[step: 2924] loss: 0.009908911772072315\n",
      "[step: 2925] loss: 0.009981564246118069\n",
      "[step: 2926] loss: 0.009919536300003529\n",
      "[step: 2927] loss: 0.009953209199011326\n",
      "[step: 2928] loss: 0.00987775344401598\n",
      "[step: 2929] loss: 0.009932011365890503\n",
      "[step: 2930] loss: 0.009920050390064716\n",
      "[step: 2931] loss: 0.009926265105605125\n",
      "[step: 2932] loss: 0.00987029355019331\n",
      "[step: 2933] loss: 0.009876522235572338\n",
      "[step: 2934] loss: 0.009931955486536026\n",
      "[step: 2935] loss: 0.010020577348768711\n",
      "[step: 2936] loss: 0.00982215628027916\n",
      "[step: 2937] loss: 0.010271812789142132\n",
      "[step: 2938] loss: 0.010566407814621925\n",
      "[step: 2939] loss: 0.010714191943407059\n",
      "[step: 2940] loss: 0.010940739884972572\n",
      "[step: 2941] loss: 0.01097047794610262\n",
      "[step: 2942] loss: 0.010762853547930717\n",
      "[step: 2943] loss: 0.010147017426788807\n",
      "[step: 2944] loss: 0.011099457740783691\n",
      "[step: 2945] loss: 0.010523006319999695\n",
      "[step: 2946] loss: 0.010927757248282433\n",
      "[step: 2947] loss: 0.010989242233335972\n",
      "[step: 2948] loss: 0.010746627114713192\n",
      "[step: 2949] loss: 0.011050964705646038\n",
      "[step: 2950] loss: 0.011169376783072948\n",
      "[step: 2951] loss: 0.010772038251161575\n",
      "[step: 2952] loss: 0.010667052119970322\n",
      "[step: 2953] loss: 0.010743863880634308\n",
      "[step: 2954] loss: 0.01069449819624424\n",
      "[step: 2955] loss: 0.010457740165293217\n",
      "[step: 2956] loss: 0.01037598680704832\n",
      "[step: 2957] loss: 0.010456361807882786\n",
      "[step: 2958] loss: 0.01029379852116108\n",
      "[step: 2959] loss: 0.01042422279715538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2960] loss: 0.010416646488010883\n",
      "[step: 2961] loss: 0.010351632721722126\n",
      "[step: 2962] loss: 0.010172456502914429\n",
      "[step: 2963] loss: 0.010535042732954025\n",
      "[step: 2964] loss: 0.010162100195884705\n",
      "[step: 2965] loss: 0.010243219323456287\n",
      "[step: 2966] loss: 0.010263314470648766\n",
      "[step: 2967] loss: 0.010232635773718357\n",
      "[step: 2968] loss: 0.010129437781870365\n",
      "[step: 2969] loss: 0.010175175033509731\n",
      "[step: 2970] loss: 0.010168894194066525\n",
      "[step: 2971] loss: 0.010096552781760693\n",
      "[step: 2972] loss: 0.010073130950331688\n",
      "[step: 2973] loss: 0.010104544460773468\n",
      "[step: 2974] loss: 0.010032333433628082\n",
      "[step: 2975] loss: 0.010003202594816685\n",
      "[step: 2976] loss: 0.010037701576948166\n",
      "[step: 2977] loss: 0.009971052408218384\n",
      "[step: 2978] loss: 0.00996248796582222\n",
      "[step: 2979] loss: 0.00995966512709856\n",
      "[step: 2980] loss: 0.009917616844177246\n",
      "[step: 2981] loss: 0.009908345527946949\n",
      "[step: 2982] loss: 0.009902860969305038\n",
      "[step: 2983] loss: 0.009875481016933918\n",
      "[step: 2984] loss: 0.009862566366791725\n",
      "[step: 2985] loss: 0.009866737760603428\n",
      "[step: 2986] loss: 0.009822694584727287\n",
      "[step: 2987] loss: 0.009810654446482658\n",
      "[step: 2988] loss: 0.009826877154409885\n",
      "[step: 2989] loss: 0.009811440482735634\n",
      "[step: 2990] loss: 0.009835651144385338\n",
      "[step: 2991] loss: 0.00982595793902874\n",
      "[step: 2992] loss: 0.009863351471722126\n",
      "[step: 2993] loss: 0.009789416566491127\n",
      "[step: 2994] loss: 0.0097670191898942\n",
      "[step: 2995] loss: 0.009733390063047409\n",
      "[step: 2996] loss: 0.009736455045640469\n",
      "[step: 2997] loss: 0.009755365550518036\n",
      "[step: 2998] loss: 0.009747027419507504\n",
      "[step: 2999] loss: 0.009805544279515743\n",
      "[step: 3000] loss: 0.009747274219989777\n",
      "[step: 3001] loss: 0.009768868796527386\n",
      "[step: 3002] loss: 0.00974878016859293\n",
      "[step: 3003] loss: 0.009879812598228455\n",
      "[step: 3004] loss: 0.009766148403286934\n",
      "[step: 3005] loss: 0.009879781864583492\n",
      "[step: 3006] loss: 0.0096982317045331\n",
      "[step: 3007] loss: 0.009711626917123795\n",
      "[step: 3008] loss: 0.009800535626709461\n",
      "[step: 3009] loss: 0.009892335161566734\n",
      "[step: 3010] loss: 0.009785808622837067\n",
      "[step: 3011] loss: 0.010433267802000046\n",
      "[step: 3012] loss: 0.010353774763643742\n",
      "[step: 3013] loss: 0.01062373910099268\n",
      "[step: 3014] loss: 0.010510414838790894\n",
      "[step: 3015] loss: 0.010634995996952057\n",
      "[step: 3016] loss: 0.010621347464621067\n",
      "[step: 3017] loss: 0.010503384284675121\n",
      "[step: 3018] loss: 0.010587248019874096\n",
      "[step: 3019] loss: 0.011143233627080917\n",
      "[step: 3020] loss: 0.01032310537993908\n",
      "[step: 3021] loss: 0.013698864728212357\n",
      "[step: 3022] loss: 0.01611999049782753\n",
      "[step: 3023] loss: 0.018405377864837646\n",
      "[step: 3024] loss: 0.016064124181866646\n",
      "[step: 3025] loss: 0.013907594606280327\n",
      "[step: 3026] loss: 0.012102413922548294\n",
      "[step: 3027] loss: 0.011882724240422249\n",
      "[step: 3028] loss: 0.013651243411004543\n",
      "[step: 3029] loss: 0.014675642363727093\n",
      "[step: 3030] loss: 0.014931456185877323\n",
      "[step: 3031] loss: 0.015004671178758144\n",
      "[step: 3032] loss: 0.013537229038774967\n",
      "[step: 3033] loss: 0.012357363477349281\n",
      "[step: 3034] loss: 0.011603250168263912\n",
      "[step: 3035] loss: 0.011104725301265717\n",
      "[step: 3036] loss: 0.011645696125924587\n",
      "[step: 3037] loss: 0.012232402339577675\n",
      "[step: 3038] loss: 0.012421972118318081\n",
      "[step: 3039] loss: 0.012536191381514072\n",
      "[step: 3040] loss: 0.01211846899241209\n",
      "[step: 3041] loss: 0.011499463580548763\n",
      "[step: 3042] loss: 0.011203969828784466\n",
      "[step: 3043] loss: 0.010943397879600525\n",
      "[step: 3044] loss: 0.010951279662549496\n",
      "[step: 3045] loss: 0.011155327782034874\n",
      "[step: 3046] loss: 0.011180881410837173\n",
      "[step: 3047] loss: 0.011292243376374245\n",
      "[step: 3048] loss: 0.011260936968028545\n",
      "[step: 3049] loss: 0.011074533686041832\n",
      "[step: 3050] loss: 0.010970001108944416\n",
      "[step: 3051] loss: 0.010755063034594059\n",
      "[step: 3052] loss: 0.010642069391906261\n",
      "[step: 3053] loss: 0.010639472864568233\n",
      "[step: 3054] loss: 0.010654934681952\n",
      "[step: 3055] loss: 0.01072240062057972\n",
      "[step: 3056] loss: 0.010771386325359344\n",
      "[step: 3057] loss: 0.010723237879574299\n",
      "[step: 3058] loss: 0.010650960728526115\n",
      "[step: 3059] loss: 0.01057776715606451\n",
      "[step: 3060] loss: 0.010506223887205124\n",
      "[step: 3061] loss: 0.010484046302735806\n",
      "[step: 3062] loss: 0.010455404408276081\n",
      "[step: 3063] loss: 0.01043794211000204\n",
      "[step: 3064] loss: 0.01043801847845316\n",
      "[step: 3065] loss: 0.010430344380438328\n",
      "[step: 3066] loss: 0.010418005287647247\n",
      "[step: 3067] loss: 0.01039169728755951\n",
      "[step: 3068] loss: 0.010356216691434383\n",
      "[step: 3069] loss: 0.010458496399223804\n",
      "[step: 3070] loss: 0.010437679477036\n",
      "[step: 3071] loss: 0.010247328318655491\n",
      "[step: 3072] loss: 0.01058975700289011\n",
      "[step: 3073] loss: 0.010677795857191086\n",
      "[step: 3074] loss: 0.010463276877999306\n",
      "[step: 3075] loss: 0.010428211651742458\n",
      "[step: 3076] loss: 0.010483580641448498\n",
      "[step: 3077] loss: 0.010881086811423302\n",
      "[step: 3078] loss: 0.0107949860394001\n",
      "[step: 3079] loss: 0.010919605381786823\n",
      "[step: 3080] loss: 0.010776265524327755\n",
      "[step: 3081] loss: 0.010783476755023003\n",
      "[step: 3082] loss: 0.010463180020451546\n",
      "[step: 3083] loss: 0.010341760702431202\n",
      "[step: 3084] loss: 0.010315530002117157\n",
      "[step: 3085] loss: 0.010389285162091255\n",
      "[step: 3086] loss: 0.010421080514788628\n",
      "[step: 3087] loss: 0.0104189058765769\n",
      "[step: 3088] loss: 0.010380995459854603\n",
      "[step: 3089] loss: 0.010299183428287506\n",
      "[step: 3090] loss: 0.010307095013558865\n",
      "[step: 3091] loss: 0.010356747545301914\n",
      "[step: 3092] loss: 0.010377735830843449\n",
      "[step: 3093] loss: 0.010647390969097614\n",
      "[step: 3094] loss: 0.010647918097674847\n",
      "[step: 3095] loss: 0.010609813965857029\n",
      "[step: 3096] loss: 0.010649141855537891\n",
      "[step: 3097] loss: 0.010592793114483356\n",
      "[step: 3098] loss: 0.010600103996694088\n",
      "[step: 3099] loss: 0.010579314082860947\n",
      "[step: 3100] loss: 0.010407054796814919\n",
      "[step: 3101] loss: 0.010403403080999851\n",
      "[step: 3102] loss: 0.010559114627540112\n",
      "[step: 3103] loss: 0.010382335633039474\n",
      "[step: 3104] loss: 0.01055674534291029\n",
      "[step: 3105] loss: 0.010582732036709785\n",
      "[step: 3106] loss: 0.010591668076813221\n",
      "[step: 3107] loss: 0.010588450357317924\n",
      "[step: 3108] loss: 0.010564220137894154\n",
      "[step: 3109] loss: 0.010566730983555317\n",
      "[step: 3110] loss: 0.010520419105887413\n",
      "[step: 3111] loss: 0.010372893884778023\n",
      "[step: 3112] loss: 0.010419458150863647\n",
      "[step: 3113] loss: 0.010442718863487244\n",
      "[step: 3114] loss: 0.010377496480941772\n",
      "[step: 3115] loss: 0.010361750610172749\n",
      "[step: 3116] loss: 0.010392224416136742\n",
      "[step: 3117] loss: 0.010350562632083893\n",
      "[step: 3118] loss: 0.010347693227231503\n",
      "[step: 3119] loss: 0.010358530096709728\n",
      "[step: 3120] loss: 0.010319004766643047\n",
      "[step: 3121] loss: 0.010305912233889103\n",
      "[step: 3122] loss: 0.010300854220986366\n",
      "[step: 3123] loss: 0.010300492867827415\n",
      "[step: 3124] loss: 0.01025286316871643\n",
      "[step: 3125] loss: 0.010390633717179298\n",
      "[step: 3126] loss: 0.010316422209143639\n",
      "[step: 3127] loss: 0.010341496206820011\n",
      "[step: 3128] loss: 0.010351551696658134\n",
      "[step: 3129] loss: 0.010348924435675144\n",
      "[step: 3130] loss: 0.0103186946362257\n",
      "[step: 3131] loss: 0.010390074923634529\n",
      "[step: 3132] loss: 0.010316062718629837\n",
      "[step: 3133] loss: 0.010292330756783485\n",
      "[step: 3134] loss: 0.0106055261567235\n",
      "[step: 3135] loss: 0.010682530701160431\n",
      "[step: 3136] loss: 0.010683294385671616\n",
      "[step: 3137] loss: 0.010794693604111671\n",
      "[step: 3138] loss: 0.010648559778928757\n",
      "[step: 3139] loss: 0.010685574263334274\n",
      "[step: 3140] loss: 0.010419486090540886\n",
      "[step: 3141] loss: 0.010354781523346901\n",
      "[step: 3142] loss: 0.010228108614683151\n",
      "[step: 3143] loss: 0.01023787260055542\n",
      "[step: 3144] loss: 0.010332870297133923\n",
      "[step: 3145] loss: 0.01031458843499422\n",
      "[step: 3146] loss: 0.010246277786791325\n",
      "[step: 3147] loss: 0.01019939687103033\n",
      "[step: 3148] loss: 0.010212145745754242\n",
      "[step: 3149] loss: 0.010274332948029041\n",
      "[step: 3150] loss: 0.010349168442189693\n",
      "[step: 3151] loss: 0.010336573235690594\n",
      "[step: 3152] loss: 0.010630303993821144\n",
      "[step: 3153] loss: 0.01065233163535595\n",
      "[step: 3154] loss: 0.010407201945781708\n",
      "[step: 3155] loss: 0.010291078127920628\n",
      "[step: 3156] loss: 0.01026588212698698\n",
      "[step: 3157] loss: 0.010365443304181099\n",
      "[step: 3158] loss: 0.010609515942633152\n",
      "[step: 3159] loss: 0.010561643168330193\n",
      "[step: 3160] loss: 0.010191056877374649\n",
      "[step: 3161] loss: 0.011349028907716274\n",
      "[step: 3162] loss: 0.010387981310486794\n",
      "[step: 3163] loss: 0.010639926418662071\n",
      "[step: 3164] loss: 0.010770685039460659\n",
      "[step: 3165] loss: 0.010864476673305035\n",
      "[step: 3166] loss: 0.0113981943577528\n",
      "[step: 3167] loss: 0.011304421350359917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3168] loss: 0.011403180658817291\n",
      "[step: 3169] loss: 0.011100083589553833\n",
      "[step: 3170] loss: 0.011061910539865494\n",
      "[step: 3171] loss: 0.011104687117040157\n",
      "[step: 3172] loss: 0.011054939590394497\n",
      "[step: 3173] loss: 0.01102613378316164\n",
      "[step: 3174] loss: 0.010999937541782856\n",
      "[step: 3175] loss: 0.010985628701746464\n",
      "[step: 3176] loss: 0.010878672823309898\n",
      "[step: 3177] loss: 0.010889780707657337\n",
      "[step: 3178] loss: 0.01087625976651907\n",
      "[step: 3179] loss: 0.01091922726482153\n",
      "[step: 3180] loss: 0.010895458981394768\n",
      "[step: 3181] loss: 0.010858096182346344\n",
      "[step: 3182] loss: 0.01194697618484497\n",
      "[step: 3183] loss: 0.011071794666349888\n",
      "[step: 3184] loss: 0.011020771227777004\n",
      "[step: 3185] loss: 0.010987543500959873\n",
      "[step: 3186] loss: 0.011051998473703861\n",
      "[step: 3187] loss: 0.010911474004387856\n",
      "[step: 3188] loss: 0.011089245788753033\n",
      "[step: 3189] loss: 0.01102586928755045\n",
      "[step: 3190] loss: 0.011003951542079449\n",
      "[step: 3191] loss: 0.011048147454857826\n",
      "[step: 3192] loss: 0.01096261478960514\n",
      "[step: 3193] loss: 0.010995238088071346\n",
      "[step: 3194] loss: 0.010970472358167171\n",
      "[step: 3195] loss: 0.010963454842567444\n",
      "[step: 3196] loss: 0.010991008952260017\n",
      "[step: 3197] loss: 0.010946550406515598\n",
      "[step: 3198] loss: 0.010959281586110592\n",
      "[step: 3199] loss: 0.010950806550681591\n",
      "[step: 3200] loss: 0.010937467217445374\n",
      "[step: 3201] loss: 0.010959469713270664\n",
      "[step: 3202] loss: 0.010933850891888142\n",
      "[step: 3203] loss: 0.010939053259789944\n",
      "[step: 3204] loss: 0.010934259742498398\n",
      "[step: 3205] loss: 0.010923841036856174\n",
      "[step: 3206] loss: 0.010936949402093887\n",
      "[step: 3207] loss: 0.010923091322183609\n",
      "[step: 3208] loss: 0.010924612171947956\n",
      "[step: 3209] loss: 0.010922200977802277\n",
      "[step: 3210] loss: 0.010914093814790249\n",
      "[step: 3211] loss: 0.01092271413654089\n",
      "[step: 3212] loss: 0.010915051214396954\n",
      "[step: 3213] loss: 0.010917368344962597\n",
      "[step: 3214] loss: 0.010913524776697159\n",
      "[step: 3215] loss: 0.010907500050961971\n",
      "[step: 3216] loss: 0.010909945704042912\n",
      "[step: 3217] loss: 0.010903309099376202\n",
      "[step: 3218] loss: 0.010904652066528797\n",
      "[step: 3219] loss: 0.01090021152049303\n",
      "[step: 3220] loss: 0.010897918604314327\n",
      "[step: 3221] loss: 0.010898170992732048\n",
      "[step: 3222] loss: 0.010894494131207466\n",
      "[step: 3223] loss: 0.010895716957747936\n",
      "[step: 3224] loss: 0.010891514830291271\n",
      "[step: 3225] loss: 0.010891275480389595\n",
      "[step: 3226] loss: 0.010889151133596897\n",
      "[step: 3227] loss: 0.01088783796876669\n",
      "[step: 3228] loss: 0.01088724471628666\n",
      "[step: 3229] loss: 0.010884429328143597\n",
      "[step: 3230] loss: 0.01088409498333931\n",
      "[step: 3231] loss: 0.010881460271775723\n",
      "[step: 3232] loss: 0.01088120136409998\n",
      "[step: 3233] loss: 0.01087928842753172\n",
      "[step: 3234] loss: 0.010878156870603561\n",
      "[step: 3235] loss: 0.010876721702516079\n",
      "[step: 3236] loss: 0.010875198058784008\n",
      "[step: 3237] loss: 0.010874483734369278\n",
      "[step: 3238] loss: 0.010872824117541313\n",
      "[step: 3239] loss: 0.01087212935090065\n",
      "[step: 3240] loss: 0.010870401747524738\n",
      "[step: 3241] loss: 0.010869687423110008\n",
      "[step: 3242] loss: 0.010868334211409092\n",
      "[step: 3243] loss: 0.010867506265640259\n",
      "[step: 3244] loss: 0.01086629368364811\n",
      "[step: 3245] loss: 0.010865236632525921\n",
      "[step: 3246] loss: 0.01086424570530653\n",
      "[step: 3247] loss: 0.010863179340958595\n",
      "[step: 3248] loss: 0.010862327180802822\n",
      "[step: 3249] loss: 0.010861165821552277\n",
      "[step: 3250] loss: 0.010860313661396503\n",
      "[step: 3251] loss: 0.01085919700562954\n",
      "[step: 3252] loss: 0.010858410969376564\n",
      "[step: 3253] loss: 0.010857333429157734\n",
      "[step: 3254] loss: 0.010856502689421177\n",
      "[step: 3255] loss: 0.010855453088879585\n",
      "[step: 3256] loss: 0.010854636318981647\n",
      "[step: 3257] loss: 0.010853664949536324\n",
      "[step: 3258] loss: 0.010852831415832043\n",
      "[step: 3259] loss: 0.010851874016225338\n",
      "[step: 3260] loss: 0.010851028375327587\n",
      "[step: 3261] loss: 0.010850121267139912\n",
      "[step: 3262] loss: 0.01084928959608078\n",
      "[step: 3263] loss: 0.010848397389054298\n",
      "[step: 3264] loss: 0.01084755826741457\n",
      "[step: 3265] loss: 0.010846682824194431\n",
      "[step: 3266] loss: 0.010845858603715897\n",
      "[step: 3267] loss: 0.01084500178694725\n",
      "[step: 3268] loss: 0.010844176635146141\n",
      "[step: 3269] loss: 0.010843319818377495\n",
      "[step: 3270] loss: 0.010842501185834408\n",
      "[step: 3271] loss: 0.010841656476259232\n",
      "[step: 3272] loss: 0.010840842500329018\n",
      "[step: 3273] loss: 0.010839997790753841\n",
      "[step: 3274] loss: 0.010839186608791351\n",
      "[step: 3275] loss: 0.01083835307508707\n",
      "[step: 3276] loss: 0.010837546549737453\n",
      "[step: 3277] loss: 0.010836715810000896\n",
      "[step: 3278] loss: 0.010835906490683556\n",
      "[step: 3279] loss: 0.010835081338882446\n",
      "[step: 3280] loss: 0.010834276676177979\n",
      "[step: 3281] loss: 0.010833454318344593\n",
      "[step: 3282] loss: 0.010832646861672401\n",
      "[step: 3283] loss: 0.010831831954419613\n",
      "[step: 3284] loss: 0.010831020772457123\n",
      "[step: 3285] loss: 0.010830210521817207\n",
      "[step: 3286] loss: 0.010829396545886993\n",
      "[step: 3287] loss: 0.010828584432601929\n",
      "[step: 3288] loss: 0.010827765800058842\n",
      "[step: 3289] loss: 0.010826957412064075\n",
      "[step: 3290] loss: 0.010826138779520988\n",
      "[step: 3291] loss: 0.01082532200962305\n",
      "[step: 3292] loss: 0.010824501514434814\n",
      "[step: 3293] loss: 0.010823685675859451\n",
      "[step: 3294] loss: 0.010822862386703491\n",
      "[step: 3295] loss: 0.010822039097547531\n",
      "[step: 3296] loss: 0.010821213945746422\n",
      "[step: 3297] loss: 0.010820389725267887\n",
      "[step: 3298] loss: 0.010819565504789352\n",
      "[step: 3299] loss: 0.01081873569637537\n",
      "[step: 3300] loss: 0.010817906819283962\n",
      "[step: 3301] loss: 0.01081707701086998\n",
      "[step: 3302] loss: 0.01081624161452055\n",
      "[step: 3303] loss: 0.010815410874783993\n",
      "[step: 3304] loss: 0.010814573615789413\n",
      "[step: 3305] loss: 0.010813738219439983\n",
      "[step: 3306] loss: 0.010812896303832531\n",
      "[step: 3307] loss: 0.010812059044837952\n",
      "[step: 3308] loss: 0.0108112134039402\n",
      "[step: 3309] loss: 0.01081036776304245\n",
      "[step: 3310] loss: 0.01080952025949955\n",
      "[step: 3311] loss: 0.01080867275595665\n",
      "[step: 3312] loss: 0.010807820595800877\n",
      "[step: 3313] loss: 0.010806969366967678\n",
      "[step: 3314] loss: 0.010806111618876457\n",
      "[step: 3315] loss: 0.010805254802107811\n",
      "[step: 3316] loss: 0.010804394260048866\n",
      "[step: 3317] loss: 0.010803533717989922\n",
      "[step: 3318] loss: 0.010802668519318104\n",
      "[step: 3319] loss: 0.01080180425196886\n",
      "[step: 3320] loss: 0.010800933465361595\n",
      "[step: 3321] loss: 0.010800063610076904\n",
      "[step: 3322] loss: 0.01079918909817934\n",
      "[step: 3323] loss: 0.010798315517604351\n",
      "[step: 3324] loss: 0.010797439143061638\n",
      "[step: 3325] loss: 0.010796560905873775\n",
      "[step: 3326] loss: 0.01079567801207304\n",
      "[step: 3327] loss: 0.01079479418694973\n",
      "[step: 3328] loss: 0.01079390849918127\n",
      "[step: 3329] loss: 0.010793017223477364\n",
      "[step: 3330] loss: 0.010792126879096031\n",
      "[step: 3331] loss: 0.010791233740746975\n",
      "[step: 3332] loss: 0.010790339671075344\n",
      "[step: 3333] loss: 0.010789441876113415\n",
      "[step: 3334] loss: 0.010788539424538612\n",
      "[step: 3335] loss: 0.01078763697296381\n",
      "[step: 3336] loss: 0.010786730796098709\n",
      "[step: 3337] loss: 0.010785824619233608\n",
      "[step: 3338] loss: 0.010784914717078209\n",
      "[step: 3339] loss: 0.01078400295227766\n",
      "[step: 3340] loss: 0.010783089324831963\n",
      "[step: 3341] loss: 0.010782171972095966\n",
      "[step: 3342] loss: 0.01078125275671482\n",
      "[step: 3343] loss: 0.01078033447265625\n",
      "[step: 3344] loss: 0.010779407806694508\n",
      "[step: 3345] loss: 0.01077848207205534\n",
      "[step: 3346] loss: 0.010777555406093597\n",
      "[step: 3347] loss: 0.010776625014841557\n",
      "[step: 3348] loss: 0.010775691829621792\n",
      "[step: 3349] loss: 0.010774759575724602\n",
      "[step: 3350] loss: 0.010773822665214539\n",
      "[step: 3351] loss: 0.0107728848233819\n",
      "[step: 3352] loss: 0.010771946050226688\n",
      "[step: 3353] loss: 0.010771004483103752\n",
      "[step: 3354] loss: 0.010770061984658241\n",
      "[step: 3355] loss: 0.010769116692245007\n",
      "[step: 3356] loss: 0.010768169537186623\n",
      "[step: 3357] loss: 0.01076722051948309\n",
      "[step: 3358] loss: 0.010766269639134407\n",
      "[step: 3359] loss: 0.0107653196901083\n",
      "[step: 3360] loss: 0.010764366947114468\n",
      "[step: 3361] loss: 0.010763410478830338\n",
      "[step: 3362] loss: 0.010762456804513931\n",
      "[step: 3363] loss: 0.010761500336229801\n",
      "[step: 3364] loss: 0.010760542936623096\n",
      "[step: 3365] loss: 0.010759583674371243\n",
      "[step: 3366] loss: 0.010758624412119389\n",
      "[step: 3367] loss: 0.01075766421854496\n",
      "[step: 3368] loss: 0.010756702162325382\n",
      "[step: 3369] loss: 0.010755738243460655\n",
      "[step: 3370] loss: 0.010754775255918503\n",
      "[step: 3371] loss: 0.010753813199698925\n",
      "[step: 3372] loss: 0.010752847418189049\n",
      "[step: 3373] loss: 0.010751885361969471\n",
      "[step: 3374] loss: 0.010750921443104744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3375] loss: 0.010749954730272293\n",
      "[step: 3376] loss: 0.010748988948762417\n",
      "[step: 3377] loss: 0.010748021304607391\n",
      "[step: 3378] loss: 0.010747055523097515\n",
      "[step: 3379] loss: 0.01074608601629734\n",
      "[step: 3380] loss: 0.010745121166110039\n",
      "[step: 3381] loss: 0.010744151659309864\n",
      "[step: 3382] loss: 0.010743184946477413\n",
      "[step: 3383] loss: 0.010742215439677238\n",
      "[step: 3384] loss: 0.010741242207586765\n",
      "[step: 3385] loss: 0.010740270838141441\n",
      "[step: 3386] loss: 0.010739299468696117\n",
      "[step: 3387] loss: 0.010738324373960495\n",
      "[step: 3388] loss: 0.010737347416579723\n",
      "[step: 3389] loss: 0.010736371390521526\n",
      "[step: 3390] loss: 0.010735390707850456\n",
      "[step: 3391] loss: 0.010734411887824535\n",
      "[step: 3392] loss: 0.010733427479863167\n",
      "[step: 3393] loss: 0.010732442140579224\n",
      "[step: 3394] loss: 0.010731455869972706\n",
      "[step: 3395] loss: 0.010730463080108166\n",
      "[step: 3396] loss: 0.010729468427598476\n",
      "[step: 3397] loss: 0.01072846632450819\n",
      "[step: 3398] loss: 0.01072746329009533\n",
      "[step: 3399] loss: 0.010726455599069595\n",
      "[step: 3400] loss: 0.010725443251430988\n",
      "[step: 3401] loss: 0.010724428109824657\n",
      "[step: 3402] loss: 0.01072340365499258\n",
      "[step: 3403] loss: 0.010722375474870205\n",
      "[step: 3404] loss: 0.01072134543210268\n",
      "[step: 3405] loss: 0.010720302350819111\n",
      "[step: 3406] loss: 0.010719258338212967\n",
      "[step: 3407] loss: 0.010718205943703651\n",
      "[step: 3408] loss: 0.010717145167291164\n",
      "[step: 3409] loss: 0.010716077871620655\n",
      "[step: 3410] loss: 0.010715007781982422\n",
      "[step: 3411] loss: 0.010713926516473293\n",
      "[step: 3412] loss: 0.010712835937738419\n",
      "[step: 3413] loss: 0.010711737908422947\n",
      "[step: 3414] loss: 0.010710634291172028\n",
      "[step: 3415] loss: 0.010709518566727638\n",
      "[step: 3416] loss: 0.010708397254347801\n",
      "[step: 3417] loss: 0.010707266628742218\n",
      "[step: 3418] loss: 0.010706125758588314\n",
      "[step: 3419] loss: 0.010704982094466686\n",
      "[step: 3420] loss: 0.010703831911087036\n",
      "[step: 3421] loss: 0.010702698491513729\n",
      "[step: 3422] loss: 0.010701611638069153\n",
      "[step: 3423] loss: 0.010700683109462261\n",
      "[step: 3424] loss: 0.010700172744691372\n",
      "[step: 3425] loss: 0.010701017454266548\n",
      "[step: 3426] loss: 0.010705320164561272\n",
      "[step: 3427] loss: 0.010721669532358646\n",
      "[step: 3428] loss: 0.010762903839349747\n",
      "[step: 3429] loss: 0.01090444065630436\n",
      "[step: 3430] loss: 0.011041650548577309\n",
      "[step: 3431] loss: 0.011402915231883526\n",
      "[step: 3432] loss: 0.010857544839382172\n",
      "[step: 3433] loss: 0.010761710815131664\n",
      "[step: 3434] loss: 0.01103201974183321\n",
      "[step: 3435] loss: 0.010759095661342144\n",
      "[step: 3436] loss: 0.010785253718495369\n",
      "[step: 3437] loss: 0.010908986441791058\n",
      "[step: 3438] loss: 0.010713856667280197\n",
      "[step: 3439] loss: 0.010892180725932121\n",
      "[step: 3440] loss: 0.010776550509035587\n",
      "[step: 3441] loss: 0.01078776828944683\n",
      "[step: 3442] loss: 0.010817508213222027\n",
      "[step: 3443] loss: 0.010716918855905533\n",
      "[step: 3444] loss: 0.010805518366396427\n",
      "[step: 3445] loss: 0.01070975698530674\n",
      "[step: 3446] loss: 0.010786476545035839\n",
      "[step: 3447] loss: 0.010726993903517723\n",
      "[step: 3448] loss: 0.010748142376542091\n",
      "[step: 3449] loss: 0.010737807489931583\n",
      "[step: 3450] loss: 0.010713501833379269\n",
      "[step: 3451] loss: 0.010745185427367687\n",
      "[step: 3452] loss: 0.010697562247514725\n",
      "[step: 3453] loss: 0.010737386532127857\n",
      "[step: 3454] loss: 0.01070043258368969\n",
      "[step: 3455] loss: 0.010710576549172401\n",
      "[step: 3456] loss: 0.010707425884902477\n",
      "[step: 3457] loss: 0.010683977045118809\n",
      "[step: 3458] loss: 0.010704997926950455\n",
      "[step: 3459] loss: 0.010678223334252834\n",
      "[step: 3460] loss: 0.010683612897992134\n",
      "[step: 3461] loss: 0.010687828063964844\n",
      "[step: 3462] loss: 0.010665878653526306\n",
      "[step: 3463] loss: 0.010679217986762524\n",
      "[step: 3464] loss: 0.010677722282707691\n",
      "[step: 3465] loss: 0.010658267885446548\n",
      "[step: 3466] loss: 0.010668477043509483\n",
      "[step: 3467] loss: 0.010677325539290905\n",
      "[step: 3468] loss: 0.010659119114279747\n",
      "[step: 3469] loss: 0.010652921162545681\n",
      "[step: 3470] loss: 0.010664256289601326\n",
      "[step: 3471] loss: 0.010668960399925709\n",
      "[step: 3472] loss: 0.010663744993507862\n",
      "[step: 3473] loss: 0.010650960728526115\n",
      "[step: 3474] loss: 0.01064517442137003\n",
      "[step: 3475] loss: 0.010647605173289776\n",
      "[step: 3476] loss: 0.01065226923674345\n",
      "[step: 3477] loss: 0.010654717683792114\n",
      "[step: 3478] loss: 0.01064874604344368\n",
      "[step: 3479] loss: 0.010640961118042469\n",
      "[step: 3480] loss: 0.01063463930040598\n",
      "[step: 3481] loss: 0.010633477009832859\n",
      "[step: 3482] loss: 0.010635403916239738\n",
      "[step: 3483] loss: 0.010635063983500004\n",
      "[step: 3484] loss: 0.010631479322910309\n",
      "[step: 3485] loss: 0.010626422241330147\n",
      "[step: 3486] loss: 0.01062401570379734\n",
      "[step: 3487] loss: 0.01062417309731245\n",
      "[step: 3488] loss: 0.010623757727444172\n",
      "[step: 3489] loss: 0.01062140055000782\n",
      "[step: 3490] loss: 0.010617564432322979\n",
      "[step: 3491] loss: 0.010614612139761448\n",
      "[step: 3492] loss: 0.01061315555125475\n",
      "[step: 3493] loss: 0.010612336918711662\n",
      "[step: 3494] loss: 0.010611316189169884\n",
      "[step: 3495] loss: 0.010609373450279236\n",
      "[step: 3496] loss: 0.010607183910906315\n",
      "[step: 3497] loss: 0.01060461439192295\n",
      "[step: 3498] loss: 0.010602366179227829\n",
      "[step: 3499] loss: 0.010600227862596512\n",
      "[step: 3500] loss: 0.010598752647638321\n",
      "[step: 3501] loss: 0.01059810258448124\n",
      "[step: 3502] loss: 0.010600321926176548\n",
      "[step: 3503] loss: 0.010607674717903137\n",
      "[step: 3504] loss: 0.010634283535182476\n",
      "[step: 3505] loss: 0.010685100220143795\n",
      "[step: 3506] loss: 0.010844148695468903\n",
      "[step: 3507] loss: 0.010869425721466541\n",
      "[step: 3508] loss: 0.010908878408372402\n",
      "[step: 3509] loss: 0.010609402321279049\n",
      "[step: 3510] loss: 0.010729041881859303\n",
      "[step: 3511] loss: 0.010862983763217926\n",
      "[step: 3512] loss: 0.010601218789815903\n",
      "[step: 3513] loss: 0.010840214788913727\n",
      "[step: 3514] loss: 0.010739495977759361\n",
      "[step: 3515] loss: 0.010706457309424877\n",
      "[step: 3516] loss: 0.010771393775939941\n",
      "[step: 3517] loss: 0.010623963549733162\n",
      "[step: 3518] loss: 0.010750352405011654\n",
      "[step: 3519] loss: 0.010613204911351204\n",
      "[step: 3520] loss: 0.010729878209531307\n",
      "[step: 3521] loss: 0.010626849718391895\n",
      "[step: 3522] loss: 0.010668033733963966\n",
      "[step: 3523] loss: 0.010635982267558575\n",
      "[step: 3524] loss: 0.010598747059702873\n",
      "[step: 3525] loss: 0.010649790987372398\n",
      "[step: 3526] loss: 0.01056554913520813\n",
      "[step: 3527] loss: 0.010615264065563679\n",
      "[step: 3528] loss: 0.010640781372785568\n",
      "[step: 3529] loss: 0.010552618652582169\n",
      "[step: 3530] loss: 0.01060432754456997\n",
      "[step: 3531] loss: 0.010687043890357018\n",
      "[step: 3532] loss: 0.01059828419238329\n",
      "[step: 3533] loss: 0.010535012930631638\n",
      "[step: 3534] loss: 0.01054839976131916\n",
      "[step: 3535] loss: 0.010580034926533699\n",
      "[step: 3536] loss: 0.010568403638899326\n",
      "[step: 3537] loss: 0.010521379299461842\n",
      "[step: 3538] loss: 0.010537165217101574\n",
      "[step: 3539] loss: 0.010566200129687786\n",
      "[step: 3540] loss: 0.01052845735102892\n",
      "[step: 3541] loss: 0.010511485859751701\n",
      "[step: 3542] loss: 0.010532526299357414\n",
      "[step: 3543] loss: 0.0105253541842103\n",
      "[step: 3544] loss: 0.010502619668841362\n",
      "[step: 3545] loss: 0.010494980961084366\n",
      "[step: 3546] loss: 0.010505693033337593\n",
      "[step: 3547] loss: 0.010519113391637802\n",
      "[step: 3548] loss: 0.010510641150176525\n",
      "[step: 3549] loss: 0.010498648509383202\n",
      "[step: 3550] loss: 0.01048188004642725\n",
      "[step: 3551] loss: 0.010472053661942482\n",
      "[step: 3552] loss: 0.010468585416674614\n",
      "[step: 3553] loss: 0.010469553992152214\n",
      "[step: 3554] loss: 0.010471963323652744\n",
      "[step: 3555] loss: 0.010468831285834312\n",
      "[step: 3556] loss: 0.010462606325745583\n",
      "[step: 3557] loss: 0.0104536646977067\n",
      "[step: 3558] loss: 0.010446779429912567\n",
      "[step: 3559] loss: 0.010442636907100677\n",
      "[step: 3560] loss: 0.010441078804433346\n",
      "[step: 3561] loss: 0.010441917926073074\n",
      "[step: 3562] loss: 0.010445267893373966\n",
      "[step: 3563] loss: 0.010459966026246548\n",
      "[step: 3564] loss: 0.0104908412322402\n",
      "[step: 3565] loss: 0.010602109134197235\n",
      "[step: 3566] loss: 0.01067163422703743\n",
      "[step: 3567] loss: 0.010813774541020393\n",
      "[step: 3568] loss: 0.0104887830093503\n",
      "[step: 3569] loss: 0.010581365786492825\n",
      "[step: 3570] loss: 0.010723474435508251\n",
      "[step: 3571] loss: 0.010466886684298515\n",
      "[step: 3572] loss: 0.010714255273342133\n",
      "[step: 3573] loss: 0.010661098174750805\n",
      "[step: 3574] loss: 0.010507947765290737\n",
      "[step: 3575] loss: 0.010777945630252361\n",
      "[step: 3576] loss: 0.010576772503554821\n",
      "[step: 3577] loss: 0.010520704090595245\n",
      "[step: 3578] loss: 0.010741385631263256\n",
      "[step: 3579] loss: 0.010533485561609268\n",
      "[step: 3580] loss: 0.010529773309826851\n",
      "[step: 3581] loss: 0.010611963458359241\n",
      "[step: 3582] loss: 0.010435959324240685\n",
      "[step: 3583] loss: 0.010553213767707348\n",
      "[step: 3584] loss: 0.010465520434081554\n",
      "[step: 3585] loss: 0.010444926097989082\n",
      "[step: 3586] loss: 0.01052002888172865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3587] loss: 0.01041779387742281\n",
      "[step: 3588] loss: 0.010458420030772686\n",
      "[step: 3589] loss: 0.010533991269767284\n",
      "[step: 3590] loss: 0.010427779518067837\n",
      "[step: 3591] loss: 0.010408097878098488\n",
      "[step: 3592] loss: 0.010473049245774746\n",
      "[step: 3593] loss: 0.010429568588733673\n",
      "[step: 3594] loss: 0.010392607189714909\n",
      "[step: 3595] loss: 0.010425466112792492\n",
      "[step: 3596] loss: 0.010429079644382\n",
      "[step: 3597] loss: 0.010400238446891308\n",
      "[step: 3598] loss: 0.010377566330134869\n",
      "[step: 3599] loss: 0.010391472838819027\n",
      "[step: 3600] loss: 0.010417859070003033\n",
      "[step: 3601] loss: 0.010414408519864082\n",
      "[step: 3602] loss: 0.010401011444628239\n",
      "[step: 3603] loss: 0.010372835211455822\n",
      "[step: 3604] loss: 0.01036502793431282\n",
      "[step: 3605] loss: 0.010376418940722942\n",
      "[step: 3606] loss: 0.010386672802269459\n",
      "[step: 3607] loss: 0.010398583486676216\n",
      "[step: 3608] loss: 0.010400183498859406\n",
      "[step: 3609] loss: 0.010423917323350906\n",
      "[step: 3610] loss: 0.010438635013997555\n",
      "[step: 3611] loss: 0.010492797009646893\n",
      "[step: 3612] loss: 0.010462828911840916\n",
      "[step: 3613] loss: 0.010427527129650116\n",
      "[step: 3614] loss: 0.010357758961617947\n",
      "[step: 3615] loss: 0.010355977341532707\n",
      "[step: 3616] loss: 0.010411281138658524\n",
      "[step: 3617] loss: 0.010454599745571613\n",
      "[step: 3618] loss: 0.01054826658219099\n",
      "[step: 3619] loss: 0.010518626309931278\n",
      "[step: 3620] loss: 0.010482761077582836\n",
      "[step: 3621] loss: 0.010352149605751038\n",
      "[step: 3622] loss: 0.010397711768746376\n",
      "[step: 3623] loss: 0.010468640364706516\n",
      "[step: 3624] loss: 0.01035604439675808\n",
      "[step: 3625] loss: 0.010349203832447529\n",
      "[step: 3626] loss: 0.01044241338968277\n",
      "[step: 3627] loss: 0.010435772128403187\n",
      "[step: 3628] loss: 0.010399380698800087\n",
      "[step: 3629] loss: 0.010328587144613266\n",
      "[step: 3630] loss: 0.010316306725144386\n",
      "[step: 3631] loss: 0.010351448319852352\n",
      "[step: 3632] loss: 0.010357273742556572\n",
      "[step: 3633] loss: 0.010332879610359669\n",
      "[step: 3634] loss: 0.010301508940756321\n",
      "[step: 3635] loss: 0.010300454683601856\n",
      "[step: 3636] loss: 0.010327461175620556\n",
      "[step: 3637] loss: 0.010375862009823322\n",
      "[step: 3638] loss: 0.010514008812606335\n",
      "[step: 3639] loss: 0.010626217350363731\n",
      "[step: 3640] loss: 0.010770794004201889\n",
      "[step: 3641] loss: 0.010359746403992176\n",
      "[step: 3642] loss: 0.010547690093517303\n",
      "[step: 3643] loss: 0.010537057183682919\n",
      "[step: 3644] loss: 0.010434212163090706\n",
      "[step: 3645] loss: 0.010547821410000324\n",
      "[step: 3646] loss: 0.010333525948226452\n",
      "[step: 3647] loss: 0.010491883382201195\n",
      "[step: 3648] loss: 0.01032798457890749\n",
      "[step: 3649] loss: 0.01040361076593399\n",
      "[step: 3650] loss: 0.010490726679563522\n",
      "[step: 3651] loss: 0.010286063887178898\n",
      "[step: 3652] loss: 0.01039665937423706\n",
      "[step: 3653] loss: 0.010437830351293087\n",
      "[step: 3654] loss: 0.010279758833348751\n",
      "[step: 3655] loss: 0.010443951934576035\n",
      "[step: 3656] loss: 0.01038734894245863\n",
      "[step: 3657] loss: 0.010279868729412556\n",
      "[step: 3658] loss: 0.010448274202644825\n",
      "[step: 3659] loss: 0.01039827335625887\n",
      "[step: 3660] loss: 0.01023621391505003\n",
      "[step: 3661] loss: 0.010409583337605\n",
      "[step: 3662] loss: 0.010448278859257698\n",
      "[step: 3663] loss: 0.010226869955658913\n",
      "[step: 3664] loss: 0.010407675057649612\n",
      "[step: 3665] loss: 0.010339782573282719\n",
      "[step: 3666] loss: 0.010275344364345074\n",
      "[step: 3667] loss: 0.010392121970653534\n",
      "[step: 3668] loss: 0.010220982134342194\n",
      "[step: 3669] loss: 0.01028693001717329\n",
      "[step: 3670] loss: 0.010299067944288254\n",
      "[step: 3671] loss: 0.010184039361774921\n",
      "[step: 3672] loss: 0.010203501209616661\n",
      "[step: 3673] loss: 0.010270565748214722\n",
      "[step: 3674] loss: 0.010241935029625893\n",
      "[step: 3675] loss: 0.010156484320759773\n",
      "[step: 3676] loss: 0.010186119936406612\n",
      "[step: 3677] loss: 0.010251633822917938\n",
      "[step: 3678] loss: 0.010206909850239754\n",
      "[step: 3679] loss: 0.010144026950001717\n",
      "[step: 3680] loss: 0.010149202309548855\n",
      "[step: 3681] loss: 0.01017802581191063\n",
      "[step: 3682] loss: 0.01015260349959135\n",
      "[step: 3683] loss: 0.010130440816283226\n",
      "[step: 3684] loss: 0.010155832394957542\n",
      "[step: 3685] loss: 0.010148512199521065\n",
      "[step: 3686] loss: 0.010120089165866375\n",
      "[step: 3687] loss: 0.010136605240404606\n",
      "[step: 3688] loss: 0.010143146850168705\n",
      "[step: 3689] loss: 0.010116266086697578\n",
      "[step: 3690] loss: 0.010121822357177734\n",
      "[step: 3691] loss: 0.010133582167327404\n",
      "[step: 3692] loss: 0.01011299341917038\n",
      "[step: 3693] loss: 0.010109517723321915\n",
      "[step: 3694] loss: 0.01011971291154623\n",
      "[step: 3695] loss: 0.010106515139341354\n",
      "[step: 3696] loss: 0.010103242471814156\n",
      "[step: 3697] loss: 0.010109723545610905\n",
      "[step: 3698] loss: 0.010099481791257858\n",
      "[step: 3699] loss: 0.010096575133502483\n",
      "[step: 3700] loss: 0.010101072490215302\n",
      "[step: 3701] loss: 0.010093849152326584\n",
      "[step: 3702] loss: 0.010089334100484848\n",
      "[step: 3703] loss: 0.010092467069625854\n",
      "[step: 3704] loss: 0.010089113377034664\n",
      "[step: 3705] loss: 0.010083159431815147\n",
      "[step: 3706] loss: 0.01008308120071888\n",
      "[step: 3707] loss: 0.010082993656396866\n",
      "[step: 3708] loss: 0.010078351013362408\n",
      "[step: 3709] loss: 0.010075111873447895\n",
      "[step: 3710] loss: 0.010075305588543415\n",
      "[step: 3711] loss: 0.010073144920170307\n",
      "[step: 3712] loss: 0.01006899680942297\n",
      "[step: 3713] loss: 0.010067570023238659\n",
      "[step: 3714] loss: 0.010066802613437176\n",
      "[step: 3715] loss: 0.010063553228974342\n",
      "[step: 3716] loss: 0.010060378350317478\n",
      "[step: 3717] loss: 0.010059134103357792\n",
      "[step: 3718] loss: 0.010057552717626095\n",
      "[step: 3719] loss: 0.010054446756839752\n",
      "[step: 3720] loss: 0.010051540099084377\n",
      "[step: 3721] loss: 0.010049807839095592\n",
      "[step: 3722] loss: 0.01004800759255886\n",
      "[step: 3723] loss: 0.010045197792351246\n",
      "[step: 3724] loss: 0.010042239911854267\n",
      "[step: 3725] loss: 0.010039962828159332\n",
      "[step: 3726] loss: 0.010037931613624096\n",
      "[step: 3727] loss: 0.010035359300673008\n",
      "[step: 3728] loss: 0.010032298043370247\n",
      "[step: 3729] loss: 0.010029425844550133\n",
      "[step: 3730] loss: 0.01002691313624382\n",
      "[step: 3731] loss: 0.010024338960647583\n",
      "[step: 3732] loss: 0.010021382942795753\n",
      "[step: 3733] loss: 0.01001813355833292\n",
      "[step: 3734] loss: 0.010014867410063744\n",
      "[step: 3735] loss: 0.010011723265051842\n",
      "[step: 3736] loss: 0.010008580982685089\n",
      "[step: 3737] loss: 0.010005232878029346\n",
      "[step: 3738] loss: 0.010001626797020435\n",
      "[step: 3739] loss: 0.009997773915529251\n",
      "[step: 3740] loss: 0.00999376829713583\n",
      "[step: 3741] loss: 0.009989626705646515\n",
      "[step: 3742] loss: 0.009985347278416157\n",
      "[step: 3743] loss: 0.009980903007090092\n",
      "[step: 3744] loss: 0.00997626967728138\n",
      "[step: 3745] loss: 0.009971512481570244\n",
      "[step: 3746] loss: 0.009966895915567875\n",
      "[step: 3747] loss: 0.00996427983045578\n",
      "[step: 3748] loss: 0.009974934160709381\n",
      "[step: 3749] loss: 0.010086791589856148\n",
      "[step: 3750] loss: 0.01067886222153902\n",
      "[step: 3751] loss: 0.011067424900829792\n",
      "[step: 3752] loss: 0.010481559671461582\n",
      "[step: 3753] loss: 0.010777940973639488\n",
      "[step: 3754] loss: 0.010558414272964\n",
      "[step: 3755] loss: 0.010513674467802048\n",
      "[step: 3756] loss: 0.010574099607765675\n",
      "[step: 3757] loss: 0.0106954425573349\n",
      "[step: 3758] loss: 0.010459556244313717\n",
      "[step: 3759] loss: 0.01066930778324604\n",
      "[step: 3760] loss: 0.010452269576489925\n",
      "[step: 3761] loss: 0.010529033839702606\n",
      "[step: 3762] loss: 0.010420051403343678\n",
      "[step: 3763] loss: 0.010424337349832058\n",
      "[step: 3764] loss: 0.01039868500083685\n",
      "[step: 3765] loss: 0.01043328270316124\n",
      "[step: 3766] loss: 0.01040730532258749\n",
      "[step: 3767] loss: 0.010375355370342731\n",
      "[step: 3768] loss: 0.010294454172253609\n",
      "[step: 3769] loss: 0.01030000764876604\n",
      "[step: 3770] loss: 0.010232212021946907\n",
      "[step: 3771] loss: 0.0102473059669137\n",
      "[step: 3772] loss: 0.010198975913226604\n",
      "[step: 3773] loss: 0.010198812000453472\n",
      "[step: 3774] loss: 0.010115811601281166\n",
      "[step: 3775] loss: 0.010128597728908062\n",
      "[step: 3776] loss: 0.010080615058541298\n",
      "[step: 3777] loss: 0.010022282600402832\n",
      "[step: 3778] loss: 0.010075991041958332\n",
      "[step: 3779] loss: 0.010229526087641716\n",
      "[step: 3780] loss: 0.010470411740243435\n",
      "[step: 3781] loss: 0.010646330192685127\n",
      "[step: 3782] loss: 0.010240341536700726\n",
      "[step: 3783] loss: 0.010205322876572609\n",
      "[step: 3784] loss: 0.01031018141657114\n",
      "[step: 3785] loss: 0.010085786692798138\n",
      "[step: 3786] loss: 0.010224872268736362\n",
      "[step: 3787] loss: 0.010106493718922138\n",
      "[step: 3788] loss: 0.010129746049642563\n",
      "[step: 3789] loss: 0.010126085951924324\n",
      "[step: 3790] loss: 0.010036688297986984\n",
      "[step: 3791] loss: 0.010092515498399734\n",
      "[step: 3792] loss: 0.010048151947557926\n",
      "[step: 3793] loss: 0.010066989809274673\n",
      "[step: 3794] loss: 0.010072454810142517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3795] loss: 0.010037164203822613\n",
      "[step: 3796] loss: 0.010050751268863678\n",
      "[step: 3797] loss: 0.01000524964183569\n",
      "[step: 3798] loss: 0.00999070517718792\n",
      "[step: 3799] loss: 0.009994672611355782\n",
      "[step: 3800] loss: 0.00996908638626337\n",
      "[step: 3801] loss: 0.010009855963289738\n",
      "[step: 3802] loss: 0.009996026754379272\n",
      "[step: 3803] loss: 0.01013680174946785\n",
      "[step: 3804] loss: 0.010568943805992603\n",
      "[step: 3805] loss: 0.010192852467298508\n",
      "[step: 3806] loss: 0.01001680176705122\n",
      "[step: 3807] loss: 0.00998203456401825\n",
      "[step: 3808] loss: 0.010068077594041824\n",
      "[step: 3809] loss: 0.010015750303864479\n",
      "[step: 3810] loss: 0.009993026964366436\n",
      "[step: 3811] loss: 0.01004072930663824\n",
      "[step: 3812] loss: 0.010011952370405197\n",
      "[step: 3813] loss: 0.009994501248002052\n",
      "[step: 3814] loss: 0.009989569894969463\n",
      "[step: 3815] loss: 0.009984385222196579\n",
      "[step: 3816] loss: 0.00995419081300497\n",
      "[step: 3817] loss: 0.00997960101813078\n",
      "[step: 3818] loss: 0.009931784123182297\n",
      "[step: 3819] loss: 0.009957327507436275\n",
      "[step: 3820] loss: 0.009947267360985279\n",
      "[step: 3821] loss: 0.010067684575915337\n",
      "[step: 3822] loss: 0.0107632577419281\n",
      "[step: 3823] loss: 0.010175463743507862\n",
      "[step: 3824] loss: 0.009934310801327229\n",
      "[step: 3825] loss: 0.01002026628702879\n",
      "[step: 3826] loss: 0.010088070295751095\n",
      "[step: 3827] loss: 0.010129517875611782\n",
      "[step: 3828] loss: 0.009972871281206608\n",
      "[step: 3829] loss: 0.010262410156428814\n",
      "[step: 3830] loss: 0.010061905719339848\n",
      "[step: 3831] loss: 0.010136216878890991\n",
      "[step: 3832] loss: 0.00997747853398323\n",
      "[step: 3833] loss: 0.010141688399016857\n",
      "[step: 3834] loss: 0.009996918961405754\n",
      "[step: 3835] loss: 0.010111261159181595\n",
      "[step: 3836] loss: 0.010008190758526325\n",
      "[step: 3837] loss: 0.010024044662714005\n",
      "[step: 3838] loss: 0.010019543580710888\n",
      "[step: 3839] loss: 0.009986970573663712\n",
      "[step: 3840] loss: 0.010014019906520844\n",
      "[step: 3841] loss: 0.0099437041208148\n",
      "[step: 3842] loss: 0.009987292811274529\n",
      "[step: 3843] loss: 0.009980798698961735\n",
      "[step: 3844] loss: 0.009986534714698792\n",
      "[step: 3845] loss: 0.009982028976082802\n",
      "[step: 3846] loss: 0.009960001334547997\n",
      "[step: 3847] loss: 0.010097571648657322\n",
      "[step: 3848] loss: 0.010146712884306908\n",
      "[step: 3849] loss: 0.010192339308559895\n",
      "[step: 3850] loss: 0.010387741960585117\n",
      "[step: 3851] loss: 0.010192331857979298\n",
      "[step: 3852] loss: 0.010209216736257076\n",
      "[step: 3853] loss: 0.010027672164142132\n",
      "[step: 3854] loss: 0.01008892897516489\n",
      "[step: 3855] loss: 0.010144650936126709\n",
      "[step: 3856] loss: 0.010046531446278095\n",
      "[step: 3857] loss: 0.010107520967721939\n",
      "[step: 3858] loss: 0.01009857002645731\n",
      "[step: 3859] loss: 0.01008628774434328\n",
      "[step: 3860] loss: 0.010096821933984756\n",
      "[step: 3861] loss: 0.01002276036888361\n",
      "[step: 3862] loss: 0.010052934288978577\n",
      "[step: 3863] loss: 0.010039032436907291\n",
      "[step: 3864] loss: 0.00996848288923502\n",
      "[step: 3865] loss: 0.00999500323086977\n",
      "[step: 3866] loss: 0.009972065687179565\n",
      "[step: 3867] loss: 0.009924336336553097\n",
      "[step: 3868] loss: 0.0099647156894207\n",
      "[step: 3869] loss: 0.009930312633514404\n",
      "[step: 3870] loss: 0.009938227944076061\n",
      "[step: 3871] loss: 0.0099635049700737\n",
      "[step: 3872] loss: 0.009927598759531975\n",
      "[step: 3873] loss: 0.009955855086445808\n",
      "[step: 3874] loss: 0.00996551662683487\n",
      "[step: 3875] loss: 0.010038893669843674\n",
      "[step: 3876] loss: 0.010136830620467663\n",
      "[step: 3877] loss: 0.010269638150930405\n",
      "[step: 3878] loss: 0.009944536723196507\n",
      "[step: 3879] loss: 0.01024052407592535\n",
      "[step: 3880] loss: 0.010013625957071781\n",
      "[step: 3881] loss: 0.010075436905026436\n",
      "[step: 3882] loss: 0.010013923048973083\n",
      "[step: 3883] loss: 0.010064364410936832\n",
      "[step: 3884] loss: 0.010034831240773201\n",
      "[step: 3885] loss: 0.010026459582149982\n",
      "[step: 3886] loss: 0.010040367022156715\n",
      "[step: 3887] loss: 0.010001732036471367\n",
      "[step: 3888] loss: 0.010003084316849709\n",
      "[step: 3889] loss: 0.00997051689773798\n",
      "[step: 3890] loss: 0.009979039430618286\n",
      "[step: 3891] loss: 0.009940729476511478\n",
      "[step: 3892] loss: 0.009951133280992508\n",
      "[step: 3893] loss: 0.009938716888427734\n",
      "[step: 3894] loss: 0.009893619455397129\n",
      "[step: 3895] loss: 0.009947910904884338\n",
      "[step: 3896] loss: 0.009968043304979801\n",
      "[step: 3897] loss: 0.009909063577651978\n",
      "[step: 3898] loss: 0.01006833091378212\n",
      "[step: 3899] loss: 0.009982094168663025\n",
      "[step: 3900] loss: 0.010138356126844883\n",
      "[step: 3901] loss: 0.010135944001376629\n",
      "[step: 3902] loss: 0.010064252652227879\n",
      "[step: 3903] loss: 0.009892151691019535\n",
      "[step: 3904] loss: 0.010058607906103134\n",
      "[step: 3905] loss: 0.009993440471589565\n",
      "[step: 3906] loss: 0.009997058659791946\n",
      "[step: 3907] loss: 0.00996361207216978\n",
      "[step: 3908] loss: 0.009963447228074074\n",
      "[step: 3909] loss: 0.009972317144274712\n",
      "[step: 3910] loss: 0.009974948130548\n",
      "[step: 3911] loss: 0.009925524704158306\n",
      "[step: 3912] loss: 0.009978274814784527\n",
      "[step: 3913] loss: 0.009891909547150135\n",
      "[step: 3914] loss: 0.009950036182999611\n",
      "[step: 3915] loss: 0.00988374836742878\n",
      "[step: 3916] loss: 0.00992630049586296\n",
      "[step: 3917] loss: 0.00988400261849165\n",
      "[step: 3918] loss: 0.009873042814433575\n",
      "[step: 3919] loss: 0.009957099333405495\n",
      "[step: 3920] loss: 0.010100159794092178\n",
      "[step: 3921] loss: 0.009986952878534794\n",
      "[step: 3922] loss: 0.010058346204459667\n",
      "[step: 3923] loss: 0.010124938562512398\n",
      "[step: 3924] loss: 0.010243674740195274\n",
      "[step: 3925] loss: 0.010115386918187141\n",
      "[step: 3926] loss: 0.010215099900960922\n",
      "[step: 3927] loss: 0.01010211557149887\n",
      "[step: 3928] loss: 0.010055262595415115\n",
      "[step: 3929] loss: 0.01011821161955595\n",
      "[step: 3930] loss: 0.010028689168393612\n",
      "[step: 3931] loss: 0.01007525622844696\n",
      "[step: 3932] loss: 0.009957002475857735\n",
      "[step: 3933] loss: 0.010039303451776505\n",
      "[step: 3934] loss: 0.009959225542843342\n",
      "[step: 3935] loss: 0.009971428662538528\n",
      "[step: 3936] loss: 0.009871063753962517\n",
      "[step: 3937] loss: 0.010060781612992287\n",
      "[step: 3938] loss: 0.010404189117252827\n",
      "[step: 3939] loss: 0.010427059605717659\n",
      "[step: 3940] loss: 0.010009363293647766\n",
      "[step: 3941] loss: 0.010114660486578941\n",
      "[step: 3942] loss: 0.009991818107664585\n",
      "[step: 3943] loss: 0.010153889656066895\n",
      "[step: 3944] loss: 0.009955339133739471\n",
      "[step: 3945] loss: 0.01010979525744915\n",
      "[step: 3946] loss: 0.00990872923284769\n",
      "[step: 3947] loss: 0.010029775090515614\n",
      "[step: 3948] loss: 0.009934906847774982\n",
      "[step: 3949] loss: 0.009942474775016308\n",
      "[step: 3950] loss: 0.009908652864396572\n",
      "[step: 3951] loss: 0.0099217239767313\n",
      "[step: 3952] loss: 0.009853579103946686\n",
      "[step: 3953] loss: 0.009861346334218979\n",
      "[step: 3954] loss: 0.009986198507249355\n",
      "[step: 3955] loss: 0.010010063648223877\n",
      "[step: 3956] loss: 0.010223306715488434\n",
      "[step: 3957] loss: 0.009826569817960262\n",
      "[step: 3958] loss: 0.010391524992883205\n",
      "[step: 3959] loss: 0.010479727759957314\n",
      "[step: 3960] loss: 0.010555359534919262\n",
      "[step: 3961] loss: 0.01044858805835247\n",
      "[step: 3962] loss: 0.010238278657197952\n",
      "[step: 3963] loss: 0.01017269678413868\n",
      "[step: 3964] loss: 0.010262921452522278\n",
      "[step: 3965] loss: 0.010377991013228893\n",
      "[step: 3966] loss: 0.010382922366261482\n",
      "[step: 3967] loss: 0.01033690758049488\n",
      "[step: 3968] loss: 0.01026023831218481\n",
      "[step: 3969] loss: 0.01022790651768446\n",
      "[step: 3970] loss: 0.010210501030087471\n",
      "[step: 3971] loss: 0.0102270832285285\n",
      "[step: 3972] loss: 0.010234145447611809\n",
      "[step: 3973] loss: 0.010245086625218391\n",
      "[step: 3974] loss: 0.01023859716951847\n",
      "[step: 3975] loss: 0.010222322307527065\n",
      "[step: 3976] loss: 0.01019205991178751\n",
      "[step: 3977] loss: 0.010156028904020786\n",
      "[step: 3978] loss: 0.010117580182850361\n",
      "[step: 3979] loss: 0.010082922875881195\n",
      "[step: 3980] loss: 0.010059566237032413\n",
      "[step: 3981] loss: 0.01004024688154459\n",
      "[step: 3982] loss: 0.010027448646724224\n",
      "[step: 3983] loss: 0.010011903941631317\n",
      "[step: 3984] loss: 0.009987488389015198\n",
      "[step: 3985] loss: 0.009967227466404438\n",
      "[step: 3986] loss: 0.009958014823496342\n",
      "[step: 3987] loss: 0.009956485591828823\n",
      "[step: 3988] loss: 0.00994767714291811\n",
      "[step: 3989] loss: 0.009933419525623322\n",
      "[step: 3990] loss: 0.009925924241542816\n",
      "[step: 3991] loss: 0.009929477237164974\n",
      "[step: 3992] loss: 0.009935294277966022\n",
      "[step: 3993] loss: 0.009925125166773796\n",
      "[step: 3994] loss: 0.009919383563101292\n",
      "[step: 3995] loss: 0.009903401136398315\n",
      "[step: 3996] loss: 0.009880756959319115\n",
      "[step: 3997] loss: 0.00985693372786045\n",
      "[step: 3998] loss: 0.009846411645412445\n",
      "[step: 3999] loss: 0.009840166196227074\n",
      "[step: 4000] loss: 0.009831828065216541\n",
      "[step: 4001] loss: 0.009827572852373123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4002] loss: 0.009815503843128681\n",
      "[step: 4003] loss: 0.009799942374229431\n",
      "[step: 4004] loss: 0.009789885953068733\n",
      "[step: 4005] loss: 0.009788796305656433\n",
      "[step: 4006] loss: 0.0097853634506464\n",
      "[step: 4007] loss: 0.009773529134690762\n",
      "[step: 4008] loss: 0.009762750007212162\n",
      "[step: 4009] loss: 0.00975565891712904\n",
      "[step: 4010] loss: 0.009751624427735806\n",
      "[step: 4011] loss: 0.009746761992573738\n",
      "[step: 4012] loss: 0.009737659245729446\n",
      "[step: 4013] loss: 0.009727762080729008\n",
      "[step: 4014] loss: 0.009720874950289726\n",
      "[step: 4015] loss: 0.009714980609714985\n",
      "[step: 4016] loss: 0.009708572179079056\n",
      "[step: 4017] loss: 0.009702326729893684\n",
      "[step: 4018] loss: 0.009695497341454029\n",
      "[step: 4019] loss: 0.009686818346381187\n",
      "[step: 4020] loss: 0.009679008275270462\n",
      "[step: 4021] loss: 0.009670192375779152\n",
      "[step: 4022] loss: 0.009661856107413769\n",
      "[step: 4023] loss: 0.009653731249272823\n",
      "[step: 4024] loss: 0.009645655751228333\n",
      "[step: 4025] loss: 0.009636227041482925\n",
      "[step: 4026] loss: 0.009626791812479496\n",
      "[step: 4027] loss: 0.009616966359317303\n",
      "[step: 4028] loss: 0.009608102962374687\n",
      "[step: 4029] loss: 0.009599126875400543\n",
      "[step: 4030] loss: 0.00959088746458292\n",
      "[step: 4031] loss: 0.009584077633917332\n",
      "[step: 4032] loss: 0.009582937695086002\n",
      "[step: 4033] loss: 0.009587574750185013\n",
      "[step: 4034] loss: 0.009645440615713596\n",
      "[step: 4035] loss: 0.00972787756472826\n",
      "[step: 4036] loss: 0.010525898076593876\n",
      "[step: 4037] loss: 0.010605871677398682\n",
      "[step: 4038] loss: 0.012088928371667862\n",
      "[step: 4039] loss: 0.01138640008866787\n",
      "[step: 4040] loss: 0.011332066729664803\n",
      "[step: 4041] loss: 0.010792804881930351\n",
      "[step: 4042] loss: 0.010929306969046593\n",
      "[step: 4043] loss: 0.01063371542841196\n",
      "[step: 4044] loss: 0.010681100189685822\n",
      "[step: 4045] loss: 0.01073562353849411\n",
      "[step: 4046] loss: 0.010790780186653137\n",
      "[step: 4047] loss: 0.010675942525267601\n",
      "[step: 4048] loss: 0.01058661937713623\n",
      "[step: 4049] loss: 0.010598882101476192\n",
      "[step: 4050] loss: 0.01064626406878233\n",
      "[step: 4051] loss: 0.010609850287437439\n",
      "[step: 4052] loss: 0.010615778155624866\n",
      "[step: 4053] loss: 0.010557149536907673\n",
      "[step: 4054] loss: 0.010612056590616703\n",
      "[step: 4055] loss: 0.010585585609078407\n",
      "[step: 4056] loss: 0.01062639057636261\n",
      "[step: 4057] loss: 0.010568276979029179\n",
      "[step: 4058] loss: 0.0105661666020751\n",
      "[step: 4059] loss: 0.010532081127166748\n",
      "[step: 4060] loss: 0.010544130578637123\n",
      "[step: 4061] loss: 0.010528577491641045\n",
      "[step: 4062] loss: 0.010507498867809772\n",
      "[step: 4063] loss: 0.010476210154592991\n",
      "[step: 4064] loss: 0.010443099774420261\n",
      "[step: 4065] loss: 0.01042907778173685\n",
      "[step: 4066] loss: 0.010399033315479755\n",
      "[step: 4067] loss: 0.010371997952461243\n",
      "[step: 4068] loss: 0.010327246971428394\n",
      "[step: 4069] loss: 0.010318225249648094\n",
      "[step: 4070] loss: 0.01028109434992075\n",
      "[step: 4071] loss: 0.010258425958454609\n",
      "[step: 4072] loss: 0.010225361213088036\n",
      "[step: 4073] loss: 0.01021072082221508\n",
      "[step: 4074] loss: 0.01017740648239851\n",
      "[step: 4075] loss: 0.01018417626619339\n",
      "[step: 4076] loss: 0.01029409933835268\n",
      "[step: 4077] loss: 0.010217716917395592\n",
      "[step: 4078] loss: 0.010395684279501438\n",
      "[step: 4079] loss: 0.010106611996889114\n",
      "[step: 4080] loss: 0.010110159404575825\n",
      "[step: 4081] loss: 0.010491399094462395\n",
      "[step: 4082] loss: 0.010143529623746872\n",
      "[step: 4083] loss: 0.011176838539540768\n",
      "[step: 4084] loss: 0.011164054274559021\n",
      "[step: 4085] loss: 0.010932369157671928\n",
      "[step: 4086] loss: 0.010129651986062527\n",
      "[step: 4087] loss: 0.010466866195201874\n",
      "[step: 4088] loss: 0.010512531735002995\n",
      "[step: 4089] loss: 0.010395095683634281\n",
      "[step: 4090] loss: 0.010289518162608147\n",
      "[step: 4091] loss: 0.010261361487209797\n",
      "[step: 4092] loss: 0.010281897149980068\n",
      "[step: 4093] loss: 0.010340070351958275\n",
      "[step: 4094] loss: 0.010275616310536861\n",
      "[step: 4095] loss: 0.010224396362900734\n",
      "[step: 4096] loss: 0.010272320359945297\n",
      "[step: 4097] loss: 0.010268651880323887\n",
      "[step: 4098] loss: 0.010275006294250488\n",
      "[step: 4099] loss: 0.010292571038007736\n",
      "[step: 4100] loss: 0.010257942602038383\n",
      "[step: 4101] loss: 0.01022794283926487\n",
      "[step: 4102] loss: 0.010222610086202621\n",
      "[step: 4103] loss: 0.010203761048614979\n",
      "[step: 4104] loss: 0.010206477716565132\n",
      "[step: 4105] loss: 0.010208964347839355\n",
      "[step: 4106] loss: 0.010162027552723885\n",
      "[step: 4107] loss: 0.010140212252736092\n",
      "[step: 4108] loss: 0.010143565014004707\n",
      "[step: 4109] loss: 0.01013147458434105\n",
      "[step: 4110] loss: 0.01011482160538435\n",
      "[step: 4111] loss: 0.01007926370948553\n",
      "[step: 4112] loss: 0.010083559900522232\n",
      "[step: 4113] loss: 0.010079223662614822\n",
      "[step: 4114] loss: 0.010038305073976517\n",
      "[step: 4115] loss: 0.01005361694842577\n",
      "[step: 4116] loss: 0.010033797472715378\n",
      "[step: 4117] loss: 0.010022826492786407\n",
      "[step: 4118] loss: 0.010028424672782421\n",
      "[step: 4119] loss: 0.010006988421082497\n",
      "[step: 4120] loss: 0.010015323758125305\n",
      "[step: 4121] loss: 0.009993556886911392\n",
      "[step: 4122] loss: 0.010004729963839054\n",
      "[step: 4123] loss: 0.00998820923268795\n",
      "[step: 4124] loss: 0.00999043695628643\n",
      "[step: 4125] loss: 0.009988754987716675\n",
      "[step: 4126] loss: 0.009976841509342194\n",
      "[step: 4127] loss: 0.009985017590224743\n",
      "[step: 4128] loss: 0.009969351813197136\n",
      "[step: 4129] loss: 0.009970022365450859\n",
      "[step: 4130] loss: 0.009971310384571552\n",
      "[step: 4131] loss: 0.009957246482372284\n",
      "[step: 4132] loss: 0.009959314949810505\n",
      "[step: 4133] loss: 0.009955130517482758\n",
      "[step: 4134] loss: 0.009944715537130833\n",
      "[step: 4135] loss: 0.009945902042090893\n",
      "[step: 4136] loss: 0.009941942989826202\n",
      "[step: 4137] loss: 0.009934003464877605\n",
      "[step: 4138] loss: 0.009938574396073818\n",
      "[step: 4139] loss: 0.009952099062502384\n",
      "[step: 4140] loss: 0.00998227670788765\n",
      "[step: 4141] loss: 0.010116859339177608\n",
      "[step: 4142] loss: 0.01022796705365181\n",
      "[step: 4143] loss: 0.010399705730378628\n",
      "[step: 4144] loss: 0.01000584103167057\n",
      "[step: 4145] loss: 0.010134943760931492\n",
      "[step: 4146] loss: 0.010424438863992691\n",
      "[step: 4147] loss: 0.010170717723667622\n",
      "[step: 4148] loss: 0.010074463672935963\n",
      "[step: 4149] loss: 0.010280214250087738\n",
      "[step: 4150] loss: 0.009977973997592926\n",
      "[step: 4151] loss: 0.010235466063022614\n",
      "[step: 4152] loss: 0.01000278815627098\n",
      "[step: 4153] loss: 0.010135062038898468\n",
      "[step: 4154] loss: 0.010041698813438416\n",
      "[step: 4155] loss: 0.010036607272922993\n",
      "[step: 4156] loss: 0.010074645280838013\n",
      "[step: 4157] loss: 0.009958918206393719\n",
      "[step: 4158] loss: 0.010095484554767609\n",
      "[step: 4159] loss: 0.00996882002800703\n",
      "[step: 4160] loss: 0.009999565780162811\n",
      "[step: 4161] loss: 0.010023090988397598\n",
      "[step: 4162] loss: 0.009923217818140984\n",
      "[step: 4163] loss: 0.009969743900001049\n",
      "[step: 4164] loss: 0.009967528283596039\n",
      "[step: 4165] loss: 0.009901774115860462\n",
      "[step: 4166] loss: 0.00993670430034399\n",
      "[step: 4167] loss: 0.009945348836481571\n",
      "[step: 4168] loss: 0.009930535219609737\n",
      "[step: 4169] loss: 0.00987710990011692\n",
      "[step: 4170] loss: 0.009935621172189713\n",
      "[step: 4171] loss: 0.010035933926701546\n",
      "[step: 4172] loss: 0.009968333877623081\n",
      "[step: 4173] loss: 0.00990264117717743\n",
      "[step: 4174] loss: 0.009880205616354942\n",
      "[step: 4175] loss: 0.00995048601180315\n",
      "[step: 4176] loss: 0.009969658218324184\n",
      "[step: 4177] loss: 0.009959621354937553\n",
      "[step: 4178] loss: 0.009947515092790127\n",
      "[step: 4179] loss: 0.009882749989628792\n",
      "[step: 4180] loss: 0.009904843755066395\n",
      "[step: 4181] loss: 0.009944950230419636\n",
      "[step: 4182] loss: 0.01000128872692585\n",
      "[step: 4183] loss: 0.00995153933763504\n",
      "[step: 4184] loss: 0.00989606138318777\n",
      "[step: 4185] loss: 0.009881111793220043\n",
      "[step: 4186] loss: 0.009856944903731346\n",
      "[step: 4187] loss: 0.009910506196320057\n",
      "[step: 4188] loss: 0.009957697242498398\n",
      "[step: 4189] loss: 0.009967031888663769\n",
      "[step: 4190] loss: 0.009912579320371151\n",
      "[step: 4191] loss: 0.00988746527582407\n",
      "[step: 4192] loss: 0.009882593527436256\n",
      "[step: 4193] loss: 0.009852428920567036\n",
      "[step: 4194] loss: 0.009869901463389397\n",
      "[step: 4195] loss: 0.009854484349489212\n",
      "[step: 4196] loss: 0.009908179752528667\n",
      "[step: 4197] loss: 0.009939098730683327\n",
      "[step: 4198] loss: 0.009993670508265495\n",
      "[step: 4199] loss: 0.010160914622247219\n",
      "[step: 4200] loss: 0.009940619580447674\n",
      "[step: 4201] loss: 0.009939835406839848\n",
      "[step: 4202] loss: 0.010202386416494846\n",
      "[step: 4203] loss: 0.010116745717823505\n",
      "[step: 4204] loss: 0.009850558824837208\n",
      "[step: 4205] loss: 0.010030926205217838\n",
      "[step: 4206] loss: 0.010004660114645958\n",
      "[step: 4207] loss: 0.009979300200939178\n",
      "[step: 4208] loss: 0.009919365867972374\n",
      "[step: 4209] loss: 0.010073148645460606\n",
      "[step: 4210] loss: 0.00993740651756525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4211] loss: 0.009928499348461628\n",
      "[step: 4212] loss: 0.009946255944669247\n",
      "[step: 4213] loss: 0.009959937073290348\n",
      "[step: 4214] loss: 0.009851324371993542\n",
      "[step: 4215] loss: 0.009948409162461758\n",
      "[step: 4216] loss: 0.009912275709211826\n",
      "[step: 4217] loss: 0.009870830923318863\n",
      "[step: 4218] loss: 0.009852861054241657\n",
      "[step: 4219] loss: 0.00990908220410347\n",
      "[step: 4220] loss: 0.009852155111730099\n",
      "[step: 4221] loss: 0.009841111488640308\n",
      "[step: 4222] loss: 0.009829848073422909\n",
      "[step: 4223] loss: 0.00986128207296133\n",
      "[step: 4224] loss: 0.009911137633025646\n",
      "[step: 4225] loss: 0.009987518191337585\n",
      "[step: 4226] loss: 0.010249733924865723\n",
      "[step: 4227] loss: 0.009911743924021721\n",
      "[step: 4228] loss: 0.01008496806025505\n",
      "[step: 4229] loss: 0.010505057871341705\n",
      "[step: 4230] loss: 0.010194782167673111\n",
      "[step: 4231] loss: 0.010149979032576084\n",
      "[step: 4232] loss: 0.010382581502199173\n",
      "[step: 4233] loss: 0.009936443530023098\n",
      "[step: 4234] loss: 0.010331721045076847\n",
      "[step: 4235] loss: 0.009952086955308914\n",
      "[step: 4236] loss: 0.010203811340034008\n",
      "[step: 4237] loss: 0.00996872503310442\n",
      "[step: 4238] loss: 0.010098529048264027\n",
      "[step: 4239] loss: 0.009974322281777859\n",
      "[step: 4240] loss: 0.01002572476863861\n",
      "[step: 4241] loss: 0.009955368936061859\n",
      "[step: 4242] loss: 0.00995883159339428\n",
      "[step: 4243] loss: 0.009998602792620659\n",
      "[step: 4244] loss: 0.009931813925504684\n",
      "[step: 4245] loss: 0.010036863386631012\n",
      "[step: 4246] loss: 0.0098883006721735\n",
      "[step: 4247] loss: 0.009930006228387356\n",
      "[step: 4248] loss: 0.009893955662846565\n",
      "[step: 4249] loss: 0.009875178337097168\n",
      "[step: 4250] loss: 0.009922991506755352\n",
      "[step: 4251] loss: 0.009848274290561676\n",
      "[step: 4252] loss: 0.009897761046886444\n",
      "[step: 4253] loss: 0.009890854358673096\n",
      "[step: 4254] loss: 0.009834823198616505\n",
      "[step: 4255] loss: 0.009883991442620754\n",
      "[step: 4256] loss: 0.009900582022964954\n",
      "[step: 4257] loss: 0.009824133478105068\n",
      "[step: 4258] loss: 0.00991733931005001\n",
      "[step: 4259] loss: 0.009976410306990147\n",
      "[step: 4260] loss: 0.009849835187196732\n",
      "[step: 4261] loss: 0.009898445568978786\n",
      "[step: 4262] loss: 0.01003227662295103\n",
      "[step: 4263] loss: 0.00985709112137556\n",
      "[step: 4264] loss: 0.009868642315268517\n",
      "[step: 4265] loss: 0.009990384802222252\n",
      "[step: 4266] loss: 0.009845112450420856\n",
      "[step: 4267] loss: 0.00985091645270586\n",
      "[step: 4268] loss: 0.009907238185405731\n",
      "[step: 4269] loss: 0.009829689748585224\n",
      "[step: 4270] loss: 0.009821180254220963\n",
      "[step: 4271] loss: 0.00987817533314228\n",
      "[step: 4272] loss: 0.009824308566749096\n",
      "[step: 4273] loss: 0.009815259836614132\n",
      "[step: 4274] loss: 0.00985532533377409\n",
      "[step: 4275] loss: 0.009844910353422165\n",
      "[step: 4276] loss: 0.009809299372136593\n",
      "[step: 4277] loss: 0.009807495400309563\n",
      "[step: 4278] loss: 0.00982703547924757\n",
      "[step: 4279] loss: 0.009858242236077785\n",
      "[step: 4280] loss: 0.009818115271627903\n",
      "[step: 4281] loss: 0.009799414314329624\n",
      "[step: 4282] loss: 0.009800954721868038\n",
      "[step: 4283] loss: 0.009822036139667034\n",
      "[step: 4284] loss: 0.009846174158155918\n",
      "[step: 4285] loss: 0.009835724718868732\n",
      "[step: 4286] loss: 0.009837914258241653\n",
      "[step: 4287] loss: 0.009819471277296543\n",
      "[step: 4288] loss: 0.009804116562008858\n",
      "[step: 4289] loss: 0.009793456643819809\n",
      "[step: 4290] loss: 0.009786278009414673\n",
      "[step: 4291] loss: 0.009784266352653503\n",
      "[step: 4292] loss: 0.009783911518752575\n",
      "[step: 4293] loss: 0.009786603040993214\n",
      "[step: 4294] loss: 0.009795431979000568\n",
      "[step: 4295] loss: 0.009822755120694637\n",
      "[step: 4296] loss: 0.009880713187158108\n",
      "[step: 4297] loss: 0.01011188980191946\n",
      "[step: 4298] loss: 0.00996785145252943\n",
      "[step: 4299] loss: 0.009903322905302048\n",
      "[step: 4300] loss: 0.009817575104534626\n",
      "[step: 4301] loss: 0.00978982262313366\n",
      "[step: 4302] loss: 0.00980419386178255\n",
      "[step: 4303] loss: 0.009826409630477428\n",
      "[step: 4304] loss: 0.009823186323046684\n",
      "[step: 4305] loss: 0.009799513034522533\n",
      "[step: 4306] loss: 0.009781642816960812\n",
      "[step: 4307] loss: 0.009800449013710022\n",
      "[step: 4308] loss: 0.009812874719500542\n",
      "[step: 4309] loss: 0.009837924502789974\n",
      "[step: 4310] loss: 0.00981887523084879\n",
      "[step: 4311] loss: 0.00983506254851818\n",
      "[step: 4312] loss: 0.009850099682807922\n",
      "[step: 4313] loss: 0.009948414750397205\n",
      "[step: 4314] loss: 0.009914753027260303\n",
      "[step: 4315] loss: 0.009983325377106667\n",
      "[step: 4316] loss: 0.009847504086792469\n",
      "[step: 4317] loss: 0.009778471663594246\n",
      "[step: 4318] loss: 0.009791542775928974\n",
      "[step: 4319] loss: 0.009836385026574135\n",
      "[step: 4320] loss: 0.009853530675172806\n",
      "[step: 4321] loss: 0.009786243550479412\n",
      "[step: 4322] loss: 0.00978525448590517\n",
      "[step: 4323] loss: 0.009832791052758694\n",
      "[step: 4324] loss: 0.009829537011682987\n",
      "[step: 4325] loss: 0.009798511862754822\n",
      "[step: 4326] loss: 0.009770318865776062\n",
      "[step: 4327] loss: 0.009766468778252602\n",
      "[step: 4328] loss: 0.009782485663890839\n",
      "[step: 4329] loss: 0.009819856844842434\n",
      "[step: 4330] loss: 0.00993926078081131\n",
      "[step: 4331] loss: 0.009974394924938679\n",
      "[step: 4332] loss: 0.010142476297914982\n",
      "[step: 4333] loss: 0.009826705791056156\n",
      "[step: 4334] loss: 0.009789874777197838\n",
      "[step: 4335] loss: 0.00996102299541235\n",
      "[step: 4336] loss: 0.009900552220642567\n",
      "[step: 4337] loss: 0.009779990650713444\n",
      "[step: 4338] loss: 0.009830745868384838\n",
      "[step: 4339] loss: 0.009871191345155239\n",
      "[step: 4340] loss: 0.009817956015467644\n",
      "[step: 4341] loss: 0.009780192747712135\n",
      "[step: 4342] loss: 0.009859057143330574\n",
      "[step: 4343] loss: 0.00988861359655857\n",
      "[step: 4344] loss: 0.009774875827133656\n",
      "[step: 4345] loss: 0.009854109957814217\n",
      "[step: 4346] loss: 0.010000180453062057\n",
      "[step: 4347] loss: 0.009842883795499802\n",
      "[step: 4348] loss: 0.009796839207410812\n",
      "[step: 4349] loss: 0.009982234798371792\n",
      "[step: 4350] loss: 0.009901219978928566\n",
      "[step: 4351] loss: 0.009776720777153969\n",
      "[step: 4352] loss: 0.009816701523959637\n",
      "[step: 4353] loss: 0.009878608398139477\n",
      "[step: 4354] loss: 0.009910681284964085\n",
      "[step: 4355] loss: 0.009770957753062248\n",
      "[step: 4356] loss: 0.009968316182494164\n",
      "[step: 4357] loss: 0.010019642300903797\n",
      "[step: 4358] loss: 0.009884682483971119\n",
      "[step: 4359] loss: 0.009965802542865276\n",
      "[step: 4360] loss: 0.010006846860051155\n",
      "[step: 4361] loss: 0.009826852940022945\n",
      "[step: 4362] loss: 0.009916724637150764\n",
      "[step: 4363] loss: 0.009935863316059113\n",
      "[step: 4364] loss: 0.009798798710107803\n",
      "[step: 4365] loss: 0.009933721274137497\n",
      "[step: 4366] loss: 0.009827508591115475\n",
      "[step: 4367] loss: 0.00982266291975975\n",
      "[step: 4368] loss: 0.009886167012155056\n",
      "[step: 4369] loss: 0.009913753718137741\n",
      "[step: 4370] loss: 0.00979575328528881\n",
      "[step: 4371] loss: 0.010042387060821056\n",
      "[step: 4372] loss: 0.010007728822529316\n",
      "[step: 4373] loss: 0.009899122640490532\n",
      "[step: 4374] loss: 0.009962653741240501\n",
      "[step: 4375] loss: 0.009937448427081108\n",
      "[step: 4376] loss: 0.009805656038224697\n",
      "[step: 4377] loss: 0.009960445575416088\n",
      "[step: 4378] loss: 0.009817552752792835\n",
      "[step: 4379] loss: 0.009886611253023148\n",
      "[step: 4380] loss: 0.009841269813477993\n",
      "[step: 4381] loss: 0.009828305803239346\n",
      "[step: 4382] loss: 0.00983741320669651\n",
      "[step: 4383] loss: 0.009826157242059708\n",
      "[step: 4384] loss: 0.00977659597992897\n",
      "[step: 4385] loss: 0.00982211995869875\n",
      "[step: 4386] loss: 0.009767170064151287\n",
      "[step: 4387] loss: 0.009786649607121944\n",
      "[step: 4388] loss: 0.009815745055675507\n",
      "[step: 4389] loss: 0.00977639015763998\n",
      "[step: 4390] loss: 0.009771721437573433\n",
      "[step: 4391] loss: 0.009806936606764793\n",
      "[step: 4392] loss: 0.009809797629714012\n",
      "[step: 4393] loss: 0.009756249375641346\n",
      "[step: 4394] loss: 0.009763889946043491\n",
      "[step: 4395] loss: 0.009771181270480156\n",
      "[step: 4396] loss: 0.00980596523731947\n",
      "[step: 4397] loss: 0.00975146982818842\n",
      "[step: 4398] loss: 0.009753522463142872\n",
      "[step: 4399] loss: 0.009761969558894634\n",
      "[step: 4400] loss: 0.009785495698451996\n",
      "[step: 4401] loss: 0.00977264903485775\n",
      "[step: 4402] loss: 0.009748082607984543\n",
      "[step: 4403] loss: 0.009743578732013702\n",
      "[step: 4404] loss: 0.009736734442412853\n",
      "[step: 4405] loss: 0.00975771527737379\n",
      "[step: 4406] loss: 0.009770110249519348\n",
      "[step: 4407] loss: 0.009776058606803417\n",
      "[step: 4408] loss: 0.009770632721483707\n",
      "[step: 4409] loss: 0.009761977940797806\n",
      "[step: 4410] loss: 0.009771830402314663\n",
      "[step: 4411] loss: 0.009755603969097137\n",
      "[step: 4412] loss: 0.009762818925082684\n",
      "[step: 4413] loss: 0.009758364409208298\n",
      "[step: 4414] loss: 0.009782988578081131\n",
      "[step: 4415] loss: 0.009775187820196152\n",
      "[step: 4416] loss: 0.00979914516210556\n",
      "[step: 4417] loss: 0.009804717265069485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4418] loss: 0.009821128100156784\n",
      "[step: 4419] loss: 0.009783322922885418\n",
      "[step: 4420] loss: 0.009760569781064987\n",
      "[step: 4421] loss: 0.009734156541526318\n",
      "[step: 4422] loss: 0.009720123372972012\n",
      "[step: 4423] loss: 0.009721070528030396\n",
      "[step: 4424] loss: 0.009732295759022236\n",
      "[step: 4425] loss: 0.009751291945576668\n",
      "[step: 4426] loss: 0.009773151017725468\n",
      "[step: 4427] loss: 0.009832455776631832\n",
      "[step: 4428] loss: 0.009863273240625858\n",
      "[step: 4429] loss: 0.009968455880880356\n",
      "[step: 4430] loss: 0.009885030798614025\n",
      "[step: 4431] loss: 0.009833517484366894\n",
      "[step: 4432] loss: 0.009741195477545261\n",
      "[step: 4433] loss: 0.009717362932860851\n",
      "[step: 4434] loss: 0.009752183221280575\n",
      "[step: 4435] loss: 0.00978007446974516\n",
      "[step: 4436] loss: 0.009773978032171726\n",
      "[step: 4437] loss: 0.009729461744427681\n",
      "[step: 4438] loss: 0.00971229374408722\n",
      "[step: 4439] loss: 0.009728983975946903\n",
      "[step: 4440] loss: 0.009756497107446194\n",
      "[step: 4441] loss: 0.009790386073291302\n",
      "[step: 4442] loss: 0.009785579517483711\n",
      "[step: 4443] loss: 0.009802540764212608\n",
      "[step: 4444] loss: 0.009788022376596928\n",
      "[step: 4445] loss: 0.009801740758121014\n",
      "[step: 4446] loss: 0.009779959917068481\n",
      "[step: 4447] loss: 0.009775291197001934\n",
      "[step: 4448] loss: 0.009744991548359394\n",
      "[step: 4449] loss: 0.009722999297082424\n",
      "[step: 4450] loss: 0.00970645621418953\n",
      "[step: 4451] loss: 0.009702048264443874\n",
      "[step: 4452] loss: 0.009707479737699032\n",
      "[step: 4453] loss: 0.009718342684209347\n",
      "[step: 4454] loss: 0.009735658764839172\n",
      "[step: 4455] loss: 0.009751184843480587\n",
      "[step: 4456] loss: 0.009790400974452496\n",
      "[step: 4457] loss: 0.009821133688092232\n",
      "[step: 4458] loss: 0.009910228662192822\n",
      "[step: 4459] loss: 0.009875760413706303\n",
      "[step: 4460] loss: 0.009860684163868427\n",
      "[step: 4461] loss: 0.009765013121068478\n",
      "[step: 4462] loss: 0.009708210825920105\n",
      "[step: 4463] loss: 0.009706320241093636\n",
      "[step: 4464] loss: 0.009742813184857368\n",
      "[step: 4465] loss: 0.009774391539394855\n",
      "[step: 4466] loss: 0.009745465591549873\n",
      "[step: 4467] loss: 0.009710695594549179\n",
      "[step: 4468] loss: 0.009693616069853306\n",
      "[step: 4469] loss: 0.009702387265861034\n",
      "[step: 4470] loss: 0.009728691540658474\n",
      "[step: 4471] loss: 0.009764534421265125\n",
      "[step: 4472] loss: 0.009842969477176666\n",
      "[step: 4473] loss: 0.009869148954749107\n",
      "[step: 4474] loss: 0.00995596218854189\n",
      "[step: 4475] loss: 0.009852014482021332\n",
      "[step: 4476] loss: 0.009760940447449684\n",
      "[step: 4477] loss: 0.00969754345715046\n",
      "[step: 4478] loss: 0.009709653444588184\n",
      "[step: 4479] loss: 0.009757092222571373\n",
      "[step: 4480] loss: 0.009760775603353977\n",
      "[step: 4481] loss: 0.009726968593895435\n",
      "[step: 4482] loss: 0.009691623970866203\n",
      "[step: 4483] loss: 0.009701618924736977\n",
      "[step: 4484] loss: 0.009736215695738792\n",
      "[step: 4485] loss: 0.00975123606622219\n",
      "[step: 4486] loss: 0.009753402322530746\n",
      "[step: 4487] loss: 0.009734056890010834\n",
      "[step: 4488] loss: 0.009726237505674362\n",
      "[step: 4489] loss: 0.00971448514610529\n",
      "[step: 4490] loss: 0.00971700344234705\n",
      "[step: 4491] loss: 0.009725386276841164\n",
      "[step: 4492] loss: 0.009757183492183685\n",
      "[step: 4493] loss: 0.009800379164516926\n",
      "[step: 4494] loss: 0.009897963143885136\n",
      "[step: 4495] loss: 0.009865106083452702\n",
      "[step: 4496] loss: 0.00982693862169981\n",
      "[step: 4497] loss: 0.009737067855894566\n",
      "[step: 4498] loss: 0.009684713557362556\n",
      "[step: 4499] loss: 0.00969823356717825\n",
      "[step: 4500] loss: 0.009740282781422138\n",
      "[step: 4501] loss: 0.009757845662534237\n",
      "[step: 4502] loss: 0.00971715897321701\n",
      "[step: 4503] loss: 0.00968176033347845\n",
      "[step: 4504] loss: 0.009677870199084282\n",
      "[step: 4505] loss: 0.009700953960418701\n",
      "[step: 4506] loss: 0.00973790418356657\n",
      "[step: 4507] loss: 0.009768698364496231\n",
      "[step: 4508] loss: 0.009821061976253986\n",
      "[step: 4509] loss: 0.009833628311753273\n",
      "[step: 4510] loss: 0.00987340696156025\n",
      "[step: 4511] loss: 0.009812699630856514\n",
      "[step: 4512] loss: 0.009752499870955944\n",
      "[step: 4513] loss: 0.009690744802355766\n",
      "[step: 4514] loss: 0.009671063162386417\n",
      "[step: 4515] loss: 0.00969408918172121\n",
      "[step: 4516] loss: 0.009723437950015068\n",
      "[step: 4517] loss: 0.009728607721626759\n",
      "[step: 4518] loss: 0.009698262438178062\n",
      "[step: 4519] loss: 0.009672043845057487\n",
      "[step: 4520] loss: 0.009665044955909252\n",
      "[step: 4521] loss: 0.009676489047706127\n",
      "[step: 4522] loss: 0.00970055628567934\n",
      "[step: 4523] loss: 0.00973315816372633\n",
      "[step: 4524] loss: 0.009794541634619236\n",
      "[step: 4525] loss: 0.009849580004811287\n",
      "[step: 4526] loss: 0.009946374222636223\n",
      "[step: 4527] loss: 0.009881136938929558\n",
      "[step: 4528] loss: 0.00979597121477127\n",
      "[step: 4529] loss: 0.00969682540744543\n",
      "[step: 4530] loss: 0.009665563702583313\n",
      "[step: 4531] loss: 0.009706554003059864\n",
      "[step: 4532] loss: 0.00974577758461237\n",
      "[step: 4533] loss: 0.009733933955430984\n",
      "[step: 4534] loss: 0.009680193848907948\n",
      "[step: 4535] loss: 0.009661128744482994\n",
      "[step: 4536] loss: 0.009685707278549671\n",
      "[step: 4537] loss: 0.009717898443341255\n",
      "[step: 4538] loss: 0.009734353050589561\n",
      "[step: 4539] loss: 0.009719827212393284\n",
      "[step: 4540] loss: 0.009701280854642391\n",
      "[step: 4541] loss: 0.009682257659733295\n",
      "[step: 4542] loss: 0.00967272650450468\n",
      "[step: 4543] loss: 0.009665933437645435\n",
      "[step: 4544] loss: 0.009665115736424923\n",
      "[step: 4545] loss: 0.00966953206807375\n",
      "[step: 4546] loss: 0.009686016477644444\n",
      "[step: 4547] loss: 0.009722939692437649\n",
      "[step: 4548] loss: 0.009813000448048115\n",
      "[step: 4549] loss: 0.009905052371323109\n",
      "[step: 4550] loss: 0.009996728971600533\n",
      "[step: 4551] loss: 0.00987155269831419\n",
      "[step: 4552] loss: 0.00974188931286335\n",
      "[step: 4553] loss: 0.00965915061533451\n",
      "[step: 4554] loss: 0.009712795726954937\n",
      "[step: 4555] loss: 0.009791712276637554\n",
      "[step: 4556] loss: 0.009742866270244122\n",
      "[step: 4557] loss: 0.009676101617515087\n",
      "[step: 4558] loss: 0.009666584432125092\n",
      "[step: 4559] loss: 0.009706547483801842\n",
      "[step: 4560] loss: 0.009751871228218079\n",
      "[step: 4561] loss: 0.009714647196233273\n",
      "[step: 4562] loss: 0.009669815190136433\n",
      "[step: 4563] loss: 0.00964515283703804\n",
      "[step: 4564] loss: 0.009643301367759705\n",
      "[step: 4565] loss: 0.00966061744838953\n",
      "[step: 4566] loss: 0.009698696434497833\n",
      "[step: 4567] loss: 0.009766430594027042\n",
      "[step: 4568] loss: 0.00983908399939537\n",
      "[step: 4569] loss: 0.009904081001877785\n",
      "[step: 4570] loss: 0.009803260676562786\n",
      "[step: 4571] loss: 0.009695746004581451\n",
      "[step: 4572] loss: 0.009644134901463985\n",
      "[step: 4573] loss: 0.009670617058873177\n",
      "[step: 4574] loss: 0.00972822867333889\n",
      "[step: 4575] loss: 0.009729270823299885\n",
      "[step: 4576] loss: 0.009678469970822334\n",
      "[step: 4577] loss: 0.009639257565140724\n",
      "[step: 4578] loss: 0.009650196880102158\n",
      "[step: 4579] loss: 0.009683053940534592\n",
      "[step: 4580] loss: 0.009703424759209156\n",
      "[step: 4581] loss: 0.009706749580800533\n",
      "[step: 4582] loss: 0.009688989259302616\n",
      "[step: 4583] loss: 0.009671565145254135\n",
      "[step: 4584] loss: 0.009655145928263664\n",
      "[step: 4585] loss: 0.00964847207069397\n",
      "[step: 4586] loss: 0.009647428058087826\n",
      "[step: 4587] loss: 0.009653139859437943\n",
      "[step: 4588] loss: 0.009671038947999477\n",
      "[step: 4589] loss: 0.009715874679386616\n",
      "[step: 4590] loss: 0.009779863990843296\n",
      "[step: 4591] loss: 0.009870510548353195\n",
      "[step: 4592] loss: 0.00987314898520708\n",
      "[step: 4593] loss: 0.00981220044195652\n",
      "[step: 4594] loss: 0.009703053161501884\n",
      "[step: 4595] loss: 0.009632144123315811\n",
      "[step: 4596] loss: 0.009650112129747868\n",
      "[step: 4597] loss: 0.009706736542284489\n",
      "[step: 4598] loss: 0.009720821864902973\n",
      "[step: 4599] loss: 0.00967669952660799\n",
      "[step: 4600] loss: 0.009632658213376999\n",
      "[step: 4601] loss: 0.00962158851325512\n",
      "[step: 4602] loss: 0.00964409951120615\n",
      "[step: 4603] loss: 0.009688206017017365\n",
      "[step: 4604] loss: 0.009734424762427807\n",
      "[step: 4605] loss: 0.009780422784388065\n",
      "[step: 4606] loss: 0.009787376038730145\n",
      "[step: 4607] loss: 0.00977589376270771\n",
      "[step: 4608] loss: 0.009734680876135826\n",
      "[step: 4609] loss: 0.009675290435552597\n",
      "[step: 4610] loss: 0.009626672603189945\n",
      "[step: 4611] loss: 0.009612499736249447\n",
      "[step: 4612] loss: 0.009630645625293255\n",
      "[step: 4613] loss: 0.009657737798988819\n",
      "[step: 4614] loss: 0.00967128574848175\n",
      "[step: 4615] loss: 0.009661880321800709\n",
      "[step: 4616] loss: 0.00963966827839613\n",
      "[step: 4617] loss: 0.009618433192372322\n",
      "[step: 4618] loss: 0.009605510160326958\n",
      "[step: 4619] loss: 0.009600901044905186\n",
      "[step: 4620] loss: 0.009602773003280163\n",
      "[step: 4621] loss: 0.009611439891159534\n",
      "[step: 4622] loss: 0.009633459150791168\n",
      "[step: 4623] loss: 0.009688912890851498\n",
      "[step: 4624] loss: 0.009822734631597996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4625] loss: 0.010051547549664974\n",
      "[step: 4626] loss: 0.010127267800271511\n",
      "[step: 4627] loss: 0.010017897933721542\n",
      "[step: 4628] loss: 0.00977358128875494\n",
      "[step: 4629] loss: 0.009620781056582928\n",
      "[step: 4630] loss: 0.009694420732557774\n",
      "[step: 4631] loss: 0.009781927801668644\n",
      "[step: 4632] loss: 0.00969026144593954\n",
      "[step: 4633] loss: 0.009623080492019653\n",
      "[step: 4634] loss: 0.009687677025794983\n",
      "[step: 4635] loss: 0.009712086990475655\n",
      "[step: 4636] loss: 0.009652167558670044\n",
      "[step: 4637] loss: 0.009621975012123585\n",
      "[step: 4638] loss: 0.009637661278247833\n",
      "[step: 4639] loss: 0.009678528644144535\n",
      "[step: 4640] loss: 0.009713884443044662\n",
      "[step: 4641] loss: 0.009656233713030815\n",
      "[step: 4642] loss: 0.009606443345546722\n",
      "[step: 4643] loss: 0.009601077996194363\n",
      "[step: 4644] loss: 0.00962070096284151\n",
      "[step: 4645] loss: 0.009638783521950245\n",
      "[step: 4646] loss: 0.00964504573494196\n",
      "[step: 4647] loss: 0.00964236631989479\n",
      "[step: 4648] loss: 0.00962448213249445\n",
      "[step: 4649] loss: 0.009599528275430202\n",
      "[step: 4650] loss: 0.009586057625710964\n",
      "[step: 4651] loss: 0.009588800370693207\n",
      "[step: 4652] loss: 0.009596945717930794\n",
      "[step: 4653] loss: 0.009603086858987808\n",
      "[step: 4654] loss: 0.009614566341042519\n",
      "[step: 4655] loss: 0.009639839641749859\n",
      "[step: 4656] loss: 0.009680069983005524\n",
      "[step: 4657] loss: 0.009729246608912945\n",
      "[step: 4658] loss: 0.009780860505998135\n",
      "[step: 4659] loss: 0.009826975874602795\n",
      "[step: 4660] loss: 0.00982688833028078\n",
      "[step: 4661] loss: 0.009790820069611073\n",
      "[step: 4662] loss: 0.0097011374309659\n",
      "[step: 4663] loss: 0.009609299711883068\n",
      "[step: 4664] loss: 0.0095840934664011\n",
      "[step: 4665] loss: 0.009615017101168633\n",
      "[step: 4666] loss: 0.009646086022257805\n",
      "[step: 4667] loss: 0.009657665155827999\n",
      "[step: 4668] loss: 0.009636730886995792\n",
      "[step: 4669] loss: 0.009599490091204643\n",
      "[step: 4670] loss: 0.00957104004919529\n",
      "[step: 4671] loss: 0.009570897556841373\n",
      "[step: 4672] loss: 0.009592235088348389\n",
      "[step: 4673] loss: 0.009620536118745804\n",
      "[step: 4674] loss: 0.009654398076236248\n",
      "[step: 4675] loss: 0.009697306901216507\n",
      "[step: 4676] loss: 0.009735630825161934\n",
      "[step: 4677] loss: 0.009778345003724098\n",
      "[step: 4678] loss: 0.009779341518878937\n",
      "[step: 4679] loss: 0.009738272987306118\n",
      "[step: 4680] loss: 0.009656551294028759\n",
      "[step: 4681] loss: 0.009589379653334618\n",
      "[step: 4682] loss: 0.009565159678459167\n",
      "[step: 4683] loss: 0.00958158913999796\n",
      "[step: 4684] loss: 0.009616759605705738\n",
      "[step: 4685] loss: 0.009645986370742321\n",
      "[step: 4686] loss: 0.009652088396251202\n",
      "[step: 4687] loss: 0.009626186452805996\n",
      "[step: 4688] loss: 0.009596438147127628\n",
      "[step: 4689] loss: 0.009571600705385208\n",
      "[step: 4690] loss: 0.009556649252772331\n",
      "[step: 4691] loss: 0.009548116475343704\n",
      "[step: 4692] loss: 0.009544451721012592\n",
      "[step: 4693] loss: 0.009544165804982185\n",
      "[step: 4694] loss: 0.009546367451548576\n",
      "[step: 4695] loss: 0.00955300685018301\n",
      "[step: 4696] loss: 0.009572870098054409\n",
      "[step: 4697] loss: 0.009635724127292633\n",
      "[step: 4698] loss: 0.009787104092538357\n",
      "[step: 4699] loss: 0.010082746855914593\n",
      "[step: 4700] loss: 0.010127931833267212\n",
      "[step: 4701] loss: 0.010056592524051666\n",
      "[step: 4702] loss: 0.009880444034934044\n",
      "[step: 4703] loss: 0.009584394283592701\n",
      "[step: 4704] loss: 0.00973986741155386\n",
      "[step: 4705] loss: 0.009856751188635826\n",
      "[step: 4706] loss: 0.009622824378311634\n",
      "[step: 4707] loss: 0.00972050242125988\n",
      "[step: 4708] loss: 0.009737370535731316\n",
      "[step: 4709] loss: 0.00971165020018816\n",
      "[step: 4710] loss: 0.009758702479302883\n",
      "[step: 4711] loss: 0.009619166143238544\n",
      "[step: 4712] loss: 0.009822937659919262\n",
      "[step: 4713] loss: 0.009808361530303955\n",
      "[step: 4714] loss: 0.009569826535880566\n",
      "[step: 4715] loss: 0.00989613588899374\n",
      "[step: 4716] loss: 0.009690854698419571\n",
      "[step: 4717] loss: 0.009644352830946445\n",
      "[step: 4718] loss: 0.009795929305255413\n",
      "[step: 4719] loss: 0.009562540799379349\n",
      "[step: 4720] loss: 0.009703547693789005\n",
      "[step: 4721] loss: 0.009573595598340034\n",
      "[step: 4722] loss: 0.009631727822124958\n",
      "[step: 4723] loss: 0.009626594372093678\n",
      "[step: 4724] loss: 0.009568714536726475\n",
      "[step: 4725] loss: 0.009650132618844509\n",
      "[step: 4726] loss: 0.009547009132802486\n",
      "[step: 4727] loss: 0.009607725776731968\n",
      "[step: 4728] loss: 0.009553888812661171\n",
      "[step: 4729] loss: 0.00956560019403696\n",
      "[step: 4730] loss: 0.009578689932823181\n",
      "[step: 4731] loss: 0.00953581277281046\n",
      "[step: 4732] loss: 0.009579021483659744\n",
      "[step: 4733] loss: 0.009532677941024303\n",
      "[step: 4734] loss: 0.009549993090331554\n",
      "[step: 4735] loss: 0.009545392356812954\n",
      "[step: 4736] loss: 0.009517552331089973\n",
      "[step: 4737] loss: 0.009541677311062813\n",
      "[step: 4738] loss: 0.009514451026916504\n",
      "[step: 4739] loss: 0.009520107880234718\n",
      "[step: 4740] loss: 0.009525418281555176\n",
      "[step: 4741] loss: 0.009501648135483265\n",
      "[step: 4742] loss: 0.009514300152659416\n",
      "[step: 4743] loss: 0.009506474249064922\n",
      "[step: 4744] loss: 0.009493526071310043\n",
      "[step: 4745] loss: 0.009506024420261383\n",
      "[step: 4746] loss: 0.009494313970208168\n",
      "[step: 4747] loss: 0.00948331132531166\n",
      "[step: 4748] loss: 0.00949251838028431\n",
      "[step: 4749] loss: 0.009488135576248169\n",
      "[step: 4750] loss: 0.009481005370616913\n",
      "[step: 4751] loss: 0.00950054544955492\n",
      "[step: 4752] loss: 0.009546762332320213\n",
      "[step: 4753] loss: 0.009710755199193954\n",
      "[step: 4754] loss: 0.0102522699162364\n",
      "[step: 4755] loss: 0.010621093213558197\n",
      "[step: 4756] loss: 0.010607428848743439\n",
      "[step: 4757] loss: 0.00969669409096241\n",
      "[step: 4758] loss: 0.009691419079899788\n",
      "[step: 4759] loss: 0.010155947878956795\n",
      "[step: 4760] loss: 0.009616972878575325\n",
      "[step: 4761] loss: 0.009806238114833832\n",
      "[step: 4762] loss: 0.009881408885121346\n",
      "[step: 4763] loss: 0.009586356580257416\n",
      "[step: 4764] loss: 0.009920471347868443\n",
      "[step: 4765] loss: 0.009570823982357979\n",
      "[step: 4766] loss: 0.009734917432069778\n",
      "[step: 4767] loss: 0.009668642655014992\n",
      "[step: 4768] loss: 0.009543010033667088\n",
      "[step: 4769] loss: 0.009677684865891933\n",
      "[step: 4770] loss: 0.009552220813930035\n",
      "[step: 4771] loss: 0.009561767801642418\n",
      "[step: 4772] loss: 0.00964683759957552\n",
      "[step: 4773] loss: 0.00949839036911726\n",
      "[step: 4774] loss: 0.009583035483956337\n",
      "[step: 4775] loss: 0.009579051285982132\n",
      "[step: 4776] loss: 0.009473510086536407\n",
      "[step: 4777] loss: 0.00955213513225317\n",
      "[step: 4778] loss: 0.009567680768668652\n",
      "[step: 4779] loss: 0.00947581883519888\n",
      "[step: 4780] loss: 0.00948952417820692\n",
      "[step: 4781] loss: 0.009549940004944801\n",
      "[step: 4782] loss: 0.009498967789113522\n",
      "[step: 4783] loss: 0.00944595318287611\n",
      "[step: 4784] loss: 0.009487018920481205\n",
      "[step: 4785] loss: 0.009488394483923912\n",
      "[step: 4786] loss: 0.009437738917768002\n",
      "[step: 4787] loss: 0.00943917315453291\n",
      "[step: 4788] loss: 0.00946348812431097\n",
      "[step: 4789] loss: 0.009448342025279999\n",
      "[step: 4790] loss: 0.009401402436196804\n",
      "[step: 4791] loss: 0.009415913373231888\n",
      "[step: 4792] loss: 0.009445978328585625\n",
      "[step: 4793] loss: 0.00944423396140337\n",
      "[step: 4794] loss: 0.009411849081516266\n",
      "[step: 4795] loss: 0.009378821589052677\n",
      "[step: 4796] loss: 0.009360067546367645\n",
      "[step: 4797] loss: 0.009352489374577999\n",
      "[step: 4798] loss: 0.0093589061871171\n",
      "[step: 4799] loss: 0.009384779259562492\n",
      "[step: 4800] loss: 0.009443309158086777\n",
      "[step: 4801] loss: 0.009670842438936234\n",
      "[step: 4802] loss: 0.010024383664131165\n",
      "[step: 4803] loss: 0.010719561949372292\n",
      "[step: 4804] loss: 0.00974186323583126\n",
      "[step: 4805] loss: 0.010360990650951862\n",
      "[step: 4806] loss: 0.011215290986001492\n",
      "[step: 4807] loss: 0.010229275561869144\n",
      "[step: 4808] loss: 0.010420437902212143\n",
      "[step: 4809] loss: 0.009859452955424786\n",
      "[step: 4810] loss: 0.010014823637902737\n",
      "[step: 4811] loss: 0.010036160238087177\n",
      "[step: 4812] loss: 0.009759748354554176\n",
      "[step: 4813] loss: 0.009968524798750877\n",
      "[step: 4814] loss: 0.009741510264575481\n",
      "[step: 4815] loss: 0.009777925908565521\n",
      "[step: 4816] loss: 0.009786268696188927\n",
      "[step: 4817] loss: 0.009590941481292248\n",
      "[step: 4818] loss: 0.00973409041762352\n",
      "[step: 4819] loss: 0.009468037635087967\n",
      "[step: 4820] loss: 0.009645394049584866\n",
      "[step: 4821] loss: 0.009469252079725266\n",
      "[step: 4822] loss: 0.00957909319549799\n",
      "[step: 4823] loss: 0.00945084635168314\n",
      "[step: 4824] loss: 0.00955139473080635\n",
      "[step: 4825] loss: 0.009420068934559822\n",
      "[step: 4826] loss: 0.009483734145760536\n",
      "[step: 4827] loss: 0.009361110627651215\n",
      "[step: 4828] loss: 0.009467948228120804\n",
      "[step: 4829] loss: 0.009348206222057343\n",
      "[step: 4830] loss: 0.009406225755810738\n",
      "[step: 4831] loss: 0.00932619534432888\n",
      "[step: 4832] loss: 0.009375727735459805\n",
      "[step: 4833] loss: 0.009299165569245815\n",
      "[step: 4834] loss: 0.009327542036771774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4835] loss: 0.00929216481745243\n",
      "[step: 4836] loss: 0.00929534062743187\n",
      "[step: 4837] loss: 0.00926232524216175\n",
      "[step: 4838] loss: 0.009259303100407124\n",
      "[step: 4839] loss: 0.009248008951544762\n",
      "[step: 4840] loss: 0.009229661896824837\n",
      "[step: 4841] loss: 0.00923407543450594\n",
      "[step: 4842] loss: 0.00921567715704441\n",
      "[step: 4843] loss: 0.009218894876539707\n",
      "[step: 4844] loss: 0.009194069541990757\n",
      "[step: 4845] loss: 0.009211411699652672\n",
      "[step: 4846] loss: 0.00921657681465149\n",
      "[step: 4847] loss: 0.009333763271570206\n",
      "[step: 4848] loss: 0.009574156254529953\n",
      "[step: 4849] loss: 0.010286825709044933\n",
      "[step: 4850] loss: 0.010907559655606747\n",
      "[step: 4851] loss: 0.009989086538553238\n",
      "[step: 4852] loss: 0.009798135608434677\n",
      "[step: 4853] loss: 0.009963677264750004\n",
      "[step: 4854] loss: 0.009520037099719048\n",
      "[step: 4855] loss: 0.009739925153553486\n",
      "[step: 4856] loss: 0.00958456564694643\n",
      "[step: 4857] loss: 0.00951660331338644\n",
      "[step: 4858] loss: 0.00948315393179655\n",
      "[step: 4859] loss: 0.009490517899394035\n",
      "[step: 4860] loss: 0.009402063675224781\n",
      "[step: 4861] loss: 0.009478571824729443\n",
      "[step: 4862] loss: 0.00931126531213522\n",
      "[step: 4863] loss: 0.009407397359609604\n",
      "[step: 4864] loss: 0.009294142946600914\n",
      "[step: 4865] loss: 0.009382781572639942\n",
      "[step: 4866] loss: 0.009299356490373611\n",
      "[step: 4867] loss: 0.00933869183063507\n",
      "[step: 4868] loss: 0.009222137741744518\n",
      "[step: 4869] loss: 0.009276504628360271\n",
      "[step: 4870] loss: 0.009170827455818653\n",
      "[step: 4871] loss: 0.00924462080001831\n",
      "[step: 4872] loss: 0.009149705991148949\n",
      "[step: 4873] loss: 0.009185743518173695\n",
      "[step: 4874] loss: 0.00914259534329176\n",
      "[step: 4875] loss: 0.009141635149717331\n",
      "[step: 4876] loss: 0.009130053222179413\n",
      "[step: 4877] loss: 0.009111258201301098\n",
      "[step: 4878] loss: 0.009098018519580364\n",
      "[step: 4879] loss: 0.009071717038750648\n",
      "[step: 4880] loss: 0.00907454825937748\n",
      "[step: 4881] loss: 0.009051303379237652\n",
      "[step: 4882] loss: 0.009062393568456173\n",
      "[step: 4883] loss: 0.00903849396854639\n",
      "[step: 4884] loss: 0.009045448154211044\n",
      "[step: 4885] loss: 0.009030921384692192\n",
      "[step: 4886] loss: 0.009040667675435543\n",
      "[step: 4887] loss: 0.009080085903406143\n",
      "[step: 4888] loss: 0.009148364886641502\n",
      "[step: 4889] loss: 0.009359451942145824\n",
      "[step: 4890] loss: 0.009609282948076725\n",
      "[step: 4891] loss: 0.010126607492566109\n",
      "[step: 4892] loss: 0.009511944837868214\n",
      "[step: 4893] loss: 0.009148345328867435\n",
      "[step: 4894] loss: 0.009201111271977425\n",
      "[step: 4895] loss: 0.00932013988494873\n",
      "[step: 4896] loss: 0.009530358947813511\n",
      "[step: 4897] loss: 0.009275131858885288\n",
      "[step: 4898] loss: 0.009064579382538795\n",
      "[step: 4899] loss: 0.009091055952012539\n",
      "[step: 4900] loss: 0.009165678173303604\n",
      "[step: 4901] loss: 0.009126586839556694\n",
      "[step: 4902] loss: 0.009078556671738625\n",
      "[step: 4903] loss: 0.009023290127515793\n",
      "[step: 4904] loss: 0.009112515486776829\n",
      "[step: 4905] loss: 0.009154201485216618\n",
      "[step: 4906] loss: 0.00918272603303194\n",
      "[step: 4907] loss: 0.009037970565259457\n",
      "[step: 4908] loss: 0.009023393504321575\n",
      "[step: 4909] loss: 0.009258036501705647\n",
      "[step: 4910] loss: 0.0092427683994174\n",
      "[step: 4911] loss: 0.009122947230935097\n",
      "[step: 4912] loss: 0.009005190804600716\n",
      "[step: 4913] loss: 0.00929345190525055\n",
      "[step: 4914] loss: 0.00942579098045826\n",
      "[step: 4915] loss: 0.009188437834382057\n",
      "[step: 4916] loss: 0.009010400623083115\n",
      "[step: 4917] loss: 0.009176727384328842\n",
      "[step: 4918] loss: 0.00933205895125866\n",
      "[step: 4919] loss: 0.009083508513867855\n",
      "[step: 4920] loss: 0.009020747616887093\n",
      "[step: 4921] loss: 0.009211497381329536\n",
      "[step: 4922] loss: 0.009392875246703625\n",
      "[step: 4923] loss: 0.0090717114508152\n",
      "[step: 4924] loss: 0.008983033709228039\n",
      "[step: 4925] loss: 0.009118909016251564\n",
      "[step: 4926] loss: 0.009132342413067818\n",
      "[step: 4927] loss: 0.00893933791667223\n",
      "[step: 4928] loss: 0.009005009196698666\n",
      "[step: 4929] loss: 0.00907362811267376\n",
      "[step: 4930] loss: 0.009113771840929985\n",
      "[step: 4931] loss: 0.008906600065529346\n",
      "[step: 4932] loss: 0.009031401947140694\n",
      "[step: 4933] loss: 0.009149366989731789\n",
      "[step: 4934] loss: 0.009124074131250381\n",
      "[step: 4935] loss: 0.008895144797861576\n",
      "[step: 4936] loss: 0.009010698646306992\n",
      "[step: 4937] loss: 0.009104627184569836\n",
      "[step: 4938] loss: 0.009156347252428532\n",
      "[step: 4939] loss: 0.00890363473445177\n",
      "[step: 4940] loss: 0.009010122157633305\n",
      "[step: 4941] loss: 0.009259270504117012\n",
      "[step: 4942] loss: 0.00926078949123621\n",
      "[step: 4943] loss: 0.008994992822408676\n",
      "[step: 4944] loss: 0.008929715491831303\n",
      "[step: 4945] loss: 0.00915201660245657\n",
      "[step: 4946] loss: 0.009253116324543953\n",
      "[step: 4947] loss: 0.009016761556267738\n",
      "[step: 4948] loss: 0.008921095170080662\n",
      "[step: 4949] loss: 0.00904415175318718\n",
      "[step: 4950] loss: 0.00906835962086916\n",
      "[step: 4951] loss: 0.008893519639968872\n",
      "[step: 4952] loss: 0.008904419839382172\n",
      "[step: 4953] loss: 0.008957837708294392\n",
      "[step: 4954] loss: 0.009021137841045856\n",
      "[step: 4955] loss: 0.008943963795900345\n",
      "[step: 4956] loss: 0.008882074616849422\n",
      "[step: 4957] loss: 0.008852588012814522\n",
      "[step: 4958] loss: 0.008895635604858398\n",
      "[step: 4959] loss: 0.008941440843045712\n",
      "[step: 4960] loss: 0.008927023969590664\n",
      "[step: 4961] loss: 0.008892117999494076\n",
      "[step: 4962] loss: 0.008813310414552689\n",
      "[step: 4963] loss: 0.00885650422424078\n",
      "[step: 4964] loss: 0.008894070982933044\n",
      "[step: 4965] loss: 0.008966402150690556\n",
      "[step: 4966] loss: 0.008954769931733608\n",
      "[step: 4967] loss: 0.008920948952436447\n",
      "[step: 4968] loss: 0.008890965953469276\n",
      "[step: 4969] loss: 0.008857070468366146\n",
      "[step: 4970] loss: 0.008822495117783546\n",
      "[step: 4971] loss: 0.008787292055785656\n",
      "[step: 4972] loss: 0.008784427307546139\n",
      "[step: 4973] loss: 0.008770003914833069\n",
      "[step: 4974] loss: 0.008778098039329052\n",
      "[step: 4975] loss: 0.008775358088314533\n",
      "[step: 4976] loss: 0.008814881555736065\n",
      "[step: 4977] loss: 0.00892516691237688\n",
      "[step: 4978] loss: 0.009311784990131855\n",
      "[step: 4979] loss: 0.010259931907057762\n",
      "[step: 4980] loss: 0.010191837325692177\n",
      "[step: 4981] loss: 0.009622427634894848\n",
      "[step: 4982] loss: 0.009158643893897533\n",
      "[step: 4983] loss: 0.009705835022032261\n",
      "[step: 4984] loss: 0.009319525212049484\n",
      "[step: 4985] loss: 0.009135234169661999\n",
      "[step: 4986] loss: 0.00908004492521286\n",
      "[step: 4987] loss: 0.009115299209952354\n",
      "[step: 4988] loss: 0.00907388050109148\n",
      "[step: 4989] loss: 0.008956734091043472\n",
      "[step: 4990] loss: 0.009032659232616425\n",
      "[step: 4991] loss: 0.009018328972160816\n",
      "[step: 4992] loss: 0.00892319343984127\n",
      "[step: 4993] loss: 0.00899128895252943\n",
      "[step: 4994] loss: 0.008907676674425602\n",
      "[step: 4995] loss: 0.008878358639776707\n",
      "[step: 4996] loss: 0.008956728503108025\n",
      "[step: 4997] loss: 0.00891131442040205\n",
      "[step: 4998] loss: 0.008848624303936958\n",
      "[step: 4999] loss: 0.00884481891989708\n",
      "[step: 5000] loss: 0.008883011527359486\n",
      "[step: 5001] loss: 0.008870097808539867\n",
      "[step: 5002] loss: 0.008842413313686848\n",
      "[step: 5003] loss: 0.008833942003548145\n",
      "[step: 5004] loss: 0.00882700178772211\n",
      "[step: 5005] loss: 0.008792517706751823\n",
      "[step: 5006] loss: 0.008802428841590881\n",
      "[step: 5007] loss: 0.008808359503746033\n",
      "[step: 5008] loss: 0.008799168281257153\n",
      "[step: 5009] loss: 0.008795282803475857\n",
      "[step: 5010] loss: 0.008784324862062931\n",
      "[step: 5011] loss: 0.00877434853464365\n",
      "[step: 5012] loss: 0.008768259547650814\n",
      "[step: 5013] loss: 0.008763848803937435\n",
      "[step: 5014] loss: 0.008766322396695614\n",
      "[step: 5015] loss: 0.008758779615163803\n",
      "[step: 5016] loss: 0.008757883682847023\n",
      "[step: 5017] loss: 0.008762122131884098\n",
      "[step: 5018] loss: 0.008766848593950272\n",
      "[step: 5019] loss: 0.008794698864221573\n",
      "[step: 5020] loss: 0.008831314742565155\n",
      "[step: 5021] loss: 0.008938314393162727\n",
      "[step: 5022] loss: 0.008929415605962276\n",
      "[step: 5023] loss: 0.008878142572939396\n",
      "[step: 5024] loss: 0.008750149980187416\n",
      "[step: 5025] loss: 0.008710450492799282\n",
      "[step: 5026] loss: 0.00878030527383089\n",
      "[step: 5027] loss: 0.008905218914151192\n",
      "[step: 5028] loss: 0.00919361226260662\n",
      "[step: 5029] loss: 0.009075119160115719\n",
      "[step: 5030] loss: 0.00886722281575203\n",
      "[step: 5031] loss: 0.00874983798712492\n",
      "[step: 5032] loss: 0.008882670663297176\n",
      "[step: 5033] loss: 0.008893751539289951\n",
      "[step: 5034] loss: 0.008724753744900227\n",
      "[step: 5035] loss: 0.008735462091863155\n",
      "[step: 5036] loss: 0.00882444716989994\n",
      "[step: 5037] loss: 0.008842402137815952\n",
      "[step: 5038] loss: 0.008769604377448559\n",
      "[step: 5039] loss: 0.008680925704538822\n",
      "[step: 5040] loss: 0.008738392032682896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 5041] loss: 0.008811242878437042\n",
      "[step: 5042] loss: 0.008784172125160694\n",
      "[step: 5043] loss: 0.008683389984071255\n",
      "[step: 5044] loss: 0.008667689748108387\n",
      "[step: 5045] loss: 0.008718246594071388\n",
      "[step: 5046] loss: 0.008778177201747894\n",
      "[step: 5047] loss: 0.008828544989228249\n",
      "[step: 5048] loss: 0.008871141821146011\n",
      "[step: 5049] loss: 0.008818745613098145\n",
      "[step: 5050] loss: 0.008698956109583378\n",
      "[step: 5051] loss: 0.008654417470097542\n",
      "[step: 5052] loss: 0.00878670159727335\n",
      "[step: 5053] loss: 0.008932397700846195\n",
      "[step: 5054] loss: 0.009011147543787956\n",
      "[step: 5055] loss: 0.008911345154047012\n",
      "[step: 5056] loss: 0.008726018480956554\n",
      "[step: 5057] loss: 0.008732162415981293\n",
      "[step: 5058] loss: 0.008840593509376049\n",
      "[step: 5059] loss: 0.008906046859920025\n",
      "[step: 5060] loss: 0.008800189010798931\n",
      "[step: 5061] loss: 0.008700192905962467\n",
      "[step: 5062] loss: 0.008686362765729427\n",
      "[step: 5063] loss: 0.008743761107325554\n",
      "[step: 5064] loss: 0.008908821269869804\n",
      "[step: 5065] loss: 0.00906423106789589\n",
      "[step: 5066] loss: 0.008902820758521557\n",
      "[step: 5067] loss: 0.008690024726092815\n",
      "[step: 5068] loss: 0.008760925382375717\n",
      "[step: 5069] loss: 0.008924731984734535\n",
      "[step: 5070] loss: 0.008819516748189926\n",
      "[step: 5071] loss: 0.008623389527201653\n",
      "[step: 5072] loss: 0.008694969117641449\n",
      "[step: 5073] loss: 0.008818994276225567\n",
      "[step: 5074] loss: 0.008823622018098831\n",
      "[step: 5075] loss: 0.008651547133922577\n",
      "[step: 5076] loss: 0.008630548603832722\n",
      "[step: 5077] loss: 0.008802850730717182\n",
      "[step: 5078] loss: 0.008975645527243614\n",
      "[step: 5079] loss: 0.008839407004415989\n",
      "[step: 5080] loss: 0.008622257970273495\n",
      "[step: 5081] loss: 0.00882443692535162\n",
      "[step: 5082] loss: 0.008978434838354588\n",
      "[step: 5083] loss: 0.008914915844798088\n",
      "[step: 5084] loss: 0.008682861924171448\n",
      "[step: 5085] loss: 0.008765668608248234\n",
      "[step: 5086] loss: 0.00894093792885542\n",
      "[step: 5087] loss: 0.008680050261318684\n",
      "[step: 5088] loss: 0.00862474087625742\n",
      "[step: 5089] loss: 0.008700140751898289\n",
      "[step: 5090] loss: 0.008761565200984478\n",
      "[step: 5091] loss: 0.008690382353961468\n",
      "[step: 5092] loss: 0.008579263463616371\n",
      "[step: 5093] loss: 0.00863484013825655\n",
      "[step: 5094] loss: 0.008693152107298374\n",
      "[step: 5095] loss: 0.008655017241835594\n",
      "[step: 5096] loss: 0.00858140829950571\n",
      "[step: 5097] loss: 0.008572189137339592\n",
      "[step: 5098] loss: 0.008607975207269192\n",
      "[step: 5099] loss: 0.00865875743329525\n",
      "[step: 5100] loss: 0.008651827462017536\n",
      "[step: 5101] loss: 0.008625480346381664\n",
      "[step: 5102] loss: 0.008568058721721172\n",
      "[step: 5103] loss: 0.008550547063350677\n",
      "[step: 5104] loss: 0.008612497709691525\n",
      "[step: 5105] loss: 0.008677623234689236\n",
      "[step: 5106] loss: 0.008768740110099316\n",
      "[step: 5107] loss: 0.008693356066942215\n",
      "[step: 5108] loss: 0.0085613913834095\n",
      "[step: 5109] loss: 0.008552937768399715\n",
      "[step: 5110] loss: 0.008634800091385841\n",
      "[step: 5111] loss: 0.008775126188993454\n",
      "[step: 5112] loss: 0.00882197916507721\n",
      "[step: 5113] loss: 0.008676714263856411\n",
      "[step: 5114] loss: 0.008539051748812199\n",
      "[step: 5115] loss: 0.008536319248378277\n",
      "[step: 5116] loss: 0.008594616316258907\n",
      "[step: 5117] loss: 0.008726116269826889\n",
      "[step: 5118] loss: 0.00885846372693777\n",
      "[step: 5119] loss: 0.008755877614021301\n",
      "[step: 5120] loss: 0.008574129082262516\n",
      "[step: 5121] loss: 0.008531051687896252\n",
      "[step: 5122] loss: 0.008577979169785976\n",
      "[step: 5123] loss: 0.008734012953937054\n",
      "[step: 5124] loss: 0.008817501366138458\n",
      "[step: 5125] loss: 0.00868865754455328\n",
      "[step: 5126] loss: 0.008561757393181324\n",
      "[step: 5127] loss: 0.00850794930011034\n",
      "[step: 5128] loss: 0.008495819754898548\n",
      "[step: 5129] loss: 0.008542240597307682\n",
      "[step: 5130] loss: 0.008608607575297356\n",
      "[step: 5131] loss: 0.008714469149708748\n",
      "[step: 5132] loss: 0.00897710956633091\n",
      "[step: 5133] loss: 0.008945684880018234\n",
      "[step: 5134] loss: 0.008652258664369583\n",
      "[step: 5135] loss: 0.008588655851781368\n",
      "[step: 5136] loss: 0.008908040821552277\n",
      "[step: 5137] loss: 0.009343394078314304\n",
      "[step: 5138] loss: 0.008862440474331379\n",
      "[step: 5139] loss: 0.008635842241346836\n",
      "[step: 5140] loss: 0.009247991256415844\n",
      "[step: 5141] loss: 0.009039698168635368\n",
      "[step: 5142] loss: 0.008742745034396648\n",
      "[step: 5143] loss: 0.00868403259664774\n",
      "[step: 5144] loss: 0.008847037330269814\n",
      "[step: 5145] loss: 0.008661387488245964\n",
      "[step: 5146] loss: 0.008563108742237091\n",
      "[step: 5147] loss: 0.008661260828375816\n",
      "[step: 5148] loss: 0.008774072863161564\n",
      "[step: 5149] loss: 0.008661587722599506\n",
      "[step: 5150] loss: 0.008519775234162807\n",
      "[step: 5151] loss: 0.008668229915201664\n",
      "[step: 5152] loss: 0.00871433224529028\n",
      "[step: 5153] loss: 0.008555363863706589\n",
      "[step: 5154] loss: 0.008530973456799984\n",
      "[step: 5155] loss: 0.008595740422606468\n",
      "[step: 5156] loss: 0.00866882037371397\n",
      "[step: 5157] loss: 0.008566796779632568\n",
      "[step: 5158] loss: 0.008474675938487053\n",
      "[step: 5159] loss: 0.008534089662134647\n",
      "[step: 5160] loss: 0.008556177839636803\n",
      "[step: 5161] loss: 0.008497928269207478\n",
      "[step: 5162] loss: 0.008466670289635658\n",
      "[step: 5163] loss: 0.008501533418893814\n",
      "[step: 5164] loss: 0.008555673994123936\n",
      "[step: 5165] loss: 0.00858222134411335\n",
      "[step: 5166] loss: 0.008493667468428612\n",
      "[step: 5167] loss: 0.008450061082839966\n",
      "[step: 5168] loss: 0.008508142083883286\n",
      "[step: 5169] loss: 0.008625040762126446\n",
      "[step: 5170] loss: 0.008690373972058296\n",
      "[step: 5171] loss: 0.00854736939072609\n",
      "[step: 5172] loss: 0.008438702672719955\n",
      "[step: 5173] loss: 0.008456436917185783\n",
      "[step: 5174] loss: 0.008547852747142315\n",
      "[step: 5175] loss: 0.00865940097719431\n",
      "[step: 5176] loss: 0.008675908669829369\n",
      "[step: 5177] loss: 0.00853381585329771\n",
      "[step: 5178] loss: 0.008447859436273575\n",
      "[step: 5179] loss: 0.008419984951615334\n",
      "[step: 5180] loss: 0.008469329215586185\n",
      "[step: 5181] loss: 0.008593697100877762\n",
      "[step: 5182] loss: 0.008705927059054375\n",
      "[step: 5183] loss: 0.008869972079992294\n",
      "[step: 5184] loss: 0.008740528486669064\n",
      "[step: 5185] loss: 0.008505890145897865\n",
      "[step: 5186] loss: 0.008495011366903782\n",
      "[step: 5187] loss: 0.008759035728871822\n",
      "[step: 5188] loss: 0.009248760528862476\n",
      "[step: 5189] loss: 0.008830503560602665\n",
      "[step: 5190] loss: 0.008549937047064304\n",
      "[step: 5191] loss: 0.009120159782469273\n",
      "[step: 5192] loss: 0.009161663241684437\n",
      "[step: 5193] loss: 0.008954276330769062\n",
      "[step: 5194] loss: 0.008749991655349731\n",
      "[step: 5195] loss: 0.008945835754275322\n",
      "[step: 5196] loss: 0.008703910745680332\n",
      "[step: 5197] loss: 0.008610313758254051\n",
      "[step: 5198] loss: 0.008729535154998302\n",
      "[step: 5199] loss: 0.008725658059120178\n",
      "[step: 5200] loss: 0.00869889184832573\n",
      "[step: 5201] loss: 0.008489440195262432\n",
      "[step: 5202] loss: 0.008738608099520206\n",
      "[step: 5203] loss: 0.00884160678833723\n",
      "[step: 5204] loss: 0.008581234142184258\n",
      "[step: 5205] loss: 0.008487217128276825\n",
      "[step: 5206] loss: 0.00863911397755146\n",
      "[step: 5207] loss: 0.008499117568135262\n",
      "[step: 5208] loss: 0.008479231968522072\n",
      "[step: 5209] loss: 0.008563412353396416\n",
      "[step: 5210] loss: 0.008648008108139038\n",
      "[step: 5211] loss: 0.008652313612401485\n",
      "[step: 5212] loss: 0.008481317199766636\n",
      "[step: 5213] loss: 0.008510078303515911\n",
      "[step: 5214] loss: 0.008774843066930771\n",
      "[step: 5215] loss: 0.008960901759564877\n",
      "[step: 5216] loss: 0.008552202954888344\n",
      "[step: 5217] loss: 0.008692833594977856\n",
      "[step: 5218] loss: 0.008758400566875935\n",
      "[step: 5219] loss: 0.008469121530652046\n",
      "[step: 5220] loss: 0.008677165023982525\n",
      "[step: 5221] loss: 0.009014328941702843\n",
      "[step: 5222] loss: 0.008665485307574272\n",
      "[step: 5223] loss: 0.008626463823020458\n",
      "[step: 5224] loss: 0.008777443319559097\n",
      "[step: 5225] loss: 0.008607910014688969\n",
      "[step: 5226] loss: 0.008466660976409912\n",
      "[step: 5227] loss: 0.008693760260939598\n",
      "[step: 5228] loss: 0.008637850172817707\n",
      "[step: 5229] loss: 0.008514544926583767\n",
      "[step: 5230] loss: 0.008505240082740784\n",
      "[step: 5231] loss: 0.008578854613006115\n",
      "[step: 5232] loss: 0.008479205891489983\n",
      "[step: 5233] loss: 0.008407101966440678\n",
      "[step: 5234] loss: 0.008501236326992512\n",
      "[step: 5235] loss: 0.00854817871004343\n",
      "[step: 5236] loss: 0.008436940610408783\n",
      "[step: 5237] loss: 0.008422879502177238\n",
      "[step: 5238] loss: 0.008441568352282047\n",
      "[step: 5239] loss: 0.008499392308294773\n",
      "[step: 5240] loss: 0.00837618950754404\n",
      "[step: 5241] loss: 0.008397428318858147\n",
      "[step: 5242] loss: 0.008363057859241962\n",
      "[step: 5243] loss: 0.008380375802516937\n",
      "[step: 5244] loss: 0.008358602412045002\n",
      "[step: 5245] loss: 0.008347325026988983\n",
      "[step: 5246] loss: 0.00835824478417635\n",
      "[step: 5247] loss: 0.008340753614902496\n",
      "[step: 5248] loss: 0.008342855609953403\n",
      "[step: 5249] loss: 0.008372009731829166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 5250] loss: 0.00844469852745533\n",
      "[step: 5251] loss: 0.008615437895059586\n",
      "[step: 5252] loss: 0.008743186481297016\n",
      "[step: 5253] loss: 0.008538184687495232\n",
      "[step: 5254] loss: 0.00840709637850523\n",
      "[step: 5255] loss: 0.008610784076154232\n",
      "[step: 5256] loss: 0.008853957988321781\n",
      "[step: 5257] loss: 0.00909294281154871\n",
      "[step: 5258] loss: 0.00865719374269247\n",
      "[step: 5259] loss: 0.008751939982175827\n",
      "[step: 5260] loss: 0.008888962678611279\n",
      "[step: 5261] loss: 0.008628669194877148\n",
      "[step: 5262] loss: 0.008559850044548512\n",
      "[step: 5263] loss: 0.0084441052749753\n",
      "[step: 5264] loss: 0.008629108779132366\n",
      "[step: 5265] loss: 0.008458171039819717\n",
      "[step: 5266] loss: 0.008524260483682156\n",
      "[step: 5267] loss: 0.008439919911324978\n",
      "[step: 5268] loss: 0.008330609649419785\n",
      "[step: 5269] loss: 0.008439388126134872\n",
      "[step: 5270] loss: 0.00837177224457264\n",
      "[step: 5271] loss: 0.00845326203852892\n",
      "[step: 5272] loss: 0.008430765941739082\n",
      "[step: 5273] loss: 0.008382948115468025\n",
      "[step: 5274] loss: 0.00840713456273079\n",
      "[step: 5275] loss: 0.008318070322275162\n",
      "[step: 5276] loss: 0.008339968509972095\n",
      "[step: 5277] loss: 0.008477739058434963\n",
      "[step: 5278] loss: 0.008724725805222988\n",
      "[step: 5279] loss: 0.009314506314694881\n",
      "[step: 5280] loss: 0.009011171758174896\n",
      "[step: 5281] loss: 0.008924620226025581\n",
      "[step: 5282] loss: 0.008743476122617722\n",
      "[step: 5283] loss: 0.00875601451843977\n",
      "[step: 5284] loss: 0.0086904875934124\n",
      "[step: 5285] loss: 0.008699160069227219\n",
      "[step: 5286] loss: 0.008636187762022018\n",
      "[step: 5287] loss: 0.008571608923375607\n",
      "[step: 5288] loss: 0.008481241762638092\n",
      "[step: 5289] loss: 0.008912092074751854\n",
      "[step: 5290] loss: 0.008916893042623997\n",
      "[step: 5291] loss: 0.008494933135807514\n",
      "[step: 5292] loss: 0.008573240600526333\n",
      "[step: 5293] loss: 0.008664745837450027\n",
      "[step: 5294] loss: 0.008504343219101429\n",
      "[step: 5295] loss: 0.008522138930857182\n",
      "[step: 5296] loss: 0.008766066282987595\n",
      "[step: 5297] loss: 0.008817109279334545\n",
      "[step: 5298] loss: 0.008864054456353188\n",
      "[step: 5299] loss: 0.008684594184160233\n",
      "[step: 5300] loss: 0.008698693476617336\n",
      "[step: 5301] loss: 0.008382612839341164\n",
      "[step: 5302] loss: 0.00858260691165924\n",
      "[step: 5303] loss: 0.008510218001902103\n",
      "[step: 5304] loss: 0.008529631420969963\n",
      "[step: 5305] loss: 0.008368763141334057\n",
      "[step: 5306] loss: 0.008469766937196255\n",
      "[step: 5307] loss: 0.008398981764912605\n",
      "[step: 5308] loss: 0.008425203152000904\n",
      "[step: 5309] loss: 0.008424580097198486\n",
      "[step: 5310] loss: 0.008311791345477104\n",
      "[step: 5311] loss: 0.008389638736844063\n",
      "[step: 5312] loss: 0.00834085512906313\n",
      "[step: 5313] loss: 0.008337575010955334\n",
      "[step: 5314] loss: 0.008322782814502716\n",
      "[step: 5315] loss: 0.008338706567883492\n",
      "[step: 5316] loss: 0.00830136239528656\n",
      "[step: 5317] loss: 0.008301887661218643\n",
      "[step: 5318] loss: 0.008267581462860107\n",
      "[step: 5319] loss: 0.008292182348668575\n",
      "[step: 5320] loss: 0.008295196108520031\n",
      "[step: 5321] loss: 0.008259673602879047\n",
      "[step: 5322] loss: 0.00834363792091608\n",
      "[step: 5323] loss: 0.008389342576265335\n",
      "[step: 5324] loss: 0.008430847898125648\n",
      "[step: 5325] loss: 0.008427338674664497\n",
      "[step: 5326] loss: 0.008455557748675346\n",
      "[step: 5327] loss: 0.008413128554821014\n",
      "[step: 5328] loss: 0.008364753797650337\n",
      "[step: 5329] loss: 0.008369003422558308\n",
      "[step: 5330] loss: 0.008578497916460037\n",
      "[step: 5331] loss: 0.008700564503669739\n",
      "[step: 5332] loss: 0.008439281955361366\n",
      "[step: 5333] loss: 0.008258633315563202\n",
      "[step: 5334] loss: 0.008538922294974327\n",
      "[step: 5335] loss: 0.00943656824529171\n",
      "[step: 5336] loss: 0.008943689055740833\n",
      "[step: 5337] loss: 0.008775399997830391\n",
      "[step: 5338] loss: 0.00912822037935257\n",
      "[step: 5339] loss: 0.008471742272377014\n",
      "[step: 5340] loss: 0.008844384923577309\n",
      "[step: 5341] loss: 0.009667083621025085\n",
      "[step: 5342] loss: 0.008834769017994404\n",
      "[step: 5343] loss: 0.00930270180106163\n",
      "[step: 5344] loss: 0.008980553597211838\n",
      "[step: 5345] loss: 0.008601623587310314\n",
      "[step: 5346] loss: 0.009183147922158241\n",
      "[step: 5347] loss: 0.008979447185993195\n",
      "[step: 5348] loss: 0.008402221836149693\n",
      "[step: 5349] loss: 0.008962402120232582\n",
      "[step: 5350] loss: 0.009031817317008972\n",
      "[step: 5351] loss: 0.008786295540630817\n",
      "[step: 5352] loss: 0.00901301670819521\n",
      "[step: 5353] loss: 0.008527925238013268\n",
      "[step: 5354] loss: 0.008604602888226509\n",
      "[step: 5355] loss: 0.008854846470057964\n",
      "[step: 5356] loss: 0.009294550865888596\n",
      "[step: 5357] loss: 0.009245322085916996\n",
      "[step: 5358] loss: 0.008987820707261562\n",
      "[step: 5359] loss: 0.008557365275919437\n",
      "[step: 5360] loss: 0.009326276369392872\n",
      "[step: 5361] loss: 0.009271912276744843\n",
      "[step: 5362] loss: 0.008660727180540562\n",
      "[step: 5363] loss: 0.008793781511485577\n",
      "[step: 5364] loss: 0.008759595453739166\n",
      "[step: 5365] loss: 0.008693140000104904\n",
      "[step: 5366] loss: 0.008552425540983677\n",
      "[step: 5367] loss: 0.008569493889808655\n",
      "[step: 5368] loss: 0.008880763314664364\n",
      "[step: 5369] loss: 0.00865272618830204\n",
      "[step: 5370] loss: 0.00840528029948473\n",
      "[step: 5371] loss: 0.00861792266368866\n",
      "[step: 5372] loss: 0.008517048321664333\n",
      "[step: 5373] loss: 0.008593986742198467\n",
      "[step: 5374] loss: 0.008416026830673218\n",
      "[step: 5375] loss: 0.00839315541088581\n",
      "[step: 5376] loss: 0.008564285933971405\n",
      "[step: 5377] loss: 0.008440980687737465\n",
      "[step: 5378] loss: 0.008367073722183704\n",
      "[step: 5379] loss: 0.008383557200431824\n",
      "[step: 5380] loss: 0.00833451934158802\n",
      "[step: 5381] loss: 0.008371037431061268\n",
      "[step: 5382] loss: 0.008381791412830353\n",
      "[step: 5383] loss: 0.008285832591354847\n",
      "[step: 5384] loss: 0.008290392346680164\n",
      "[step: 5385] loss: 0.008343811146914959\n",
      "[step: 5386] loss: 0.00836622528731823\n",
      "[step: 5387] loss: 0.008301995694637299\n",
      "[step: 5388] loss: 0.008264536038041115\n",
      "[step: 5389] loss: 0.008318845182657242\n",
      "[step: 5390] loss: 0.008359919302165508\n",
      "[step: 5391] loss: 0.008279843255877495\n",
      "[step: 5392] loss: 0.008236746303737164\n",
      "[step: 5393] loss: 0.008243697695434093\n",
      "[step: 5394] loss: 0.00828121043741703\n",
      "[step: 5395] loss: 0.008385597728192806\n",
      "[step: 5396] loss: 0.008437559008598328\n",
      "[step: 5397] loss: 0.008456004783511162\n",
      "[step: 5398] loss: 0.008315971121191978\n",
      "[step: 5399] loss: 0.008377454243600368\n",
      "[step: 5400] loss: 0.008359306491911411\n",
      "[step: 5401] loss: 0.008295336738228798\n",
      "[step: 5402] loss: 0.008271787315607071\n",
      "[step: 5403] loss: 0.008281787857413292\n",
      "[step: 5404] loss: 0.008226709440350533\n",
      "[step: 5405] loss: 0.008214646019041538\n",
      "[step: 5406] loss: 0.00821000523865223\n",
      "[step: 5407] loss: 0.008199038915336132\n",
      "[step: 5408] loss: 0.00820673257112503\n",
      "[step: 5409] loss: 0.008221419528126717\n",
      "[step: 5410] loss: 0.008201748132705688\n",
      "[step: 5411] loss: 0.008222912438213825\n",
      "[step: 5412] loss: 0.008281768299639225\n",
      "[step: 5413] loss: 0.008356832899153233\n",
      "[step: 5414] loss: 0.008310802280902863\n",
      "[step: 5415] loss: 0.008177459239959717\n",
      "[step: 5416] loss: 0.008194253779947758\n",
      "[step: 5417] loss: 0.008474744856357574\n",
      "[step: 5418] loss: 0.009425449185073376\n",
      "[step: 5419] loss: 0.008813257329165936\n",
      "[step: 5420] loss: 0.0091948127374053\n",
      "[step: 5421] loss: 0.008502226322889328\n",
      "[step: 5422] loss: 0.00879182480275631\n",
      "[step: 5423] loss: 0.00917066726833582\n",
      "[step: 5424] loss: 0.009235864505171776\n",
      "[step: 5425] loss: 0.008553956635296345\n",
      "[step: 5426] loss: 0.00898850429803133\n",
      "[step: 5427] loss: 0.008643931709229946\n",
      "[step: 5428] loss: 0.008680273778736591\n",
      "[step: 5429] loss: 0.008636179380118847\n",
      "[step: 5430] loss: 0.008580280467867851\n",
      "[step: 5431] loss: 0.0088517339900136\n",
      "[step: 5432] loss: 0.008345033042132854\n",
      "[step: 5433] loss: 0.008737623691558838\n",
      "[step: 5434] loss: 0.008491499349474907\n",
      "[step: 5435] loss: 0.008401596918702126\n",
      "[step: 5436] loss: 0.008644873276352882\n",
      "[step: 5437] loss: 0.008405126631259918\n",
      "[step: 5438] loss: 0.008354747667908669\n",
      "[step: 5439] loss: 0.008212984539568424\n",
      "[step: 5440] loss: 0.008320661261677742\n",
      "[step: 5441] loss: 0.008266089484095573\n",
      "[step: 5442] loss: 0.008286100812256336\n",
      "[step: 5443] loss: 0.00824426393955946\n",
      "[step: 5444] loss: 0.008249759674072266\n",
      "[step: 5445] loss: 0.008202464319765568\n",
      "[step: 5446] loss: 0.008224411867558956\n",
      "[step: 5447] loss: 0.008207251317799091\n",
      "[step: 5448] loss: 0.008232355117797852\n",
      "[step: 5449] loss: 0.008238282985985279\n",
      "[step: 5450] loss: 0.00822592806071043\n",
      "[step: 5451] loss: 0.008212306536734104\n",
      "[step: 5452] loss: 0.008182259276509285\n",
      "[step: 5453] loss: 0.008249684236943722\n",
      "[step: 5454] loss: 0.008233493193984032\n",
      "[step: 5455] loss: 0.008308341726660728\n",
      "[step: 5456] loss: 0.008220570161938667\n",
      "[step: 5457] loss: 0.00816053245216608\n",
      "[step: 5458] loss: 0.008245390839874744\n",
      "[step: 5459] loss: 0.008355781435966492\n",
      "[step: 5460] loss: 0.008614479564130306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 5461] loss: 0.008374160155653954\n",
      "[step: 5462] loss: 0.008310808800160885\n",
      "[step: 5463] loss: 0.008639540523290634\n",
      "[step: 5464] loss: 0.008394273929297924\n",
      "[step: 5465] loss: 0.008284064009785652\n",
      "[step: 5466] loss: 0.008325393311679363\n",
      "[step: 5467] loss: 0.008243256248533726\n",
      "[step: 5468] loss: 0.008203336037695408\n",
      "[step: 5469] loss: 0.008269954472780228\n",
      "[step: 5470] loss: 0.008376619778573513\n",
      "[step: 5471] loss: 0.008600671775639057\n",
      "[step: 5472] loss: 0.008329668082296848\n",
      "[step: 5473] loss: 0.008476583287119865\n",
      "[step: 5474] loss: 0.008375407196581364\n",
      "[step: 5475] loss: 0.008251577615737915\n",
      "[step: 5476] loss: 0.008445655927062035\n",
      "[step: 5477] loss: 0.008744078688323498\n",
      "[step: 5478] loss: 0.0084403520449996\n",
      "[step: 5479] loss: 0.008666861802339554\n",
      "[step: 5480] loss: 0.008272898383438587\n",
      "[step: 5481] loss: 0.008424784988164902\n",
      "[step: 5482] loss: 0.008925140835344791\n",
      "[step: 5483] loss: 0.009026811458170414\n",
      "[step: 5484] loss: 0.008561748079955578\n",
      "[step: 5485] loss: 0.008945024572312832\n",
      "[step: 5486] loss: 0.008593129925429821\n",
      "[step: 5487] loss: 0.008510551415383816\n",
      "[step: 5488] loss: 0.008424116298556328\n",
      "[step: 5489] loss: 0.00853241328150034\n",
      "[step: 5490] loss: 0.008359557949006557\n",
      "[step: 5491] loss: 0.008306736126542091\n",
      "[step: 5492] loss: 0.008459746837615967\n",
      "[step: 5493] loss: 0.008246666751801968\n",
      "[step: 5494] loss: 0.008268791250884533\n",
      "[step: 5495] loss: 0.008316888473927975\n",
      "[step: 5496] loss: 0.008367098867893219\n",
      "[step: 5497] loss: 0.00833787489682436\n",
      "[step: 5498] loss: 0.008345477283000946\n",
      "[step: 5499] loss: 0.008403785526752472\n",
      "[step: 5500] loss: 0.00827864371240139\n",
      "[step: 5501] loss: 0.00818602740764618\n",
      "[step: 5502] loss: 0.008333675563335419\n",
      "[step: 5503] loss: 0.008213184773921967\n",
      "[step: 5504] loss: 0.008361893706023693\n",
      "[step: 5505] loss: 0.008344825357198715\n",
      "[step: 5506] loss: 0.00823181215673685\n",
      "[step: 5507] loss: 0.008288553915917873\n",
      "[step: 5508] loss: 0.008421740494668484\n",
      "[step: 5509] loss: 0.008493582718074322\n",
      "[step: 5510] loss: 0.008280378766357899\n",
      "[step: 5511] loss: 0.00821539107710123\n",
      "[step: 5512] loss: 0.00842780526727438\n",
      "[step: 5513] loss: 0.008331059478223324\n",
      "[step: 5514] loss: 0.00834165420383215\n",
      "[step: 5515] loss: 0.008204209618270397\n",
      "[step: 5516] loss: 0.008181914687156677\n",
      "[step: 5517] loss: 0.008343490771949291\n",
      "[step: 5518] loss: 0.008261630311608315\n",
      "[step: 5519] loss: 0.008204127661883831\n",
      "[step: 5520] loss: 0.008171357214450836\n",
      "[step: 5521] loss: 0.008352834731340408\n",
      "[step: 5522] loss: 0.008700289763510227\n",
      "[step: 5523] loss: 0.008590588346123695\n",
      "[step: 5524] loss: 0.008330653421580791\n",
      "[step: 5525] loss: 0.008582080714404583\n",
      "[step: 5526] loss: 0.00822327472269535\n",
      "[step: 5527] loss: 0.008253749459981918\n",
      "[step: 5528] loss: 0.00867629423737526\n",
      "[step: 5529] loss: 0.008333985693752766\n",
      "[step: 5530] loss: 0.008263937197625637\n",
      "[step: 5531] loss: 0.008487126789987087\n",
      "[step: 5532] loss: 0.008130484260618687\n",
      "[step: 5533] loss: 0.008352951146662235\n",
      "[step: 5534] loss: 0.008568767458200455\n",
      "[step: 5535] loss: 0.008210273459553719\n",
      "[step: 5536] loss: 0.008345128037035465\n",
      "[step: 5537] loss: 0.008403762243688107\n",
      "[step: 5538] loss: 0.008145051077008247\n",
      "[step: 5539] loss: 0.008382854051887989\n",
      "[step: 5540] loss: 0.008421000093221664\n",
      "[step: 5541] loss: 0.008126665838062763\n",
      "[step: 5542] loss: 0.008350030519068241\n",
      "[step: 5543] loss: 0.008365628309547901\n",
      "[step: 5544] loss: 0.008134555071592331\n",
      "[step: 5545] loss: 0.008333001285791397\n",
      "[step: 5546] loss: 0.008376887999475002\n",
      "[step: 5547] loss: 0.008099483326077461\n",
      "[step: 5548] loss: 0.008295233361423016\n",
      "[step: 5549] loss: 0.008387003093957901\n",
      "[step: 5550] loss: 0.008095445111393929\n",
      "[step: 5551] loss: 0.008273279294371605\n",
      "[step: 5552] loss: 0.008395844139158726\n",
      "[step: 5553] loss: 0.008105440996587276\n",
      "[step: 5554] loss: 0.008262296207249165\n",
      "[step: 5555] loss: 0.00836233701556921\n",
      "[step: 5556] loss: 0.008082578890025616\n",
      "[step: 5557] loss: 0.008179081603884697\n",
      "[step: 5558] loss: 0.008320193737745285\n",
      "[step: 5559] loss: 0.008088165894150734\n",
      "[step: 5560] loss: 0.008204786106944084\n",
      "[step: 5561] loss: 0.008332929573953152\n",
      "[step: 5562] loss: 0.008092815056443214\n",
      "[step: 5563] loss: 0.008171102963387966\n",
      "[step: 5564] loss: 0.008308866061270237\n",
      "[step: 5565] loss: 0.008085140027105808\n",
      "[step: 5566] loss: 0.008107920177280903\n",
      "[step: 5567] loss: 0.008250707760453224\n",
      "[step: 5568] loss: 0.008096097968518734\n",
      "[step: 5569] loss: 0.008070254698395729\n",
      "[step: 5570] loss: 0.008203668519854546\n",
      "[step: 5571] loss: 0.00816162396222353\n",
      "[step: 5572] loss: 0.008075481280684471\n",
      "[step: 5573] loss: 0.008070165291428566\n",
      "[step: 5574] loss: 0.008141135796904564\n",
      "[step: 5575] loss: 0.008128568530082703\n",
      "[step: 5576] loss: 0.008048086427152157\n",
      "[step: 5577] loss: 0.00804368406534195\n",
      "[step: 5578] loss: 0.008142390288412571\n",
      "[step: 5579] loss: 0.00821260828524828\n",
      "[step: 5580] loss: 0.00825380440801382\n",
      "[step: 5581] loss: 0.008136540651321411\n",
      "[step: 5582] loss: 0.008139525540173054\n",
      "[step: 5583] loss: 0.008221310563385487\n",
      "[step: 5584] loss: 0.008133026771247387\n",
      "[step: 5585] loss: 0.008193830959498882\n",
      "[step: 5586] loss: 0.008110127411782742\n",
      "[step: 5587] loss: 0.008099709637463093\n",
      "[step: 5588] loss: 0.008194562047719955\n",
      "[step: 5589] loss: 0.008124146610498428\n",
      "[step: 5590] loss: 0.00817019585520029\n",
      "[step: 5591] loss: 0.008069160394370556\n",
      "[step: 5592] loss: 0.008121847175061703\n",
      "[step: 5593] loss: 0.008325382135808468\n",
      "[step: 5594] loss: 0.00843008141964674\n",
      "[step: 5595] loss: 0.00813639909029007\n",
      "[step: 5596] loss: 0.008242691867053509\n",
      "[step: 5597] loss: 0.008469190448522568\n",
      "[step: 5598] loss: 0.008507031947374344\n",
      "[step: 5599] loss: 0.008337059058248997\n",
      "[step: 5600] loss: 0.008332335390150547\n",
      "[step: 5601] loss: 0.008167063817381859\n",
      "[step: 5602] loss: 0.008210205473005772\n",
      "[step: 5603] loss: 0.008231939747929573\n",
      "[step: 5604] loss: 0.008187379688024521\n",
      "[step: 5605] loss: 0.008077419362962246\n",
      "[step: 5606] loss: 0.008280773647129536\n",
      "[step: 5607] loss: 0.008170487359166145\n",
      "[step: 5608] loss: 0.008286787196993828\n",
      "[step: 5609] loss: 0.008108441717922688\n",
      "[step: 5610] loss: 0.008215285837650299\n",
      "[step: 5611] loss: 0.008460341952741146\n",
      "[step: 5612] loss: 0.009124119766056538\n",
      "[step: 5613] loss: 0.009300228208303452\n",
      "[step: 5614] loss: 0.009395979344844818\n",
      "[step: 5615] loss: 0.00860602967441082\n",
      "[step: 5616] loss: 0.008774012327194214\n",
      "[step: 5617] loss: 0.008933112025260925\n",
      "[step: 5618] loss: 0.00898742862045765\n",
      "[step: 5619] loss: 0.00871884822845459\n",
      "[step: 5620] loss: 0.008483695797622204\n",
      "[step: 5621] loss: 0.008491394110023975\n",
      "[step: 5622] loss: 0.008690991438925266\n",
      "[step: 5623] loss: 0.00860110018402338\n",
      "[step: 5624] loss: 0.008522783406078815\n",
      "[step: 5625] loss: 0.008339010179042816\n",
      "[step: 5626] loss: 0.008481943048536777\n",
      "[step: 5627] loss: 0.00835674162954092\n",
      "[step: 5628] loss: 0.008431843481957912\n",
      "[step: 5629] loss: 0.00828222744166851\n",
      "[step: 5630] loss: 0.008232959546148777\n",
      "[step: 5631] loss: 0.00829668901860714\n",
      "[step: 5632] loss: 0.008166372776031494\n",
      "[step: 5633] loss: 0.008203097619116306\n",
      "[step: 5634] loss: 0.00820624828338623\n",
      "[step: 5635] loss: 0.008159264922142029\n",
      "[step: 5636] loss: 0.00810756254941225\n",
      "[step: 5637] loss: 0.008150750771164894\n",
      "[step: 5638] loss: 0.008115052245557308\n",
      "[step: 5639] loss: 0.008071823045611382\n",
      "[step: 5640] loss: 0.008095874451100826\n",
      "[step: 5641] loss: 0.008133675903081894\n",
      "[step: 5642] loss: 0.008144579827785492\n",
      "[step: 5643] loss: 0.008247616700828075\n",
      "[step: 5644] loss: 0.008178140968084335\n",
      "[step: 5645] loss: 0.008055148646235466\n",
      "[step: 5646] loss: 0.008187834173440933\n",
      "[step: 5647] loss: 0.00832530576735735\n",
      "[step: 5648] loss: 0.008488777093589306\n",
      "[step: 5649] loss: 0.008367727510631084\n",
      "[step: 5650] loss: 0.008244914002716541\n",
      "[step: 5651] loss: 0.008391819894313812\n",
      "[step: 5652] loss: 0.008337309584021568\n",
      "[step: 5653] loss: 0.008572365157306194\n",
      "[step: 5654] loss: 0.00857490859925747\n",
      "[step: 5655] loss: 0.008213253691792488\n",
      "[step: 5656] loss: 0.008555987849831581\n",
      "[step: 5657] loss: 0.008203107863664627\n",
      "[step: 5658] loss: 0.00820515863597393\n",
      "[step: 5659] loss: 0.008380042389035225\n",
      "[step: 5660] loss: 0.008023791015148163\n",
      "[step: 5661] loss: 0.008302390575408936\n",
      "[step: 5662] loss: 0.008218337781727314\n",
      "[step: 5663] loss: 0.008176971226930618\n",
      "[step: 5664] loss: 0.008170215412974358\n",
      "[step: 5665] loss: 0.00812757108360529\n",
      "[step: 5666] loss: 0.008404659107327461\n",
      "[step: 5667] loss: 0.008276933804154396\n",
      "[step: 5668] loss: 0.008182606659829617\n",
      "[step: 5669] loss: 0.008248865604400635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 5670] loss: 0.008100691251456738\n",
      "[step: 5671] loss: 0.008127973414957523\n",
      "[step: 5672] loss: 0.007989046163856983\n",
      "[step: 5673] loss: 0.008081205189228058\n",
      "[step: 5674] loss: 0.008024366572499275\n",
      "[step: 5675] loss: 0.007986757904291153\n",
      "[step: 5676] loss: 0.008052170276641846\n",
      "[step: 5677] loss: 0.007986105047166348\n",
      "[step: 5678] loss: 0.00803292915225029\n",
      "[step: 5679] loss: 0.00800922978669405\n",
      "[step: 5680] loss: 0.007958846166729927\n",
      "[step: 5681] loss: 0.007981190457940102\n",
      "[step: 5682] loss: 0.007953276857733727\n",
      "[step: 5683] loss: 0.008064556866884232\n",
      "[step: 5684] loss: 0.008161851204931736\n",
      "[step: 5685] loss: 0.008271998725831509\n",
      "[step: 5686] loss: 0.008302262052893639\n",
      "[step: 5687] loss: 0.008256920613348484\n",
      "[step: 5688] loss: 0.008000760339200497\n",
      "[step: 5689] loss: 0.008074970915913582\n",
      "[step: 5690] loss: 0.008537241257727146\n",
      "[step: 5691] loss: 0.009043710306286812\n",
      "[step: 5692] loss: 0.00868480559438467\n",
      "[step: 5693] loss: 0.008893451653420925\n",
      "[step: 5694] loss: 0.00818601343780756\n",
      "[step: 5695] loss: 0.00855328794568777\n",
      "[step: 5696] loss: 0.009522611275315285\n",
      "[step: 5697] loss: 0.010103770531713963\n",
      "[step: 5698] loss: 0.009044401347637177\n",
      "[step: 5699] loss: 0.009679554961621761\n",
      "[step: 5700] loss: 0.00930752418935299\n",
      "[step: 5701] loss: 0.008684113621711731\n",
      "[step: 5702] loss: 0.009077814407646656\n",
      "[step: 5703] loss: 0.008936289697885513\n",
      "[step: 5704] loss: 0.00930417887866497\n",
      "[step: 5705] loss: 0.008825201541185379\n",
      "[step: 5706] loss: 0.008751311339437962\n",
      "[step: 5707] loss: 0.008991187438368797\n",
      "[step: 5708] loss: 0.008972532115876675\n",
      "[step: 5709] loss: 0.008960897102952003\n",
      "[step: 5710] loss: 0.00860684085637331\n",
      "[step: 5711] loss: 0.008595614694058895\n",
      "[step: 5712] loss: 0.008507255464792252\n",
      "[step: 5713] loss: 0.008727881126105785\n",
      "[step: 5714] loss: 0.008564310148358345\n",
      "[step: 5715] loss: 0.00854773074388504\n",
      "[step: 5716] loss: 0.00834600068628788\n",
      "[step: 5717] loss: 0.008416377939283848\n",
      "[step: 5718] loss: 0.008410430513322353\n",
      "[step: 5719] loss: 0.00839155726134777\n",
      "[step: 5720] loss: 0.008283199742436409\n",
      "[step: 5721] loss: 0.008179472759366035\n",
      "[step: 5722] loss: 0.008235829882323742\n",
      "[step: 5723] loss: 0.008180704899132252\n",
      "[step: 5724] loss: 0.008156434632837772\n",
      "[step: 5725] loss: 0.008213832974433899\n",
      "[step: 5726] loss: 0.008146530017256737\n",
      "[step: 5727] loss: 0.008119932375848293\n",
      "[step: 5728] loss: 0.008156194351613522\n",
      "[step: 5729] loss: 0.008105640299618244\n",
      "[step: 5730] loss: 0.008074583485722542\n",
      "[step: 5731] loss: 0.008091895841062069\n",
      "[step: 5732] loss: 0.00807259138673544\n",
      "[step: 5733] loss: 0.00803134124726057\n",
      "[step: 5734] loss: 0.008012048900127411\n",
      "[step: 5735] loss: 0.008011026307940483\n",
      "[step: 5736] loss: 0.00800210889428854\n",
      "[step: 5737] loss: 0.007977966219186783\n",
      "[step: 5738] loss: 0.0079750781878829\n",
      "[step: 5739] loss: 0.007977272383868694\n",
      "[step: 5740] loss: 0.007979735732078552\n",
      "[step: 5741] loss: 0.007949464954435825\n",
      "[step: 5742] loss: 0.007940567098557949\n",
      "[step: 5743] loss: 0.007936250418424606\n",
      "[step: 5744] loss: 0.00793465692549944\n",
      "[step: 5745] loss: 0.007925538346171379\n",
      "[step: 5746] loss: 0.007924703881144524\n",
      "[step: 5747] loss: 0.007935337722301483\n",
      "[step: 5748] loss: 0.008021146059036255\n",
      "[step: 5749] loss: 0.008285525254905224\n",
      "[step: 5750] loss: 0.008325622417032719\n",
      "[step: 5751] loss: 0.008205125108361244\n",
      "[step: 5752] loss: 0.008004684932529926\n",
      "[step: 5753] loss: 0.008355150930583477\n",
      "[step: 5754] loss: 0.00912430789321661\n",
      "[step: 5755] loss: 0.008948227390646935\n",
      "[step: 5756] loss: 0.0089880907908082\n",
      "[step: 5757] loss: 0.009229997172951698\n",
      "[step: 5758] loss: 0.008959447965025902\n",
      "[step: 5759] loss: 0.008849426172673702\n",
      "[step: 5760] loss: 0.008659365586936474\n",
      "[step: 5761] loss: 0.009144558571279049\n",
      "[step: 5762] loss: 0.008707712404429913\n",
      "[step: 5763] loss: 0.008899306878447533\n",
      "[step: 5764] loss: 0.00897575356066227\n",
      "[step: 5765] loss: 0.008808450773358345\n",
      "[step: 5766] loss: 0.00857663992792368\n",
      "[step: 5767] loss: 0.008717752061784267\n",
      "[step: 5768] loss: 0.00857689417898655\n",
      "[step: 5769] loss: 0.00852667260915041\n",
      "[step: 5770] loss: 0.008587265387177467\n",
      "[step: 5771] loss: 0.008429066278040409\n",
      "[step: 5772] loss: 0.0085303895175457\n",
      "[step: 5773] loss: 0.008540796115994453\n",
      "[step: 5774] loss: 0.008358593098819256\n",
      "[step: 5775] loss: 0.008384897373616695\n",
      "[step: 5776] loss: 0.008391735143959522\n",
      "[step: 5777] loss: 0.008344641886651516\n",
      "[step: 5778] loss: 0.008386120200157166\n",
      "[step: 5779] loss: 0.008298324421048164\n",
      "[step: 5780] loss: 0.008309494704008102\n",
      "[step: 5781] loss: 0.008269979618489742\n",
      "[step: 5782] loss: 0.008225836791098118\n",
      "[step: 5783] loss: 0.008343297056853771\n",
      "[step: 5784] loss: 0.008487068116664886\n",
      "[step: 5785] loss: 0.008478405885398388\n",
      "[step: 5786] loss: 0.008412165567278862\n",
      "[step: 5787] loss: 0.008339297026395798\n",
      "[step: 5788] loss: 0.008277448825538158\n",
      "[step: 5789] loss: 0.008226689882576466\n",
      "[step: 5790] loss: 0.008258044719696045\n",
      "[step: 5791] loss: 0.008250664919614792\n",
      "[step: 5792] loss: 0.00821669027209282\n",
      "[step: 5793] loss: 0.008142408914864063\n",
      "[step: 5794] loss: 0.00812584813684225\n",
      "[step: 5795] loss: 0.008204497396945953\n",
      "[step: 5796] loss: 0.008105738088488579\n",
      "[step: 5797] loss: 0.008062694221735\n",
      "[step: 5798] loss: 0.008111128583550453\n",
      "[step: 5799] loss: 0.008056426420807838\n",
      "[step: 5800] loss: 0.008054832927882671\n",
      "[step: 5801] loss: 0.008011446334421635\n",
      "[step: 5802] loss: 0.008035458624362946\n",
      "[step: 5803] loss: 0.008024499751627445\n",
      "[step: 5804] loss: 0.007993157021701336\n",
      "[step: 5805] loss: 0.007990214042365551\n",
      "[step: 5806] loss: 0.008009110577404499\n",
      "[step: 5807] loss: 0.00799285713583231\n",
      "[step: 5808] loss: 0.008044576272368431\n",
      "[step: 5809] loss: 0.008014651015400887\n",
      "[step: 5810] loss: 0.008042627014219761\n",
      "[step: 5811] loss: 0.007977285422384739\n",
      "[step: 5812] loss: 0.007940557785332203\n",
      "[step: 5813] loss: 0.007941316813230515\n",
      "[step: 5814] loss: 0.007969815284013748\n",
      "[step: 5815] loss: 0.008102369494736195\n",
      "[step: 5816] loss: 0.007991445250809193\n",
      "[step: 5817] loss: 0.008004818111658096\n",
      "[step: 5818] loss: 0.007948865182697773\n",
      "[step: 5819] loss: 0.007959973067045212\n",
      "[step: 5820] loss: 0.007954716682434082\n",
      "[step: 5821] loss: 0.008060835301876068\n",
      "[step: 5822] loss: 0.00795163493603468\n",
      "[step: 5823] loss: 0.007936680689454079\n",
      "[step: 5824] loss: 0.007940944284200668\n",
      "[step: 5825] loss: 0.007995536550879478\n",
      "[step: 5826] loss: 0.008087853901088238\n",
      "[step: 5827] loss: 0.00892012007534504\n",
      "[step: 5828] loss: 0.00859883613884449\n",
      "[step: 5829] loss: 0.008951260708272457\n",
      "[step: 5830] loss: 0.00825213547796011\n",
      "[step: 5831] loss: 0.008450199849903584\n",
      "[step: 5832] loss: 0.008493208326399326\n",
      "[step: 5833] loss: 0.008345270529389381\n",
      "[step: 5834] loss: 0.008484018966555595\n",
      "[step: 5835] loss: 0.008451945148408413\n",
      "[step: 5836] loss: 0.008344332687556744\n",
      "[step: 5837] loss: 0.00846050027757883\n",
      "[step: 5838] loss: 0.008289594203233719\n",
      "[step: 5839] loss: 0.008755506947636604\n",
      "[step: 5840] loss: 0.008602021262049675\n",
      "[step: 5841] loss: 0.008723150938749313\n",
      "[step: 5842] loss: 0.008347724564373493\n",
      "[step: 5843] loss: 0.008570092730224133\n",
      "[step: 5844] loss: 0.009121596813201904\n",
      "[step: 5845] loss: 0.008195833303034306\n",
      "[step: 5846] loss: 0.00866116862744093\n",
      "[step: 5847] loss: 0.008325510658323765\n",
      "[step: 5848] loss: 0.008687905967235565\n",
      "[step: 5849] loss: 0.008349041454494\n",
      "[step: 5850] loss: 0.00838625617325306\n",
      "[step: 5851] loss: 0.008395867422223091\n",
      "[step: 5852] loss: 0.008173363283276558\n",
      "[step: 5853] loss: 0.008375735022127628\n",
      "[step: 5854] loss: 0.008168910630047321\n",
      "[step: 5855] loss: 0.00819116085767746\n",
      "[step: 5856] loss: 0.008146243169903755\n",
      "[step: 5857] loss: 0.008238859474658966\n",
      "[step: 5858] loss: 0.008020812645554543\n",
      "[step: 5859] loss: 0.008159375749528408\n",
      "[step: 5860] loss: 0.008016876876354218\n",
      "[step: 5861] loss: 0.008116299286484718\n",
      "[step: 5862] loss: 0.008018981665372849\n",
      "[step: 5863] loss: 0.008060094900429249\n",
      "[step: 5864] loss: 0.00795992836356163\n",
      "[step: 5865] loss: 0.00800375733524561\n",
      "[step: 5866] loss: 0.007974471896886826\n",
      "[step: 5867] loss: 0.00796881876885891\n",
      "[step: 5868] loss: 0.007932234555482864\n",
      "[step: 5869] loss: 0.007940871641039848\n",
      "[step: 5870] loss: 0.007914380170404911\n",
      "[step: 5871] loss: 0.007936145178973675\n",
      "[step: 5872] loss: 0.00789258535951376\n",
      "[step: 5873] loss: 0.00789713766425848\n",
      "[step: 5874] loss: 0.007888054475188255\n",
      "[step: 5875] loss: 0.007866610772907734\n",
      "[step: 5876] loss: 0.007887648418545723\n",
      "[step: 5877] loss: 0.007855999283492565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 5878] loss: 0.007842403836548328\n",
      "[step: 5879] loss: 0.007850175723433495\n",
      "[step: 5880] loss: 0.007848207838833332\n",
      "[step: 5881] loss: 0.007840389385819435\n",
      "[step: 5882] loss: 0.007812086958438158\n",
      "[step: 5883] loss: 0.007833370938897133\n",
      "[step: 5884] loss: 0.007847866974771023\n",
      "[step: 5885] loss: 0.007908245548605919\n",
      "[step: 5886] loss: 0.008283551782369614\n",
      "[step: 5887] loss: 0.008723961189389229\n",
      "[step: 5888] loss: 0.009009812958538532\n",
      "[step: 5889] loss: 0.008887174539268017\n",
      "[step: 5890] loss: 0.00832431297749281\n",
      "[step: 5891] loss: 0.008938655257225037\n",
      "[step: 5892] loss: 0.008339881896972656\n",
      "[step: 5893] loss: 0.008454261347651482\n",
      "[step: 5894] loss: 0.008483350276947021\n",
      "[step: 5895] loss: 0.00828921515494585\n",
      "[step: 5896] loss: 0.008279649540781975\n",
      "[step: 5897] loss: 0.008308086544275284\n",
      "[step: 5898] loss: 0.008192197419703007\n",
      "[step: 5899] loss: 0.008209715597331524\n",
      "[step: 5900] loss: 0.008223367854952812\n",
      "[step: 5901] loss: 0.00806462112814188\n",
      "[step: 5902] loss: 0.00814887136220932\n",
      "[step: 5903] loss: 0.008060088381171227\n",
      "[step: 5904] loss: 0.008082514628767967\n",
      "[step: 5905] loss: 0.00807445403188467\n",
      "[step: 5906] loss: 0.008023940026760101\n",
      "[step: 5907] loss: 0.008048570714890957\n",
      "[step: 5908] loss: 0.007974458858370781\n",
      "[step: 5909] loss: 0.007983350194990635\n",
      "[step: 5910] loss: 0.00798563752323389\n",
      "[step: 5911] loss: 0.007918049581348896\n",
      "[step: 5912] loss: 0.007943200878798962\n",
      "[step: 5913] loss: 0.007923536002635956\n",
      "[step: 5914] loss: 0.007882415316998959\n",
      "[step: 5915] loss: 0.007881366647779942\n",
      "[step: 5916] loss: 0.00788162462413311\n",
      "[step: 5917] loss: 0.007870183326303959\n",
      "[step: 5918] loss: 0.007848735898733139\n",
      "[step: 5919] loss: 0.007850361056625843\n",
      "[step: 5920] loss: 0.007839745841920376\n",
      "[step: 5921] loss: 0.007837329991161823\n",
      "[step: 5922] loss: 0.007820207625627518\n",
      "[step: 5923] loss: 0.007813356816768646\n",
      "[step: 5924] loss: 0.007813241332769394\n",
      "[step: 5925] loss: 0.007809770759195089\n",
      "[step: 5926] loss: 0.007817511446774006\n",
      "[step: 5927] loss: 0.00781098660081625\n",
      "[step: 5928] loss: 0.007829210720956326\n",
      "[step: 5929] loss: 0.007862761616706848\n",
      "[step: 5930] loss: 0.007954079657793045\n",
      "[step: 5931] loss: 0.008104110136628151\n",
      "[step: 5932] loss: 0.008125340566039085\n",
      "[step: 5933] loss: 0.007967142388224602\n",
      "[step: 5934] loss: 0.007826331071555614\n",
      "[step: 5935] loss: 0.007971341721713543\n",
      "[step: 5936] loss: 0.008204910904169083\n",
      "[step: 5937] loss: 0.008109738118946552\n",
      "[step: 5938] loss: 0.007886518724262714\n",
      "[step: 5939] loss: 0.0078652398660779\n",
      "[step: 5940] loss: 0.008070063777267933\n",
      "[step: 5941] loss: 0.008110146038234234\n",
      "[step: 5942] loss: 0.007859773002564907\n",
      "[step: 5943] loss: 0.007933295331895351\n",
      "[step: 5944] loss: 0.008021533489227295\n",
      "[step: 5945] loss: 0.007843764498829842\n",
      "[step: 5946] loss: 0.00781957060098648\n",
      "[step: 5947] loss: 0.0079257283359766\n",
      "[step: 5948] loss: 0.007838637568056583\n",
      "[step: 5949] loss: 0.007786065340042114\n",
      "[step: 5950] loss: 0.00789398979395628\n",
      "[step: 5951] loss: 0.007909588515758514\n",
      "[step: 5952] loss: 0.007819107733666897\n",
      "[step: 5953] loss: 0.007840690203011036\n",
      "[step: 5954] loss: 0.007854865863919258\n",
      "[step: 5955] loss: 0.007785648573189974\n",
      "[step: 5956] loss: 0.007771741598844528\n",
      "[step: 5957] loss: 0.007822535932064056\n",
      "[step: 5958] loss: 0.00785998348146677\n",
      "[step: 5959] loss: 0.007786332163959742\n",
      "[step: 5960] loss: 0.007747333031147718\n",
      "[step: 5961] loss: 0.007765482179820538\n",
      "[step: 5962] loss: 0.007814331911504269\n",
      "[step: 5963] loss: 0.007831580936908722\n",
      "[step: 5964] loss: 0.007781777996569872\n",
      "[step: 5965] loss: 0.007741698995232582\n",
      "[step: 5966] loss: 0.007724414579570293\n",
      "[step: 5967] loss: 0.007721718866378069\n",
      "[step: 5968] loss: 0.007729336619377136\n",
      "[step: 5969] loss: 0.0077543254010379314\n",
      "[step: 5970] loss: 0.00790089275687933\n",
      "[step: 5971] loss: 0.008454909548163414\n",
      "[step: 5972] loss: 0.008272887207567692\n",
      "[step: 5973] loss: 0.008026905357837677\n",
      "[step: 5974] loss: 0.008056474849581718\n",
      "[step: 5975] loss: 0.007966913282871246\n",
      "[step: 5976] loss: 0.007864325307309628\n",
      "[step: 5977] loss: 0.007895156741142273\n",
      "[step: 5978] loss: 0.007928252220153809\n",
      "[step: 5979] loss: 0.0078115314245224\n",
      "[step: 5980] loss: 0.007862535305321217\n",
      "[step: 5981] loss: 0.007834099233150482\n",
      "[step: 5982] loss: 0.007823927327990532\n",
      "[step: 5983] loss: 0.007844536565244198\n",
      "[step: 5984] loss: 0.007759336847811937\n",
      "[step: 5985] loss: 0.007812953554093838\n",
      "[step: 5986] loss: 0.007769602816551924\n",
      "[step: 5987] loss: 0.007763520814478397\n",
      "[step: 5988] loss: 0.0078020356595516205\n",
      "[step: 5989] loss: 0.007730051409453154\n",
      "[step: 5990] loss: 0.007763742934912443\n",
      "[step: 5991] loss: 0.007744736038148403\n",
      "[step: 5992] loss: 0.007744751404970884\n",
      "[step: 5993] loss: 0.007736124098300934\n",
      "[step: 5994] loss: 0.007724335882812738\n",
      "[step: 5995] loss: 0.007711087353527546\n",
      "[step: 5996] loss: 0.007705457508563995\n",
      "[step: 5997] loss: 0.007719321176409721\n",
      "[step: 5998] loss: 0.007687920704483986\n",
      "[step: 5999] loss: 0.007710600271821022\n",
      "[step: 6000] loss: 0.007690113037824631\n",
      "[step: 6001] loss: 0.007749064359813929\n",
      "[step: 6002] loss: 0.008004866540431976\n",
      "[step: 6003] loss: 0.008983228355646133\n",
      "[step: 6004] loss: 0.011337930336594582\n",
      "[step: 6005] loss: 0.010005692951381207\n",
      "[step: 6006] loss: 0.009563304483890533\n",
      "[step: 6007] loss: 0.010197569616138935\n",
      "[step: 6008] loss: 0.009581644088029861\n",
      "[step: 6009] loss: 0.009131882339715958\n",
      "[step: 6010] loss: 0.009282265789806843\n",
      "[step: 6011] loss: 0.008919280022382736\n",
      "[step: 6012] loss: 0.009255954064428806\n",
      "[step: 6013] loss: 0.010356088168919086\n",
      "[step: 6014] loss: 0.00892812293022871\n",
      "[step: 6015] loss: 0.009373148903250694\n",
      "[step: 6016] loss: 0.009987350553274155\n",
      "[step: 6017] loss: 0.00905813742429018\n",
      "[step: 6018] loss: 0.008639276027679443\n",
      "[step: 6019] loss: 0.009097018279135227\n",
      "[step: 6020] loss: 0.009017596952617168\n",
      "[step: 6021] loss: 0.008628156036138535\n",
      "[step: 6022] loss: 0.008344447240233421\n",
      "[step: 6023] loss: 0.008740019984543324\n",
      "[step: 6024] loss: 0.008679067716002464\n",
      "[step: 6025] loss: 0.008660705760121346\n",
      "[step: 6026] loss: 0.008356567472219467\n",
      "[step: 6027] loss: 0.008326241746544838\n",
      "[step: 6028] loss: 0.008290715515613556\n",
      "[step: 6029] loss: 0.008393754251301289\n",
      "[step: 6030] loss: 0.008281375281512737\n",
      "[step: 6031] loss: 0.008212865330278873\n",
      "[step: 6032] loss: 0.008069268427789211\n",
      "[step: 6033] loss: 0.008159308694303036\n",
      "[step: 6034] loss: 0.008229493163526058\n",
      "[step: 6035] loss: 0.008128048852086067\n",
      "[step: 6036] loss: 0.008007846772670746\n",
      "[step: 6037] loss: 0.0079994210973382\n",
      "[step: 6038] loss: 0.008122583851218224\n",
      "[step: 6039] loss: 0.00804616417735815\n",
      "[step: 6040] loss: 0.007952994666993618\n",
      "[step: 6041] loss: 0.007967380806803703\n",
      "[step: 6042] loss: 0.008017604239284992\n",
      "[step: 6043] loss: 0.007941337302327156\n",
      "[step: 6044] loss: 0.007925660349428654\n",
      "[step: 6045] loss: 0.007919677533209324\n",
      "[step: 6046] loss: 0.007913714274764061\n",
      "[step: 6047] loss: 0.007905388250946999\n",
      "[step: 6048] loss: 0.00785184558480978\n",
      "[step: 6049] loss: 0.007858974859118462\n",
      "[step: 6050] loss: 0.00784214772284031\n",
      "[step: 6051] loss: 0.00782616063952446\n",
      "[step: 6052] loss: 0.007813632488250732\n",
      "[step: 6053] loss: 0.007796998135745525\n",
      "[step: 6054] loss: 0.007777238264679909\n",
      "[step: 6055] loss: 0.007789472118020058\n",
      "[step: 6056] loss: 0.007758813910186291\n",
      "[step: 6057] loss: 0.007747175171971321\n",
      "[step: 6058] loss: 0.007757099810987711\n",
      "[step: 6059] loss: 0.007732531055808067\n",
      "[step: 6060] loss: 0.007731950376182795\n",
      "[step: 6061] loss: 0.007720305118709803\n",
      "[step: 6062] loss: 0.007714888546615839\n",
      "[step: 6063] loss: 0.007716430816799402\n",
      "[step: 6064] loss: 0.007714304607361555\n",
      "[step: 6065] loss: 0.007734749000519514\n",
      "[step: 6066] loss: 0.007751875091344118\n",
      "[step: 6067] loss: 0.007764316629618406\n",
      "[step: 6068] loss: 0.007882178761065006\n",
      "[step: 6069] loss: 0.008148428052663803\n",
      "[step: 6070] loss: 0.008439002558588982\n",
      "[step: 6071] loss: 0.007879290729761124\n",
      "[step: 6072] loss: 0.008535885252058506\n",
      "[step: 6073] loss: 0.008606813848018646\n",
      "[step: 6074] loss: 0.008042811416089535\n",
      "[step: 6075] loss: 0.00841693114489317\n",
      "[step: 6076] loss: 0.00813873577862978\n",
      "[step: 6077] loss: 0.008037993684411049\n",
      "[step: 6078] loss: 0.008225217461585999\n",
      "[step: 6079] loss: 0.007779238745570183\n",
      "[step: 6080] loss: 0.008372281678020954\n",
      "[step: 6081] loss: 0.008195736445486546\n",
      "[step: 6082] loss: 0.008015508763492107\n",
      "[step: 6083] loss: 0.008144345134496689\n",
      "[step: 6084] loss: 0.007906628772616386\n",
      "[step: 6085] loss: 0.008113945834338665\n",
      "[step: 6086] loss: 0.007890800014138222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 6087] loss: 0.00799926370382309\n",
      "[step: 6088] loss: 0.007896172814071178\n",
      "[step: 6089] loss: 0.0078663295134902\n",
      "[step: 6090] loss: 0.007941262796521187\n",
      "[step: 6091] loss: 0.007787859067320824\n",
      "[step: 6092] loss: 0.007972096092998981\n",
      "[step: 6093] loss: 0.007745635230094194\n",
      "[step: 6094] loss: 0.007854284718632698\n",
      "[step: 6095] loss: 0.007792341988533735\n",
      "[step: 6096] loss: 0.007779086008667946\n",
      "[step: 6097] loss: 0.007811332121491432\n",
      "[step: 6098] loss: 0.007730931043624878\n",
      "[step: 6099] loss: 0.0077905417419970036\n",
      "[step: 6100] loss: 0.007782597094774246\n",
      "[step: 6101] loss: 0.0076849209144711494\n",
      "[step: 6102] loss: 0.007737368810921907\n",
      "[step: 6103] loss: 0.007717087399214506\n",
      "[step: 6104] loss: 0.0076832277700304985\n",
      "[step: 6105] loss: 0.007694890722632408\n",
      "[step: 6106] loss: 0.007700745947659016\n",
      "[step: 6107] loss: 0.007655661087483168\n",
      "[step: 6108] loss: 0.0076660714112222195\n",
      "[step: 6109] loss: 0.007710929028689861\n",
      "[step: 6110] loss: 0.007731680758297443\n",
      "[step: 6111] loss: 0.00770990364253521\n",
      "[step: 6112] loss: 0.0076478929258883\n",
      "[step: 6113] loss: 0.007649336475878954\n",
      "[step: 6114] loss: 0.007690518628805876\n",
      "[step: 6115] loss: 0.007743409834802151\n",
      "[step: 6116] loss: 0.007816464640200138\n",
      "[step: 6117] loss: 0.0077379802241921425\n",
      "[step: 6118] loss: 0.007741373032331467\n",
      "[step: 6119] loss: 0.007709731813520193\n",
      "[step: 6120] loss: 0.0076668402180075645\n",
      "[step: 6121] loss: 0.007620646618306637\n",
      "[step: 6122] loss: 0.007615967188030481\n",
      "[step: 6123] loss: 0.0076511818915605545\n",
      "[step: 6124] loss: 0.007728168740868568\n",
      "[step: 6125] loss: 0.007901763543486595\n",
      "[step: 6126] loss: 0.007845099084079266\n",
      "[step: 6127] loss: 0.0077856481075286865\n",
      "[step: 6128] loss: 0.007633245084434748\n",
      "[step: 6129] loss: 0.007751771714538336\n",
      "[step: 6130] loss: 0.008051799610257149\n",
      "[step: 6131] loss: 0.007971836254000664\n",
      "[step: 6132] loss: 0.007855487056076527\n",
      "[step: 6133] loss: 0.007757037412375212\n",
      "[step: 6134] loss: 0.007805681321769953\n",
      "[step: 6135] loss: 0.007732970640063286\n",
      "[step: 6136] loss: 0.00781119242310524\n",
      "[step: 6137] loss: 0.007806341163814068\n",
      "[step: 6138] loss: 0.007718413602560759\n",
      "[step: 6139] loss: 0.007752589415758848\n",
      "[step: 6140] loss: 0.007728588301688433\n",
      "[step: 6141] loss: 0.0077050006948411465\n",
      "[step: 6142] loss: 0.00773978978395462\n",
      "[step: 6143] loss: 0.007661642041057348\n",
      "[step: 6144] loss: 0.0077312542125582695\n",
      "[step: 6145] loss: 0.00775021780282259\n",
      "[step: 6146] loss: 0.007693905383348465\n",
      "[step: 6147] loss: 0.007660740986466408\n",
      "[step: 6148] loss: 0.007708841469138861\n",
      "[step: 6149] loss: 0.007768194656819105\n",
      "[step: 6150] loss: 0.00782160647213459\n",
      "[step: 6151] loss: 0.007765930611640215\n",
      "[step: 6152] loss: 0.007764582522213459\n",
      "[step: 6153] loss: 0.007696350570768118\n",
      "[step: 6154] loss: 0.007962615229189396\n",
      "[step: 6155] loss: 0.00807340256869793\n",
      "[step: 6156] loss: 0.008082917891442776\n",
      "[step: 6157] loss: 0.007945369929075241\n",
      "[step: 6158] loss: 0.008046390488743782\n",
      "[step: 6159] loss: 0.008044475689530373\n",
      "[step: 6160] loss: 0.00796950701624155\n",
      "[step: 6161] loss: 0.008115685544908047\n",
      "[step: 6162] loss: 0.008255154825747013\n",
      "[step: 6163] loss: 0.007783912122249603\n",
      "[step: 6164] loss: 0.00830894522368908\n",
      "[step: 6165] loss: 0.008082916960120201\n",
      "[step: 6166] loss: 0.00796284805983305\n",
      "[step: 6167] loss: 0.008086132816970348\n",
      "[step: 6168] loss: 0.007742130197584629\n",
      "[step: 6169] loss: 0.008128030225634575\n",
      "[step: 6170] loss: 0.007883439771831036\n",
      "[step: 6171] loss: 0.00822574831545353\n",
      "[step: 6172] loss: 0.007858268916606903\n",
      "[step: 6173] loss: 0.007843172177672386\n",
      "[step: 6174] loss: 0.008003919385373592\n",
      "[step: 6175] loss: 0.007886524312198162\n",
      "[step: 6176] loss: 0.007998659275472164\n",
      "[step: 6177] loss: 0.00781152443960309\n",
      "[step: 6178] loss: 0.007937788031995296\n",
      "[step: 6179] loss: 0.007687328848987818\n",
      "[step: 6180] loss: 0.007945017889142036\n",
      "[step: 6181] loss: 0.00779243977740407\n",
      "[step: 6182] loss: 0.007729950826615095\n",
      "[step: 6183] loss: 0.007834192365407944\n",
      "[step: 6184] loss: 0.00765316653996706\n",
      "[step: 6185] loss: 0.0077667683362960815\n",
      "[step: 6186] loss: 0.007730682380497456\n",
      "[step: 6187] loss: 0.007665218319743872\n",
      "[step: 6188] loss: 0.00775285717099905\n",
      "[step: 6189] loss: 0.007670696824789047\n",
      "[step: 6190] loss: 0.007656788919121027\n",
      "[step: 6191] loss: 0.007728348020464182\n",
      "[step: 6192] loss: 0.0077405753545463085\n",
      "[step: 6193] loss: 0.007661858107894659\n",
      "[step: 6194] loss: 0.00767984613776207\n",
      "[step: 6195] loss: 0.0077760713174939156\n",
      "[step: 6196] loss: 0.007628334686160088\n",
      "[step: 6197] loss: 0.007602974772453308\n",
      "[step: 6198] loss: 0.007729076314717531\n",
      "[step: 6199] loss: 0.007647735066711903\n",
      "[step: 6200] loss: 0.007579261437058449\n",
      "[step: 6201] loss: 0.007611061446368694\n",
      "[step: 6202] loss: 0.00762757146731019\n",
      "[step: 6203] loss: 0.007654935121536255\n",
      "[step: 6204] loss: 0.0075608897022902966\n",
      "[step: 6205] loss: 0.007591018918901682\n",
      "[step: 6206] loss: 0.0076667796820402145\n",
      "[step: 6207] loss: 0.007708317134529352\n",
      "[step: 6208] loss: 0.007737423293292522\n",
      "[step: 6209] loss: 0.007598988711833954\n",
      "[step: 6210] loss: 0.007606464438140392\n",
      "[step: 6211] loss: 0.007584679406136274\n",
      "[step: 6212] loss: 0.007597587537020445\n",
      "[step: 6213] loss: 0.007574636489152908\n",
      "[step: 6214] loss: 0.0075734928250312805\n",
      "[step: 6215] loss: 0.0075460574589669704\n",
      "[step: 6216] loss: 0.0075645120814442635\n",
      "[step: 6217] loss: 0.007548823952674866\n",
      "[step: 6218] loss: 0.007536119781434536\n",
      "[step: 6219] loss: 0.007552263326942921\n",
      "[step: 6220] loss: 0.007558861281722784\n",
      "[step: 6221] loss: 0.007594401948153973\n",
      "[step: 6222] loss: 0.00761469965800643\n",
      "[step: 6223] loss: 0.007800315972417593\n",
      "[step: 6224] loss: 0.008015935309231281\n",
      "[step: 6225] loss: 0.008746255189180374\n",
      "[step: 6226] loss: 0.008413045667111874\n",
      "[step: 6227] loss: 0.008776715025305748\n",
      "[step: 6228] loss: 0.008623335510492325\n",
      "[step: 6229] loss: 0.00789524707943201\n",
      "[step: 6230] loss: 0.008176579140126705\n",
      "[step: 6231] loss: 0.00813994836062193\n",
      "[step: 6232] loss: 0.007965678349137306\n",
      "[step: 6233] loss: 0.007972045801579952\n",
      "[step: 6234] loss: 0.007981986738741398\n",
      "[step: 6235] loss: 0.007982471026480198\n",
      "[step: 6236] loss: 0.007893005385994911\n",
      "[step: 6237] loss: 0.007805991917848587\n",
      "[step: 6238] loss: 0.008068560622632504\n",
      "[step: 6239] loss: 0.007995941676199436\n",
      "[step: 6240] loss: 0.008145335130393505\n",
      "[step: 6241] loss: 0.007962125353515148\n",
      "[step: 6242] loss: 0.00792856514453888\n",
      "[step: 6243] loss: 0.007797903846949339\n",
      "[step: 6244] loss: 0.007951905950903893\n",
      "[step: 6245] loss: 0.007824034430086613\n",
      "[step: 6246] loss: 0.007792862597852945\n",
      "[step: 6247] loss: 0.007802797015756369\n",
      "[step: 6248] loss: 0.007812859490513802\n",
      "[step: 6249] loss: 0.007728212978690863\n",
      "[step: 6250] loss: 0.007749265991151333\n",
      "[step: 6251] loss: 0.007668552454560995\n",
      "[step: 6252] loss: 0.007720975670963526\n",
      "[step: 6253] loss: 0.007683565374463797\n",
      "[step: 6254] loss: 0.007639181334525347\n",
      "[step: 6255] loss: 0.0076573919504880905\n",
      "[step: 6256] loss: 0.007618897594511509\n",
      "[step: 6257] loss: 0.007644697558134794\n",
      "[step: 6258] loss: 0.007603232283145189\n",
      "[step: 6259] loss: 0.007587376981973648\n",
      "[step: 6260] loss: 0.0075745717622339725\n",
      "[step: 6261] loss: 0.007592480629682541\n",
      "[step: 6262] loss: 0.0076356870122253895\n",
      "[step: 6263] loss: 0.007602125406265259\n",
      "[step: 6264] loss: 0.007583408150821924\n",
      "[step: 6265] loss: 0.007544712163507938\n",
      "[step: 6266] loss: 0.0075288754887878895\n",
      "[step: 6267] loss: 0.00755830854177475\n",
      "[step: 6268] loss: 0.0076106637716293335\n",
      "[step: 6269] loss: 0.007916586473584175\n",
      "[step: 6270] loss: 0.00792129710316658\n",
      "[step: 6271] loss: 0.008311877027153969\n",
      "[step: 6272] loss: 0.007634068839251995\n",
      "[step: 6273] loss: 0.008183369413018227\n",
      "[step: 6274] loss: 0.009184642694890499\n",
      "[step: 6275] loss: 0.008459467440843582\n",
      "[step: 6276] loss: 0.009959647431969643\n",
      "[step: 6277] loss: 0.009114141575992107\n",
      "[step: 6278] loss: 0.008776906877756119\n",
      "[step: 6279] loss: 0.008290421217679977\n",
      "[step: 6280] loss: 0.008591744117438793\n",
      "[step: 6281] loss: 0.008426330983638763\n",
      "[step: 6282] loss: 0.00866052508354187\n",
      "[step: 6283] loss: 0.00796070508658886\n",
      "[step: 6284] loss: 0.008208582177758217\n",
      "[step: 6285] loss: 0.008315558545291424\n",
      "[step: 6286] loss: 0.008154301904141903\n",
      "[step: 6287] loss: 0.007966017350554466\n",
      "[step: 6288] loss: 0.00811548437923193\n",
      "[step: 6289] loss: 0.008097171783447266\n",
      "[step: 6290] loss: 0.00782550685107708\n",
      "[step: 6291] loss: 0.007915107533335686\n",
      "[step: 6292] loss: 0.007927769795060158\n",
      "[step: 6293] loss: 0.007908707484602928\n",
      "[step: 6294] loss: 0.007752775214612484\n",
      "[step: 6295] loss: 0.0077658058144152164\n",
      "[step: 6296] loss: 0.007854346185922623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 6297] loss: 0.007757909595966339\n",
      "[step: 6298] loss: 0.0077064018696546555\n",
      "[step: 6299] loss: 0.007698491681367159\n",
      "[step: 6300] loss: 0.007701733615249395\n",
      "[step: 6301] loss: 0.007719173561781645\n",
      "[step: 6302] loss: 0.007652599364519119\n",
      "[step: 6303] loss: 0.007641190197318792\n",
      "[step: 6304] loss: 0.007637204602360725\n",
      "[step: 6305] loss: 0.007642395328730345\n",
      "[step: 6306] loss: 0.007594507187604904\n",
      "[step: 6307] loss: 0.007594079244881868\n",
      "[step: 6308] loss: 0.007580120116472244\n",
      "[step: 6309] loss: 0.007590551860630512\n",
      "[step: 6310] loss: 0.007559308782219887\n",
      "[step: 6311] loss: 0.00755115132778883\n",
      "[step: 6312] loss: 0.007542342413216829\n",
      "[step: 6313] loss: 0.007555555552244186\n",
      "[step: 6314] loss: 0.007514014840126038\n",
      "[step: 6315] loss: 0.007524764630943537\n",
      "[step: 6316] loss: 0.007524916436523199\n",
      "[step: 6317] loss: 0.007502370979636908\n",
      "[step: 6318] loss: 0.0075073568150401115\n",
      "[step: 6319] loss: 0.007508132606744766\n",
      "[step: 6320] loss: 0.007494108285754919\n",
      "[step: 6321] loss: 0.0074916137382388115\n",
      "[step: 6322] loss: 0.0074945492669939995\n",
      "[step: 6323] loss: 0.007482270244508982\n",
      "[step: 6324] loss: 0.007481617387384176\n",
      "[step: 6325] loss: 0.007486395072191954\n",
      "[step: 6326] loss: 0.007479381747543812\n",
      "[step: 6327] loss: 0.007470073644071817\n",
      "[step: 6328] loss: 0.007472366560250521\n",
      "[step: 6329] loss: 0.007479476742446423\n",
      "[step: 6330] loss: 0.007474141661077738\n",
      "[step: 6331] loss: 0.0074818553403019905\n",
      "[step: 6332] loss: 0.0075112017802894115\n",
      "[step: 6333] loss: 0.007602369412779808\n",
      "[step: 6334] loss: 0.007645341567695141\n",
      "[step: 6335] loss: 0.007830934599041939\n",
      "[step: 6336] loss: 0.007692684885114431\n",
      "[step: 6337] loss: 0.007544123101979494\n",
      "[step: 6338] loss: 0.007493927609175444\n",
      "[step: 6339] loss: 0.007712468970566988\n",
      "[step: 6340] loss: 0.00810067169368267\n",
      "[step: 6341] loss: 0.007741217035800219\n",
      "[step: 6342] loss: 0.007840663194656372\n",
      "[step: 6343] loss: 0.007921181619167328\n",
      "[step: 6344] loss: 0.007601266261190176\n",
      "[step: 6345] loss: 0.008141851983964443\n",
      "[step: 6346] loss: 0.007954620756208897\n",
      "[step: 6347] loss: 0.00785242673009634\n",
      "[step: 6348] loss: 0.008170430548489094\n",
      "[step: 6349] loss: 0.007599851116538048\n",
      "[step: 6350] loss: 0.007963107898831367\n",
      "[step: 6351] loss: 0.007700425107032061\n",
      "[step: 6352] loss: 0.007792329415678978\n",
      "[step: 6353] loss: 0.00781748816370964\n",
      "[step: 6354] loss: 0.007591621018946171\n",
      "[step: 6355] loss: 0.0077635725028812885\n",
      "[step: 6356] loss: 0.007605785969644785\n",
      "[step: 6357] loss: 0.007719251327216625\n",
      "[step: 6358] loss: 0.007588264532387257\n",
      "[step: 6359] loss: 0.00767260417342186\n",
      "[step: 6360] loss: 0.00753429951146245\n",
      "[step: 6361] loss: 0.007690367754548788\n",
      "[step: 6362] loss: 0.007594388909637928\n",
      "[step: 6363] loss: 0.007606011815369129\n",
      "[step: 6364] loss: 0.007576523814350367\n",
      "[step: 6365] loss: 0.007559130899608135\n",
      "[step: 6366] loss: 0.0075780549086630344\n",
      "[step: 6367] loss: 0.007528616581112146\n",
      "[step: 6368] loss: 0.0075520118698477745\n",
      "[step: 6369] loss: 0.007519047241657972\n",
      "[step: 6370] loss: 0.007503047585487366\n",
      "[step: 6371] loss: 0.007538963109254837\n",
      "[step: 6372] loss: 0.007469342555850744\n",
      "[step: 6373] loss: 0.00749346474185586\n",
      "[step: 6374] loss: 0.0074897087179124355\n",
      "[step: 6375] loss: 0.007447737269103527\n",
      "[step: 6376] loss: 0.007521280087530613\n",
      "[step: 6377] loss: 0.007482361979782581\n",
      "[step: 6378] loss: 0.007449825294315815\n",
      "[step: 6379] loss: 0.007492738775908947\n",
      "[step: 6380] loss: 0.007497772574424744\n",
      "[step: 6381] loss: 0.007426874712109566\n",
      "[step: 6382] loss: 0.007437840104103088\n",
      "[step: 6383] loss: 0.007472757715731859\n",
      "[step: 6384] loss: 0.007440207526087761\n",
      "[step: 6385] loss: 0.0074102492071688175\n",
      "[step: 6386] loss: 0.007423731032758951\n",
      "[step: 6387] loss: 0.007444713730365038\n",
      "[step: 6388] loss: 0.007432619575411081\n",
      "[step: 6389] loss: 0.0074087707325816154\n",
      "[step: 6390] loss: 0.007402636110782623\n",
      "[step: 6391] loss: 0.00740800378844142\n",
      "[step: 6392] loss: 0.007413933053612709\n",
      "[step: 6393] loss: 0.00741460919380188\n",
      "[step: 6394] loss: 0.007403778377920389\n",
      "[step: 6395] loss: 0.007394012529402971\n",
      "[step: 6396] loss: 0.007387714460492134\n",
      "[step: 6397] loss: 0.007385396398603916\n",
      "[step: 6398] loss: 0.00738503597676754\n",
      "[step: 6399] loss: 0.007388491183519363\n",
      "[step: 6400] loss: 0.007395710330456495\n",
      "[step: 6401] loss: 0.007402567192912102\n",
      "[step: 6402] loss: 0.007426259573549032\n",
      "[step: 6403] loss: 0.007456829305738211\n",
      "[step: 6404] loss: 0.007529080845415592\n",
      "[step: 6405] loss: 0.007514683995395899\n",
      "[step: 6406] loss: 0.007549212779849768\n",
      "[step: 6407] loss: 0.007465083617717028\n",
      "[step: 6408] loss: 0.007410778664052486\n",
      "[step: 6409] loss: 0.0073744808323681355\n",
      "[step: 6410] loss: 0.007373965345323086\n",
      "[step: 6411] loss: 0.007401334121823311\n",
      "[step: 6412] loss: 0.0074340589344501495\n",
      "[step: 6413] loss: 0.007482229266315699\n",
      "[step: 6414] loss: 0.007456235121935606\n",
      "[step: 6415] loss: 0.007427235599607229\n",
      "[step: 6416] loss: 0.007373579777777195\n",
      "[step: 6417] loss: 0.007367233745753765\n",
      "[step: 6418] loss: 0.007403939962387085\n",
      "[step: 6419] loss: 0.007444457616657019\n",
      "[step: 6420] loss: 0.0074921948835253716\n",
      "[step: 6421] loss: 0.007435718551278114\n",
      "[step: 6422] loss: 0.007396439556032419\n",
      "[step: 6423] loss: 0.007363653741776943\n",
      "[step: 6424] loss: 0.007368067745119333\n",
      "[step: 6425] loss: 0.00740416906774044\n",
      "[step: 6426] loss: 0.0074320402927696705\n",
      "[step: 6427] loss: 0.007448879070580006\n",
      "[step: 6428] loss: 0.007393015548586845\n",
      "[step: 6429] loss: 0.007365731056779623\n",
      "[step: 6430] loss: 0.007354055996984243\n",
      "[step: 6431] loss: 0.007363952696323395\n",
      "[step: 6432] loss: 0.007387076038867235\n",
      "[step: 6433] loss: 0.007391646504402161\n",
      "[step: 6434] loss: 0.007401580456644297\n",
      "[step: 6435] loss: 0.007373455911874771\n",
      "[step: 6436] loss: 0.007357483264058828\n",
      "[step: 6437] loss: 0.00734757399186492\n",
      "[step: 6438] loss: 0.007338866591453552\n",
      "[step: 6439] loss: 0.0073462799191474915\n",
      "[step: 6440] loss: 0.007355387322604656\n",
      "[step: 6441] loss: 0.007377827540040016\n",
      "[step: 6442] loss: 0.007384369615465403\n",
      "[step: 6443] loss: 0.007414105348289013\n",
      "[step: 6444] loss: 0.0074223605915904045\n",
      "[step: 6445] loss: 0.007417330984026194\n",
      "[step: 6446] loss: 0.007378196809440851\n",
      "[step: 6447] loss: 0.007339614909142256\n",
      "[step: 6448] loss: 0.007337540853768587\n",
      "[step: 6449] loss: 0.007334466092288494\n",
      "[step: 6450] loss: 0.007349018938839436\n",
      "[step: 6451] loss: 0.007387313060462475\n",
      "[step: 6452] loss: 0.00741966487839818\n",
      "[step: 6453] loss: 0.007419142406433821\n",
      "[step: 6454] loss: 0.007402436342090368\n",
      "[step: 6455] loss: 0.007374797016382217\n",
      "[step: 6456] loss: 0.007334480527788401\n",
      "[step: 6457] loss: 0.007325139828026295\n",
      "[step: 6458] loss: 0.00732690142467618\n",
      "[step: 6459] loss: 0.007324904669076204\n",
      "[step: 6460] loss: 0.007344424724578857\n",
      "[step: 6461] loss: 0.007357875816524029\n",
      "[step: 6462] loss: 0.007365518249571323\n",
      "[step: 6463] loss: 0.007381234783679247\n",
      "[step: 6464] loss: 0.007379970513284206\n",
      "[step: 6465] loss: 0.007402397692203522\n",
      "[step: 6466] loss: 0.0073785558342933655\n",
      "[step: 6467] loss: 0.007380850613117218\n",
      "[step: 6468] loss: 0.007368595339357853\n",
      "[step: 6469] loss: 0.007333641871809959\n",
      "[step: 6470] loss: 0.007323786150664091\n",
      "[step: 6471] loss: 0.007311020977795124\n",
      "[step: 6472] loss: 0.00731081236153841\n",
      "[step: 6473] loss: 0.007325754035264254\n",
      "[step: 6474] loss: 0.00733307097107172\n",
      "[step: 6475] loss: 0.007341975346207619\n",
      "[step: 6476] loss: 0.0073559824377298355\n",
      "[step: 6477] loss: 0.0073407795280218124\n",
      "[step: 6478] loss: 0.00733930803835392\n",
      "[step: 6479] loss: 0.007323388010263443\n",
      "[step: 6480] loss: 0.007313073612749577\n",
      "[step: 6481] loss: 0.0073089865036308765\n",
      "[step: 6482] loss: 0.007298900280147791\n",
      "[step: 6483] loss: 0.007293473929166794\n",
      "[step: 6484] loss: 0.007294538430869579\n",
      "[step: 6485] loss: 0.00729219289496541\n",
      "[step: 6486] loss: 0.007290608715265989\n",
      "[step: 6487] loss: 0.007290265988558531\n",
      "[step: 6488] loss: 0.007287791930139065\n",
      "[step: 6489] loss: 0.007285357918590307\n",
      "[step: 6490] loss: 0.007286633364856243\n",
      "[step: 6491] loss: 0.007287480868399143\n",
      "[step: 6492] loss: 0.00729405740275979\n",
      "[step: 6493] loss: 0.007311087567359209\n",
      "[step: 6494] loss: 0.007357236463576555\n",
      "[step: 6495] loss: 0.007440806832164526\n",
      "[step: 6496] loss: 0.0077055590227246284\n",
      "[step: 6497] loss: 0.007602705620229244\n",
      "[step: 6498] loss: 0.0076640211045742035\n",
      "[step: 6499] loss: 0.0073642367497086525\n",
      "[step: 6500] loss: 0.007322649471461773\n",
      "[step: 6501] loss: 0.007523959502577782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 6502] loss: 0.007542199920862913\n",
      "[step: 6503] loss: 0.007428270298987627\n",
      "[step: 6504] loss: 0.007344049867242575\n",
      "[step: 6505] loss: 0.007539587095379829\n",
      "[step: 6506] loss: 0.007707266137003899\n",
      "[step: 6507] loss: 0.00802425853908062\n",
      "[step: 6508] loss: 0.008099476806819439\n",
      "[step: 6509] loss: 0.008584409020841122\n",
      "[step: 6510] loss: 0.010162748396396637\n",
      "[step: 6511] loss: 0.01016257330775261\n",
      "[step: 6512] loss: 0.012993726879358292\n",
      "[step: 6513] loss: 0.011053120717406273\n",
      "[step: 6514] loss: 0.010701584629714489\n",
      "[step: 6515] loss: 0.011353966780006886\n",
      "[step: 6516] loss: 0.010626656003296375\n",
      "[step: 6517] loss: 0.011078164912760258\n",
      "[step: 6518] loss: 0.009831106290221214\n",
      "[step: 6519] loss: 0.009496128186583519\n",
      "[step: 6520] loss: 0.010099701583385468\n",
      "[step: 6521] loss: 0.010065191425383091\n",
      "[step: 6522] loss: 0.009309445507824421\n",
      "[step: 6523] loss: 0.009064696729183197\n",
      "[step: 6524] loss: 0.008789783343672752\n",
      "[step: 6525] loss: 0.008997084572911263\n",
      "[step: 6526] loss: 0.009064947254955769\n",
      "[step: 6527] loss: 0.009075410664081573\n",
      "[step: 6528] loss: 0.008578997105360031\n",
      "[step: 6529] loss: 0.00869853887706995\n",
      "[step: 6530] loss: 0.00856081023812294\n",
      "[step: 6531] loss: 0.008553109131753445\n",
      "[step: 6532] loss: 0.008440177887678146\n",
      "[step: 6533] loss: 0.008406298235058784\n",
      "[step: 6534] loss: 0.008124541491270065\n",
      "[step: 6535] loss: 0.008202249184250832\n",
      "[step: 6536] loss: 0.008189331740140915\n",
      "[step: 6537] loss: 0.008222944103181362\n",
      "[step: 6538] loss: 0.008056837134063244\n",
      "[step: 6539] loss: 0.00802429486066103\n",
      "[step: 6540] loss: 0.008001009933650494\n",
      "[step: 6541] loss: 0.008003510534763336\n",
      "[step: 6542] loss: 0.007979527115821838\n",
      "[step: 6543] loss: 0.007933426648378372\n",
      "[step: 6544] loss: 0.007822719402611256\n",
      "[step: 6545] loss: 0.007822579704225063\n",
      "[step: 6546] loss: 0.007774644531309605\n",
      "[step: 6547] loss: 0.007791345939040184\n",
      "[step: 6548] loss: 0.0077386898919939995\n",
      "[step: 6549] loss: 0.007659615017473698\n",
      "[step: 6550] loss: 0.007704547606408596\n",
      "[step: 6551] loss: 0.007666928693652153\n",
      "[step: 6552] loss: 0.00767596997320652\n",
      "[step: 6553] loss: 0.007632255554199219\n",
      "[step: 6554] loss: 0.007637287490069866\n",
      "[step: 6555] loss: 0.007631003390997648\n",
      "[step: 6556] loss: 0.007612879388034344\n",
      "[step: 6557] loss: 0.007589947897940874\n",
      "[step: 6558] loss: 0.00758499326184392\n",
      "[step: 6559] loss: 0.007556663826107979\n",
      "[step: 6560] loss: 0.007543708197772503\n",
      "[step: 6561] loss: 0.007527558133006096\n",
      "[step: 6562] loss: 0.0075218346901237965\n",
      "[step: 6563] loss: 0.007495300378650427\n",
      "[step: 6564] loss: 0.007488560862839222\n",
      "[step: 6565] loss: 0.007477353326976299\n",
      "[step: 6566] loss: 0.0074755121022462845\n",
      "[step: 6567] loss: 0.0074477968737483025\n",
      "[step: 6568] loss: 0.007449519820511341\n",
      "[step: 6569] loss: 0.007439394947141409\n",
      "[step: 6570] loss: 0.007425099145621061\n",
      "[step: 6571] loss: 0.007417359855026007\n",
      "[step: 6572] loss: 0.007405588403344154\n",
      "[step: 6573] loss: 0.007410726044327021\n",
      "[step: 6574] loss: 0.007398403249680996\n",
      "[step: 6575] loss: 0.007382570765912533\n",
      "[step: 6576] loss: 0.007381302770227194\n",
      "[step: 6577] loss: 0.0073668938130140305\n",
      "[step: 6578] loss: 0.007363620679825544\n",
      "[step: 6579] loss: 0.007366546429693699\n",
      "[step: 6580] loss: 0.007355124689638615\n",
      "[step: 6581] loss: 0.007349431049078703\n",
      "[step: 6582] loss: 0.00734553299844265\n",
      "[step: 6583] loss: 0.007337549701333046\n",
      "[step: 6584] loss: 0.007336501497775316\n",
      "[step: 6585] loss: 0.0073375459760427475\n",
      "[step: 6586] loss: 0.007336357608437538\n",
      "[step: 6587] loss: 0.007338740862905979\n",
      "[step: 6588] loss: 0.007347677834331989\n",
      "[step: 6589] loss: 0.007372803520411253\n",
      "[step: 6590] loss: 0.0073979501612484455\n",
      "[step: 6591] loss: 0.007474489044398069\n",
      "[step: 6592] loss: 0.007471560034900904\n",
      "[step: 6593] loss: 0.007541276980191469\n",
      "[step: 6594] loss: 0.0074357278645038605\n",
      "[step: 6595] loss: 0.00735855707898736\n",
      "[step: 6596] loss: 0.007308554835617542\n",
      "[step: 6597] loss: 0.00732366181910038\n",
      "[step: 6598] loss: 0.007380158174782991\n",
      "[step: 6599] loss: 0.007429708726704121\n",
      "[step: 6600] loss: 0.007479014340788126\n",
      "[step: 6601] loss: 0.00741090951487422\n",
      "[step: 6602] loss: 0.0073143900372087955\n",
      "[step: 6603] loss: 0.007326874881982803\n",
      "[step: 6604] loss: 0.0073718661442399025\n",
      "[step: 6605] loss: 0.00737055903300643\n",
      "[step: 6606] loss: 0.007383538875728846\n",
      "[step: 6607] loss: 0.0073102242313325405\n",
      "[step: 6608] loss: 0.007331527769565582\n",
      "[step: 6609] loss: 0.00745950173586607\n",
      "[step: 6610] loss: 0.00744674215093255\n",
      "[step: 6611] loss: 0.00736183300614357\n",
      "[step: 6612] loss: 0.00736185722053051\n",
      "[step: 6613] loss: 0.0074005210772156715\n",
      "[step: 6614] loss: 0.007440449669957161\n",
      "[step: 6615] loss: 0.007366173435002565\n",
      "[step: 6616] loss: 0.007305447943508625\n",
      "[step: 6617] loss: 0.007361549884080887\n",
      "[step: 6618] loss: 0.007459088694304228\n",
      "[step: 6619] loss: 0.0073900423012673855\n",
      "[step: 6620] loss: 0.0072928499430418015\n",
      "[step: 6621] loss: 0.007374505512416363\n",
      "[step: 6622] loss: 0.007419221568852663\n",
      "[step: 6623] loss: 0.00737415999174118\n",
      "[step: 6624] loss: 0.007282843813300133\n",
      "[step: 6625] loss: 0.007402968127280474\n",
      "[step: 6626] loss: 0.007422666065394878\n",
      "[step: 6627] loss: 0.007342588622123003\n",
      "[step: 6628] loss: 0.007315285503864288\n",
      "[step: 6629] loss: 0.0074526285752654076\n",
      "[step: 6630] loss: 0.007429202552884817\n",
      "[step: 6631] loss: 0.0073695108294487\n",
      "[step: 6632] loss: 0.007309724111109972\n",
      "[step: 6633] loss: 0.007422103546559811\n",
      "[step: 6634] loss: 0.007310567889362574\n",
      "[step: 6635] loss: 0.007325350772589445\n",
      "[step: 6636] loss: 0.007313705515116453\n",
      "[step: 6637] loss: 0.007329950574785471\n",
      "[step: 6638] loss: 0.007292950060218573\n",
      "[step: 6639] loss: 0.007292782422155142\n",
      "[step: 6640] loss: 0.007264535408467054\n",
      "[step: 6641] loss: 0.0073012700304389\n",
      "[step: 6642] loss: 0.00728612532839179\n",
      "[step: 6643] loss: 0.007271905429661274\n",
      "[step: 6644] loss: 0.0072584692388772964\n",
      "[step: 6645] loss: 0.0072852917946875095\n",
      "[step: 6646] loss: 0.007270101457834244\n",
      "[step: 6647] loss: 0.0072621251456439495\n",
      "[step: 6648] loss: 0.007257755380123854\n",
      "[step: 6649] loss: 0.007270684000104666\n",
      "[step: 6650] loss: 0.00725856376811862\n",
      "[step: 6651] loss: 0.007243713829666376\n",
      "[step: 6652] loss: 0.007244662381708622\n",
      "[step: 6653] loss: 0.007254547439515591\n",
      "[step: 6654] loss: 0.0072481026872992516\n",
      "[step: 6655] loss: 0.007246385328471661\n",
      "[step: 6656] loss: 0.007240007631480694\n",
      "[step: 6657] loss: 0.00724568497389555\n",
      "[step: 6658] loss: 0.007250814698636532\n",
      "[step: 6659] loss: 0.007246360182762146\n",
      "[step: 6660] loss: 0.007228068541735411\n",
      "[step: 6661] loss: 0.007229553069919348\n",
      "[step: 6662] loss: 0.007235767785459757\n",
      "[step: 6663] loss: 0.007242827210575342\n",
      "[step: 6664] loss: 0.007221702951937914\n",
      "[step: 6665] loss: 0.00722880195826292\n",
      "[step: 6666] loss: 0.007247649133205414\n",
      "[step: 6667] loss: 0.00726744020357728\n",
      "[step: 6668] loss: 0.00723637081682682\n",
      "[step: 6669] loss: 0.007217485457658768\n",
      "[step: 6670] loss: 0.007220094557851553\n",
      "[step: 6671] loss: 0.007242320571094751\n",
      "[step: 6672] loss: 0.007221614476293325\n",
      "[step: 6673] loss: 0.0072088991291821\n",
      "[step: 6674] loss: 0.00721002696081996\n",
      "[step: 6675] loss: 0.007225933950394392\n",
      "[step: 6676] loss: 0.007226019632071257\n",
      "[step: 6677] loss: 0.007210301700979471\n",
      "[step: 6678] loss: 0.007199778221547604\n",
      "[step: 6679] loss: 0.007202643435448408\n",
      "[step: 6680] loss: 0.00720930052921176\n",
      "[step: 6681] loss: 0.00721194501966238\n",
      "[step: 6682] loss: 0.007207734044641256\n",
      "[step: 6683] loss: 0.007199970073997974\n",
      "[step: 6684] loss: 0.007194713223725557\n",
      "[step: 6685] loss: 0.007195937912911177\n",
      "[step: 6686] loss: 0.007202689070254564\n",
      "[step: 6687] loss: 0.007208464667201042\n",
      "[step: 6688] loss: 0.007214161101728678\n",
      "[step: 6689] loss: 0.007222864776849747\n",
      "[step: 6690] loss: 0.007240598089993\n",
      "[step: 6691] loss: 0.007290253881365061\n",
      "[step: 6692] loss: 0.007356364745646715\n",
      "[step: 6693] loss: 0.007465496193617582\n",
      "[step: 6694] loss: 0.007433607708662748\n",
      "[step: 6695] loss: 0.0073630716651678085\n",
      "[step: 6696] loss: 0.007229879032820463\n",
      "[step: 6697] loss: 0.007225039415061474\n",
      "[step: 6698] loss: 0.007283700164407492\n",
      "[step: 6699] loss: 0.007357445545494556\n",
      "[step: 6700] loss: 0.007374369073659182\n",
      "[step: 6701] loss: 0.007311845663934946\n",
      "[step: 6702] loss: 0.007227683439850807\n",
      "[step: 6703] loss: 0.007206265348941088\n",
      "[step: 6704] loss: 0.007258700206875801\n",
      "[step: 6705] loss: 0.007304042112082243\n",
      "[step: 6706] loss: 0.0072759161703288555\n",
      "[step: 6707] loss: 0.007188897579908371\n",
      "[step: 6708] loss: 0.007285015657544136\n",
      "[step: 6709] loss: 0.007451021112501621\n",
      "[step: 6710] loss: 0.007267605978995562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 6711] loss: 0.007411794736981392\n",
      "[step: 6712] loss: 0.007496811915189028\n",
      "[step: 6713] loss: 0.007587932515889406\n",
      "[step: 6714] loss: 0.007773598190397024\n",
      "[step: 6715] loss: 0.0073121460154652596\n",
      "[step: 6716] loss: 0.007794260047376156\n",
      "[step: 6717] loss: 0.007883589714765549\n",
      "[step: 6718] loss: 0.007784285582602024\n",
      "[step: 6719] loss: 0.00733685027807951\n",
      "[step: 6720] loss: 0.008102454245090485\n",
      "[step: 6721] loss: 0.007601367309689522\n",
      "[step: 6722] loss: 0.007742672227323055\n",
      "[step: 6723] loss: 0.00781729444861412\n",
      "[step: 6724] loss: 0.008628017269074917\n",
      "[step: 6725] loss: 0.008927925489842892\n",
      "[step: 6726] loss: 0.0088946633040905\n",
      "[step: 6727] loss: 0.009331031702458858\n",
      "[step: 6728] loss: 0.009242414496839046\n",
      "[step: 6729] loss: 0.009329905733466148\n",
      "[step: 6730] loss: 0.009274083189666271\n",
      "[step: 6731] loss: 0.0092487633228302\n",
      "[step: 6732] loss: 0.00894852913916111\n",
      "[step: 6733] loss: 0.008770579472184181\n",
      "[step: 6734] loss: 0.008573876693844795\n",
      "[step: 6735] loss: 0.008496223948895931\n",
      "[step: 6736] loss: 0.008391164243221283\n",
      "[step: 6737] loss: 0.008077563717961311\n",
      "[step: 6738] loss: 0.008099361322820187\n",
      "[step: 6739] loss: 0.008078260347247124\n",
      "[step: 6740] loss: 0.007923976518213749\n",
      "[step: 6741] loss: 0.007769621908664703\n",
      "[step: 6742] loss: 0.007746782619506121\n",
      "[step: 6743] loss: 0.00783229898661375\n",
      "[step: 6744] loss: 0.0077108643017709255\n",
      "[step: 6745] loss: 0.007653980050235987\n",
      "[step: 6746] loss: 0.00776386633515358\n",
      "[step: 6747] loss: 0.007662882097065449\n",
      "[step: 6748] loss: 0.007562702056020498\n",
      "[step: 6749] loss: 0.007577274460345507\n",
      "[step: 6750] loss: 0.007530078757554293\n",
      "[step: 6751] loss: 0.007487704511731863\n",
      "[step: 6752] loss: 0.007497014477849007\n",
      "[step: 6753] loss: 0.007507402449846268\n",
      "[step: 6754] loss: 0.007482888177037239\n",
      "[step: 6755] loss: 0.007440440822392702\n",
      "[step: 6756] loss: 0.0074362726882100105\n",
      "[step: 6757] loss: 0.00739497784525156\n",
      "[step: 6758] loss: 0.00739365816116333\n",
      "[step: 6759] loss: 0.007387451361864805\n",
      "[step: 6760] loss: 0.007373670116066933\n",
      "[step: 6761] loss: 0.007348938845098019\n",
      "[step: 6762] loss: 0.007311414927244186\n",
      "[step: 6763] loss: 0.007327042054384947\n",
      "[step: 6764] loss: 0.007305051665753126\n",
      "[step: 6765] loss: 0.007294260896742344\n",
      "[step: 6766] loss: 0.007300162687897682\n",
      "[step: 6767] loss: 0.007287471555173397\n",
      "[step: 6768] loss: 0.007268206216394901\n",
      "[step: 6769] loss: 0.007261124439537525\n",
      "[step: 6770] loss: 0.00725795142352581\n",
      "[step: 6771] loss: 0.007256777957081795\n",
      "[step: 6772] loss: 0.007248316891491413\n",
      "[step: 6773] loss: 0.007241449784487486\n",
      "[step: 6774] loss: 0.0072388737462460995\n",
      "[step: 6775] loss: 0.007235378492623568\n",
      "[step: 6776] loss: 0.007230404298752546\n",
      "[step: 6777] loss: 0.007223916705697775\n",
      "[step: 6778] loss: 0.007222708314657211\n",
      "[step: 6779] loss: 0.0072173867374658585\n",
      "[step: 6780] loss: 0.00721258157864213\n",
      "[step: 6781] loss: 0.00721095222979784\n",
      "[step: 6782] loss: 0.007207398768514395\n",
      "[step: 6783] loss: 0.007202500477433205\n",
      "[step: 6784] loss: 0.00720019917935133\n",
      "[step: 6785] loss: 0.007201435975730419\n",
      "[step: 6786] loss: 0.007197496015578508\n",
      "[step: 6787] loss: 0.0071942489594221115\n",
      "[step: 6788] loss: 0.007195188198238611\n",
      "[step: 6789] loss: 0.007197168190032244\n",
      "[step: 6790] loss: 0.007204401306807995\n",
      "[step: 6791] loss: 0.007211968768388033\n",
      "[step: 6792] loss: 0.007232456933706999\n",
      "[step: 6793] loss: 0.007252352777868509\n",
      "[step: 6794] loss: 0.0073160771280527115\n",
      "[step: 6795] loss: 0.007353618275374174\n",
      "[step: 6796] loss: 0.007476362865418196\n",
      "[step: 6797] loss: 0.007356537040323019\n",
      "[step: 6798] loss: 0.007274972274899483\n",
      "[step: 6799] loss: 0.007184331770986319\n",
      "[step: 6800] loss: 0.007189985364675522\n",
      "[step: 6801] loss: 0.007264662533998489\n",
      "[step: 6802] loss: 0.00731040770187974\n",
      "[step: 6803] loss: 0.007363290525972843\n",
      "[step: 6804] loss: 0.007271497510373592\n",
      "[step: 6805] loss: 0.007195694837719202\n",
      "[step: 6806] loss: 0.007174992933869362\n",
      "[step: 6807] loss: 0.007219880353659391\n",
      "[step: 6808] loss: 0.007277299650013447\n",
      "[step: 6809] loss: 0.00723852775990963\n",
      "[step: 6810] loss: 0.007182018365710974\n",
      "[step: 6811] loss: 0.007159565109759569\n",
      "[step: 6812] loss: 0.007175287697464228\n",
      "[step: 6813] loss: 0.007206218782812357\n",
      "[step: 6814] loss: 0.0071922726929187775\n",
      "[step: 6815] loss: 0.007165182381868362\n",
      "[step: 6816] loss: 0.007149444427341223\n",
      "[step: 6817] loss: 0.007157924585044384\n",
      "[step: 6818] loss: 0.007170957513153553\n",
      "[step: 6819] loss: 0.007175206672400236\n",
      "[step: 6820] loss: 0.00716776866465807\n",
      "[step: 6821] loss: 0.0071524037048220634\n",
      "[step: 6822] loss: 0.007142098154872656\n",
      "[step: 6823] loss: 0.007143320050090551\n",
      "[step: 6824] loss: 0.007152084726840258\n",
      "[step: 6825] loss: 0.007164440583437681\n",
      "[step: 6826] loss: 0.00716799683868885\n",
      "[step: 6827] loss: 0.007169620133936405\n",
      "[step: 6828] loss: 0.007158908527344465\n",
      "[step: 6829] loss: 0.007147813215851784\n",
      "[step: 6830] loss: 0.007135694846510887\n",
      "[step: 6831] loss: 0.007130416110157967\n",
      "[step: 6832] loss: 0.00712923239916563\n",
      "[step: 6833] loss: 0.007131157908588648\n",
      "[step: 6834] loss: 0.007136151194572449\n",
      "[step: 6835] loss: 0.0071441615000367165\n",
      "[step: 6836] loss: 0.007156167179346085\n",
      "[step: 6837] loss: 0.0071669211611151695\n",
      "[step: 6838] loss: 0.0071856300346553326\n",
      "[step: 6839] loss: 0.007195593323558569\n",
      "[step: 6840] loss: 0.007223853841423988\n",
      "[step: 6841] loss: 0.007226658519357443\n",
      "[step: 6842] loss: 0.007249531336128712\n",
      "[step: 6843] loss: 0.007221778854727745\n",
      "[step: 6844] loss: 0.007208199705928564\n",
      "[step: 6845] loss: 0.007171603851020336\n",
      "[step: 6846] loss: 0.0071482532657682896\n",
      "[step: 6847] loss: 0.007128875236958265\n",
      "[step: 6848] loss: 0.00711845001205802\n",
      "[step: 6849] loss: 0.007113022264093161\n",
      "[step: 6850] loss: 0.007110786624252796\n",
      "[step: 6851] loss: 0.007110940292477608\n",
      "[step: 6852] loss: 0.007112998049706221\n",
      "[step: 6853] loss: 0.007117845583707094\n",
      "[step: 6854] loss: 0.0071246386505663395\n",
      "[step: 6855] loss: 0.007137445267289877\n",
      "[step: 6856] loss: 0.00715358043089509\n",
      "[step: 6857] loss: 0.007186207454651594\n",
      "[step: 6858] loss: 0.007215067278593779\n",
      "[step: 6859] loss: 0.007276502437889576\n",
      "[step: 6860] loss: 0.007297816686332226\n",
      "[step: 6861] loss: 0.007339998614042997\n",
      "[step: 6862] loss: 0.007256317883729935\n",
      "[step: 6863] loss: 0.007187936455011368\n",
      "[step: 6864] loss: 0.007120152004063129\n",
      "[step: 6865] loss: 0.0070999544113874435\n",
      "[step: 6866] loss: 0.007122088223695755\n",
      "[step: 6867] loss: 0.0071831559762358665\n",
      "[step: 6868] loss: 0.007275781128555536\n",
      "[step: 6869] loss: 0.0073580192402005196\n",
      "[step: 6870] loss: 0.007397947367280722\n",
      "[step: 6871] loss: 0.0072005256079137325\n",
      "[step: 6872] loss: 0.007112691644579172\n",
      "[step: 6873] loss: 0.007191489450633526\n",
      "[step: 6874] loss: 0.007296876981854439\n",
      "[step: 6875] loss: 0.007455821149051189\n",
      "[step: 6876] loss: 0.007660691626369953\n",
      "[step: 6877] loss: 0.007568201050162315\n",
      "[step: 6878] loss: 0.007395876105874777\n",
      "[step: 6879] loss: 0.007394679822027683\n",
      "[step: 6880] loss: 0.007562684360891581\n",
      "[step: 6881] loss: 0.007338840514421463\n",
      "[step: 6882] loss: 0.0072796884924173355\n",
      "[step: 6883] loss: 0.007379710208624601\n",
      "[step: 6884] loss: 0.007500477135181427\n",
      "[step: 6885] loss: 0.007726472802460194\n",
      "[step: 6886] loss: 0.0075208512134850025\n",
      "[step: 6887] loss: 0.008135135285556316\n",
      "[step: 6888] loss: 0.008887008763849735\n",
      "[step: 6889] loss: 0.008773243986070156\n",
      "[step: 6890] loss: 0.008209981955587864\n",
      "[step: 6891] loss: 0.009075208567082882\n",
      "[step: 6892] loss: 0.008610526099801064\n",
      "[step: 6893] loss: 0.008404375985264778\n",
      "[step: 6894] loss: 0.008502666838467121\n",
      "[step: 6895] loss: 0.00866146944463253\n",
      "[step: 6896] loss: 0.008048192597925663\n",
      "[step: 6897] loss: 0.008106152527034283\n",
      "[step: 6898] loss: 0.008167550899088383\n",
      "[step: 6899] loss: 0.007921489886939526\n",
      "[step: 6900] loss: 0.007921777665615082\n",
      "[step: 6901] loss: 0.00795619748532772\n",
      "[step: 6902] loss: 0.007811053656041622\n",
      "[step: 6903] loss: 0.007734672166407108\n",
      "[step: 6904] loss: 0.007772395852953196\n",
      "[step: 6905] loss: 0.007672040723264217\n",
      "[step: 6906] loss: 0.007570040877908468\n",
      "[step: 6907] loss: 0.007546551059931517\n",
      "[step: 6908] loss: 0.00758381700143218\n",
      "[step: 6909] loss: 0.007538266014307737\n",
      "[step: 6910] loss: 0.007405145559459925\n",
      "[step: 6911] loss: 0.007441800087690353\n",
      "[step: 6912] loss: 0.007427360396832228\n",
      "[step: 6913] loss: 0.007387214340269566\n",
      "[step: 6914] loss: 0.007337425835430622\n",
      "[step: 6915] loss: 0.007368254009634256\n",
      "[step: 6916] loss: 0.007366596255451441\n",
      "[step: 6917] loss: 0.007282279431819916\n",
      "[step: 6918] loss: 0.007286528125405312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 6919] loss: 0.007304376922547817\n",
      "[step: 6920] loss: 0.007235737983137369\n",
      "[step: 6921] loss: 0.0072382292710244656\n",
      "[step: 6922] loss: 0.007221077103167772\n",
      "[step: 6923] loss: 0.007217591628432274\n",
      "[step: 6924] loss: 0.007212160155177116\n",
      "[step: 6925] loss: 0.007184009067714214\n",
      "[step: 6926] loss: 0.007187479641288519\n",
      "[step: 6927] loss: 0.007167274132370949\n",
      "[step: 6928] loss: 0.007181202527135611\n",
      "[step: 6929] loss: 0.007161651272326708\n",
      "[step: 6930] loss: 0.007153518032282591\n",
      "[step: 6931] loss: 0.0071363248862326145\n",
      "[step: 6932] loss: 0.0071570235304534435\n",
      "[step: 6933] loss: 0.007137375883758068\n",
      "[step: 6934] loss: 0.007120131980627775\n",
      "[step: 6935] loss: 0.007114357780665159\n",
      "[step: 6936] loss: 0.007121226750314236\n",
      "[step: 6937] loss: 0.007119457703083754\n",
      "[step: 6938] loss: 0.007108792662620544\n",
      "[step: 6939] loss: 0.00710421334952116\n",
      "[step: 6940] loss: 0.007099797949194908\n",
      "[step: 6941] loss: 0.007097247056663036\n",
      "[step: 6942] loss: 0.0070966933853924274\n",
      "[step: 6943] loss: 0.007095857989042997\n",
      "[step: 6944] loss: 0.007090957835316658\n",
      "[step: 6945] loss: 0.007088560611009598\n",
      "[step: 6946] loss: 0.007085239514708519\n",
      "[step: 6947] loss: 0.007086888886988163\n",
      "[step: 6948] loss: 0.007083605043590069\n",
      "[step: 6949] loss: 0.007081249728798866\n",
      "[step: 6950] loss: 0.007078827824443579\n",
      "[step: 6951] loss: 0.007077937480062246\n",
      "[step: 6952] loss: 0.007073316257447004\n",
      "[step: 6953] loss: 0.007068614009767771\n",
      "[step: 6954] loss: 0.007067779079079628\n",
      "[step: 6955] loss: 0.007068968378007412\n",
      "[step: 6956] loss: 0.00706865219399333\n",
      "[step: 6957] loss: 0.007068074308335781\n",
      "[step: 6958] loss: 0.007068032398819923\n",
      "[step: 6959] loss: 0.007069109473377466\n",
      "[step: 6960] loss: 0.007069950457662344\n",
      "[step: 6961] loss: 0.00706908805295825\n",
      "[step: 6962] loss: 0.007069805637001991\n",
      "[step: 6963] loss: 0.007075173780322075\n",
      "[step: 6964] loss: 0.007086796220391989\n",
      "[step: 6965] loss: 0.00710827112197876\n",
      "[step: 6966] loss: 0.007135174702852964\n",
      "[step: 6967] loss: 0.007191536948084831\n",
      "[step: 6968] loss: 0.007235425990074873\n",
      "[step: 6969] loss: 0.007296303752809763\n",
      "[step: 6970] loss: 0.0072470493614673615\n",
      "[step: 6971] loss: 0.007212575990706682\n",
      "[step: 6972] loss: 0.007129067089408636\n",
      "[step: 6973] loss: 0.007075922563672066\n",
      "[step: 6974] loss: 0.00704905204474926\n",
      "[step: 6975] loss: 0.007049054838716984\n",
      "[step: 6976] loss: 0.007068393286317587\n",
      "[step: 6977] loss: 0.007104593329131603\n",
      "[step: 6978] loss: 0.007149021606892347\n",
      "[step: 6979] loss: 0.0071684508584439754\n",
      "[step: 6980] loss: 0.0071686627343297005\n",
      "[step: 6981] loss: 0.0071039749309420586\n",
      "[step: 6982] loss: 0.007052279077470303\n",
      "[step: 6983] loss: 0.007035757414996624\n",
      "[step: 6984] loss: 0.00705421157181263\n",
      "[step: 6985] loss: 0.00709945522248745\n",
      "[step: 6986] loss: 0.007157571613788605\n",
      "[step: 6987] loss: 0.007214140146970749\n",
      "[step: 6988] loss: 0.007190498989075422\n",
      "[step: 6989] loss: 0.007136722095310688\n",
      "[step: 6990] loss: 0.007055896800011396\n",
      "[step: 6991] loss: 0.007035164162516594\n",
      "[step: 6992] loss: 0.007068767677992582\n",
      "[step: 6993] loss: 0.0071267252787947655\n",
      "[step: 6994] loss: 0.007194899953901768\n",
      "[step: 6995] loss: 0.007242488209158182\n",
      "[step: 6996] loss: 0.007227815687656403\n",
      "[step: 6997] loss: 0.007098401430994272\n",
      "[step: 6998] loss: 0.007037881761789322\n",
      "[step: 6999] loss: 0.007076959125697613\n",
      "[step: 7000] loss: 0.0071440571919083595\n",
      "[step: 7001] loss: 0.007205708883702755\n",
      "[step: 7002] loss: 0.0072763715870678425\n",
      "[step: 7003] loss: 0.007235727272927761\n",
      "[step: 7004] loss: 0.007082231342792511\n",
      "[step: 7005] loss: 0.007051507476717234\n",
      "[step: 7006] loss: 0.007163060829043388\n",
      "[step: 7007] loss: 0.007186446338891983\n",
      "[step: 7008] loss: 0.007194292265921831\n",
      "[step: 7009] loss: 0.007223682012408972\n",
      "[step: 7010] loss: 0.007182559464126825\n",
      "[step: 7011] loss: 0.0070871831849217415\n",
      "[step: 7012] loss: 0.007099096197634935\n",
      "[step: 7013] loss: 0.00715895090252161\n",
      "[step: 7014] loss: 0.007139366120100021\n",
      "[step: 7015] loss: 0.007074596825987101\n",
      "[step: 7016] loss: 0.007045367266982794\n",
      "[step: 7017] loss: 0.007072382606565952\n",
      "[step: 7018] loss: 0.007059877272695303\n",
      "[step: 7019] loss: 0.007069418206810951\n",
      "[step: 7020] loss: 0.0070768422447144985\n",
      "[step: 7021] loss: 0.007098715752363205\n",
      "[step: 7022] loss: 0.007043876685202122\n",
      "[step: 7023] loss: 0.007029025815427303\n",
      "[step: 7024] loss: 0.007020012941211462\n",
      "[step: 7025] loss: 0.007035337388515472\n",
      "[step: 7026] loss: 0.007028305437415838\n",
      "[step: 7027] loss: 0.0070327515713870525\n",
      "[step: 7028] loss: 0.0070142545737326145\n",
      "[step: 7029] loss: 0.007008606102317572\n",
      "[step: 7030] loss: 0.007018661592155695\n",
      "[step: 7031] loss: 0.007021714933216572\n",
      "[step: 7032] loss: 0.007014283444732428\n",
      "[step: 7033] loss: 0.006993891671299934\n",
      "[step: 7034] loss: 0.007004243787378073\n",
      "[step: 7035] loss: 0.0070142559707164764\n",
      "[step: 7036] loss: 0.007043562363833189\n",
      "[step: 7037] loss: 0.007048272993415594\n",
      "[step: 7038] loss: 0.007059120107442141\n",
      "[step: 7039] loss: 0.007058979012072086\n",
      "[step: 7040] loss: 0.007066001649945974\n",
      "[step: 7041] loss: 0.007047062739729881\n",
      "[step: 7042] loss: 0.007027509622275829\n",
      "[step: 7043] loss: 0.007010491099208593\n",
      "[step: 7044] loss: 0.0070007918402552605\n",
      "[step: 7045] loss: 0.007004102226346731\n",
      "[step: 7046] loss: 0.007010484114289284\n",
      "[step: 7047] loss: 0.007030182052403688\n",
      "[step: 7048] loss: 0.007016436196863651\n",
      "[step: 7049] loss: 0.007029862608760595\n",
      "[step: 7050] loss: 0.007046770304441452\n",
      "[step: 7051] loss: 0.007063280325382948\n",
      "[step: 7052] loss: 0.007085171528160572\n",
      "[step: 7053] loss: 0.007117695640772581\n",
      "[step: 7054] loss: 0.007206365931779146\n",
      "[step: 7055] loss: 0.0072597782127559185\n",
      "[step: 7056] loss: 0.007323581259697676\n",
      "[step: 7057] loss: 0.007203848101198673\n",
      "[step: 7058] loss: 0.007082450669258833\n",
      "[step: 7059] loss: 0.006978211458772421\n",
      "[step: 7060] loss: 0.007001509889960289\n",
      "[step: 7061] loss: 0.007114754989743233\n",
      "[step: 7062] loss: 0.007452031597495079\n",
      "[step: 7063] loss: 0.007574870716780424\n",
      "[step: 7064] loss: 0.007197029422968626\n",
      "[step: 7065] loss: 0.007029439322650433\n",
      "[step: 7066] loss: 0.007199130021035671\n",
      "[step: 7067] loss: 0.007192207034677267\n",
      "[step: 7068] loss: 0.007191566284745932\n",
      "[step: 7069] loss: 0.007177381310611963\n",
      "[step: 7070] loss: 0.007086960598826408\n",
      "[step: 7071] loss: 0.007038723211735487\n",
      "[step: 7072] loss: 0.0070787216536700726\n",
      "[step: 7073] loss: 0.007101248484104872\n",
      "[step: 7074] loss: 0.007060288917273283\n",
      "[step: 7075] loss: 0.0070054735988378525\n",
      "[step: 7076] loss: 0.0070207430981099606\n",
      "[step: 7077] loss: 0.007041077129542828\n",
      "[step: 7078] loss: 0.007012763526290655\n",
      "[step: 7079] loss: 0.00698305107653141\n",
      "[step: 7080] loss: 0.006982593331485987\n",
      "[step: 7081] loss: 0.006985296495258808\n",
      "[step: 7082] loss: 0.0069677322171628475\n",
      "[step: 7083] loss: 0.006984709296375513\n",
      "[step: 7084] loss: 0.006959124468266964\n",
      "[step: 7085] loss: 0.006955957505851984\n",
      "[step: 7086] loss: 0.00696395942941308\n",
      "[step: 7087] loss: 0.006998077500611544\n",
      "[step: 7088] loss: 0.007008926477283239\n",
      "[step: 7089] loss: 0.006985231302678585\n",
      "[step: 7090] loss: 0.0069527095183730125\n",
      "[step: 7091] loss: 0.006962889339774847\n",
      "[step: 7092] loss: 0.0069725401699543\n",
      "[step: 7093] loss: 0.006972138304263353\n",
      "[step: 7094] loss: 0.0069455658085644245\n",
      "[step: 7095] loss: 0.006950154900550842\n",
      "[step: 7096] loss: 0.00696105370298028\n",
      "[step: 7097] loss: 0.00695144385099411\n",
      "[step: 7098] loss: 0.00693637877702713\n",
      "[step: 7099] loss: 0.006937025580555201\n",
      "[step: 7100] loss: 0.006946321111172438\n",
      "[step: 7101] loss: 0.00693426001816988\n",
      "[step: 7102] loss: 0.00693886261433363\n",
      "[step: 7103] loss: 0.006928714923560619\n",
      "[step: 7104] loss: 0.006935215089470148\n",
      "[step: 7105] loss: 0.006949290633201599\n",
      "[step: 7106] loss: 0.0069794319570064545\n",
      "[step: 7107] loss: 0.007056888658553362\n",
      "[step: 7108] loss: 0.00716016860678792\n",
      "[step: 7109] loss: 0.007388073019683361\n",
      "[step: 7110] loss: 0.00736978929489851\n",
      "[step: 7111] loss: 0.007343916688114405\n",
      "[step: 7112] loss: 0.007008508779108524\n",
      "[step: 7113] loss: 0.006928674876689911\n",
      "[step: 7114] loss: 0.007074848748743534\n",
      "[step: 7115] loss: 0.007226823829114437\n",
      "[step: 7116] loss: 0.007430736906826496\n",
      "[step: 7117] loss: 0.007553513161838055\n",
      "[step: 7118] loss: 0.007365219295024872\n",
      "[step: 7119] loss: 0.007101002614945173\n",
      "[step: 7120] loss: 0.007336702663451433\n",
      "[step: 7121] loss: 0.007566583342850208\n",
      "[step: 7122] loss: 0.007104824297130108\n",
      "[step: 7123] loss: 0.007386122830212116\n",
      "[step: 7124] loss: 0.007742853369563818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 7125] loss: 0.0069776419550180435\n",
      "[step: 7126] loss: 0.007662521675229073\n",
      "[step: 7127] loss: 0.00865517184138298\n",
      "[step: 7128] loss: 0.008475396782159805\n",
      "[step: 7129] loss: 0.008331529796123505\n",
      "[step: 7130] loss: 0.008673562668263912\n",
      "[step: 7131] loss: 0.008901909925043583\n",
      "[step: 7132] loss: 0.00855307001620531\n",
      "[step: 7133] loss: 0.009009287692606449\n",
      "[step: 7134] loss: 0.008986816741526127\n",
      "[step: 7135] loss: 0.008145775645971298\n",
      "[step: 7136] loss: 0.008689334616065025\n",
      "[step: 7137] loss: 0.008147952146828175\n",
      "[step: 7138] loss: 0.0079802256077528\n",
      "[step: 7139] loss: 0.008336583152413368\n",
      "[step: 7140] loss: 0.008045753464102745\n",
      "[step: 7141] loss: 0.007794371340423822\n",
      "[step: 7142] loss: 0.007903778925538063\n",
      "[step: 7143] loss: 0.007681171875447035\n",
      "[step: 7144] loss: 0.007664607372134924\n",
      "[step: 7145] loss: 0.007619604002684355\n",
      "[step: 7146] loss: 0.0075028701685369015\n",
      "[step: 7147] loss: 0.0074287960305809975\n",
      "[step: 7148] loss: 0.007483239285647869\n",
      "[step: 7149] loss: 0.007362206932157278\n",
      "[step: 7150] loss: 0.007358125876635313\n",
      "[step: 7151] loss: 0.007244163192808628\n",
      "[step: 7152] loss: 0.007358350791037083\n",
      "[step: 7153] loss: 0.007225397974252701\n",
      "[step: 7154] loss: 0.007243669591844082\n",
      "[step: 7155] loss: 0.007192520424723625\n",
      "[step: 7156] loss: 0.007174561731517315\n",
      "[step: 7157] loss: 0.00714335311204195\n",
      "[step: 7158] loss: 0.007153661921620369\n",
      "[step: 7159] loss: 0.007120268885046244\n",
      "[step: 7160] loss: 0.007082156836986542\n",
      "[step: 7161] loss: 0.007095790933817625\n",
      "[step: 7162] loss: 0.00709636602550745\n",
      "[step: 7163] loss: 0.007070395164191723\n",
      "[step: 7164] loss: 0.007064690347760916\n",
      "[step: 7165] loss: 0.007059063296765089\n",
      "[step: 7166] loss: 0.007018995471298695\n",
      "[step: 7167] loss: 0.007011975161731243\n",
      "[step: 7168] loss: 0.00701241847127676\n",
      "[step: 7169] loss: 0.007005476858466864\n",
      "[step: 7170] loss: 0.007082545198500156\n",
      "[step: 7171] loss: 0.007154533639550209\n",
      "[step: 7172] loss: 0.007034050300717354\n",
      "[step: 7173] loss: 0.007039278745651245\n",
      "[step: 7174] loss: 0.007159462198615074\n",
      "[step: 7175] loss: 0.0072164940647780895\n",
      "[step: 7176] loss: 0.007119864225387573\n",
      "[step: 7177] loss: 0.006992677226662636\n",
      "[step: 7178] loss: 0.0071186888962984085\n",
      "[step: 7179] loss: 0.0071097491309046745\n",
      "[step: 7180] loss: 0.006950652226805687\n",
      "[step: 7181] loss: 0.0071487086825072765\n",
      "[step: 7182] loss: 0.007122513838112354\n",
      "[step: 7183] loss: 0.0069220713339746\n",
      "[step: 7184] loss: 0.007127837277948856\n",
      "[step: 7185] loss: 0.007092152256518602\n",
      "[step: 7186] loss: 0.0069327885285019875\n",
      "[step: 7187] loss: 0.0071311574429273605\n",
      "[step: 7188] loss: 0.007050617132335901\n",
      "[step: 7189] loss: 0.006957408040761948\n",
      "[step: 7190] loss: 0.007134539540857077\n",
      "[step: 7191] loss: 0.007085352670401335\n",
      "[step: 7192] loss: 0.006964193657040596\n",
      "[step: 7193] loss: 0.007085457444190979\n",
      "[step: 7194] loss: 0.007081740070134401\n",
      "[step: 7195] loss: 0.006997444201260805\n",
      "[step: 7196] loss: 0.007101631257683039\n",
      "[step: 7197] loss: 0.007127567194402218\n",
      "[step: 7198] loss: 0.006902832537889481\n",
      "[step: 7199] loss: 0.007158641237765551\n",
      "[step: 7200] loss: 0.007086755707859993\n",
      "[step: 7201] loss: 0.006895618047565222\n",
      "[step: 7202] loss: 0.007108799647539854\n",
      "[step: 7203] loss: 0.0070119211450219154\n",
      "[step: 7204] loss: 0.0069146351888775826\n",
      "[step: 7205] loss: 0.007006532978266478\n",
      "[step: 7206] loss: 0.006977051962167025\n",
      "[step: 7207] loss: 0.006932124961167574\n",
      "[step: 7208] loss: 0.006945753004401922\n",
      "[step: 7209] loss: 0.006971036083996296\n",
      "[step: 7210] loss: 0.006845115218311548\n",
      "[step: 7211] loss: 0.006920923944562674\n",
      "[step: 7212] loss: 0.0069861216470599174\n",
      "[step: 7213] loss: 0.006880429573357105\n",
      "[step: 7214] loss: 0.006838444620370865\n",
      "[step: 7215] loss: 0.006919106002897024\n",
      "[step: 7216] loss: 0.006843812298029661\n",
      "[step: 7217] loss: 0.00683959573507309\n",
      "[step: 7218] loss: 0.006891830824315548\n",
      "[step: 7219] loss: 0.006842771545052528\n",
      "[step: 7220] loss: 0.00682888925075531\n",
      "[step: 7221] loss: 0.006821923423558474\n",
      "[step: 7222] loss: 0.006832387764006853\n",
      "[step: 7223] loss: 0.006837206892669201\n",
      "[step: 7224] loss: 0.006812420208007097\n",
      "[step: 7225] loss: 0.0068005528301000595\n",
      "[step: 7226] loss: 0.00682672206312418\n",
      "[step: 7227] loss: 0.006804607342928648\n",
      "[step: 7228] loss: 0.006777807604521513\n",
      "[step: 7229] loss: 0.00679331598803401\n",
      "[step: 7230] loss: 0.006808408070355654\n",
      "[step: 7231] loss: 0.006791246589273214\n",
      "[step: 7232] loss: 0.00676876213401556\n",
      "[step: 7233] loss: 0.006772624794393778\n",
      "[step: 7234] loss: 0.0067818849347531796\n",
      "[step: 7235] loss: 0.006774793844670057\n",
      "[step: 7236] loss: 0.006757122930139303\n",
      "[step: 7237] loss: 0.006756546441465616\n",
      "[step: 7238] loss: 0.006761279888451099\n",
      "[step: 7239] loss: 0.006761224940419197\n",
      "[step: 7240] loss: 0.006752405781298876\n",
      "[step: 7241] loss: 0.006748896092176437\n",
      "[step: 7242] loss: 0.006750891916453838\n",
      "[step: 7243] loss: 0.00675526587292552\n",
      "[step: 7244] loss: 0.0067492397502064705\n",
      "[step: 7245] loss: 0.006739895790815353\n",
      "[step: 7246] loss: 0.006731946486979723\n",
      "[step: 7247] loss: 0.006728490814566612\n",
      "[step: 7248] loss: 0.006727994419634342\n",
      "[step: 7249] loss: 0.006728279870003462\n",
      "[step: 7250] loss: 0.006726723629981279\n",
      "[step: 7251] loss: 0.006722265854477882\n",
      "[step: 7252] loss: 0.006718154530972242\n",
      "[step: 7253] loss: 0.006713313981890678\n",
      "[step: 7254] loss: 0.006710194982588291\n",
      "[step: 7255] loss: 0.006707397289574146\n",
      "[step: 7256] loss: 0.006705761421471834\n",
      "[step: 7257] loss: 0.006704301573336124\n",
      "[step: 7258] loss: 0.006703775841742754\n",
      "[step: 7259] loss: 0.006704071536660194\n",
      "[step: 7260] loss: 0.006707218941301107\n",
      "[step: 7261] loss: 0.006716318428516388\n",
      "[step: 7262] loss: 0.006744846235960722\n",
      "[step: 7263] loss: 0.006831002421677113\n",
      "[step: 7264] loss: 0.007105668541043997\n",
      "[step: 7265] loss: 0.007977653294801712\n",
      "[step: 7266] loss: 0.008555163629353046\n",
      "[step: 7267] loss: 0.006968939211219549\n",
      "[step: 7268] loss: 0.008144007064402103\n",
      "[step: 7269] loss: 0.008982782252132893\n",
      "[step: 7270] loss: 0.008234424516558647\n",
      "[step: 7271] loss: 0.009340042248368263\n",
      "[step: 7272] loss: 0.00775444321334362\n",
      "[step: 7273] loss: 0.008337452076375484\n",
      "[step: 7274] loss: 0.007885566912591457\n",
      "[step: 7275] loss: 0.007259420584887266\n",
      "[step: 7276] loss: 0.007893581874668598\n",
      "[step: 7277] loss: 0.00802169181406498\n",
      "[step: 7278] loss: 0.007860256358981133\n",
      "[step: 7279] loss: 0.00815803837031126\n",
      "[step: 7280] loss: 0.00846189260482788\n",
      "[step: 7281] loss: 0.008101003244519234\n",
      "[step: 7282] loss: 0.007942148484289646\n",
      "[step: 7283] loss: 0.00766005739569664\n",
      "[step: 7284] loss: 0.0076502528972923756\n",
      "[step: 7285] loss: 0.007720471825450659\n",
      "[step: 7286] loss: 0.007416525389999151\n",
      "[step: 7287] loss: 0.007650637999176979\n",
      "[step: 7288] loss: 0.007720356807112694\n",
      "[step: 7289] loss: 0.007487265858799219\n",
      "[step: 7290] loss: 0.007203054614365101\n",
      "[step: 7291] loss: 0.007408322300761938\n",
      "[step: 7292] loss: 0.007340605370700359\n",
      "[step: 7293] loss: 0.007279701065272093\n",
      "[step: 7294] loss: 0.007230446208268404\n",
      "[step: 7295] loss: 0.0071551501750946045\n",
      "[step: 7296] loss: 0.007118940353393555\n",
      "[step: 7297] loss: 0.0071795061230659485\n",
      "[step: 7298] loss: 0.007067414931952953\n",
      "[step: 7299] loss: 0.006979063618928194\n",
      "[step: 7300] loss: 0.007055383175611496\n",
      "[step: 7301] loss: 0.007007404696196318\n",
      "[step: 7302] loss: 0.006893339101225138\n",
      "[step: 7303] loss: 0.006945470348000526\n",
      "[step: 7304] loss: 0.006961208302527666\n",
      "[step: 7305] loss: 0.00691742729395628\n",
      "[step: 7306] loss: 0.006885026581585407\n",
      "[step: 7307] loss: 0.006859899498522282\n",
      "[step: 7308] loss: 0.006840874440968037\n",
      "[step: 7309] loss: 0.006893106270581484\n",
      "[step: 7310] loss: 0.006826865952461958\n",
      "[step: 7311] loss: 0.0068042282946407795\n",
      "[step: 7312] loss: 0.006834362167865038\n",
      "[step: 7313] loss: 0.006804668810218573\n",
      "[step: 7314] loss: 0.0067801205441355705\n",
      "[step: 7315] loss: 0.006792502477765083\n",
      "[step: 7316] loss: 0.0067780218087136745\n",
      "[step: 7317] loss: 0.006763103883713484\n",
      "[step: 7318] loss: 0.006754634436219931\n",
      "[step: 7319] loss: 0.006745962891727686\n",
      "[step: 7320] loss: 0.006754959933459759\n",
      "[step: 7321] loss: 0.0067308684810996056\n",
      "[step: 7322] loss: 0.0067305429838597775\n",
      "[step: 7323] loss: 0.006720986682921648\n",
      "[step: 7324] loss: 0.006721692159771919\n",
      "[step: 7325] loss: 0.00671101501211524\n",
      "[step: 7326] loss: 0.006706442218273878\n",
      "[step: 7327] loss: 0.006703645922243595\n",
      "[step: 7328] loss: 0.006698260549455881\n",
      "[step: 7329] loss: 0.006687378045171499\n",
      "[step: 7330] loss: 0.006691344548016787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 7331] loss: 0.006683679297566414\n",
      "[step: 7332] loss: 0.00667790649458766\n",
      "[step: 7333] loss: 0.006674092262983322\n",
      "[step: 7334] loss: 0.006673426833003759\n",
      "[step: 7335] loss: 0.006666537839919329\n",
      "[step: 7336] loss: 0.006662443745881319\n",
      "[step: 7337] loss: 0.006662111729383469\n",
      "[step: 7338] loss: 0.006658060941845179\n",
      "[step: 7339] loss: 0.006652502808719873\n",
      "[step: 7340] loss: 0.00665126321837306\n",
      "[step: 7341] loss: 0.006646307650953531\n",
      "[step: 7342] loss: 0.0066442363895475864\n",
      "[step: 7343] loss: 0.006640884559601545\n",
      "[step: 7344] loss: 0.006637627258896828\n",
      "[step: 7345] loss: 0.0066344765946269035\n",
      "[step: 7346] loss: 0.006632004864513874\n",
      "[step: 7347] loss: 0.006629628129303455\n",
      "[step: 7348] loss: 0.006625267211347818\n",
      "[step: 7349] loss: 0.006623362191021442\n",
      "[step: 7350] loss: 0.006621090695261955\n",
      "[step: 7351] loss: 0.006618567276746035\n",
      "[step: 7352] loss: 0.006615574937313795\n",
      "[step: 7353] loss: 0.006613670848309994\n",
      "[step: 7354] loss: 0.006612051278352737\n",
      "[step: 7355] loss: 0.006612217985093594\n",
      "[step: 7356] loss: 0.006616430357098579\n",
      "[step: 7357] loss: 0.006632867734879255\n",
      "[step: 7358] loss: 0.0066597312688827515\n",
      "[step: 7359] loss: 0.006693734787404537\n",
      "[step: 7360] loss: 0.006670976057648659\n",
      "[step: 7361] loss: 0.006622534245252609\n",
      "[step: 7362] loss: 0.00660292524844408\n",
      "[step: 7363] loss: 0.006613977253437042\n",
      "[step: 7364] loss: 0.006649528164416552\n",
      "[step: 7365] loss: 0.006705887150019407\n",
      "[step: 7366] loss: 0.006868905853480101\n",
      "[step: 7367] loss: 0.006903418805450201\n",
      "[step: 7368] loss: 0.006720306817442179\n",
      "[step: 7369] loss: 0.006654310971498489\n",
      "[step: 7370] loss: 0.006821472197771072\n",
      "[step: 7371] loss: 0.00687872339040041\n",
      "[step: 7372] loss: 0.006937965285032988\n",
      "[step: 7373] loss: 0.0073895505629479885\n",
      "[step: 7374] loss: 0.007039989810436964\n",
      "[step: 7375] loss: 0.006977090612053871\n",
      "[step: 7376] loss: 0.007058973889797926\n",
      "[step: 7377] loss: 0.007293178234249353\n",
      "[step: 7378] loss: 0.006865416653454304\n",
      "[step: 7379] loss: 0.0070052193477749825\n",
      "[step: 7380] loss: 0.006849631667137146\n",
      "[step: 7381] loss: 0.006920928135514259\n",
      "[step: 7382] loss: 0.006854619365185499\n",
      "[step: 7383] loss: 0.00693044438958168\n",
      "[step: 7384] loss: 0.006859627552330494\n",
      "[step: 7385] loss: 0.006797616370022297\n",
      "[step: 7386] loss: 0.006744652986526489\n",
      "[step: 7387] loss: 0.006804896984249353\n",
      "[step: 7388] loss: 0.006786656100302935\n",
      "[step: 7389] loss: 0.006688449531793594\n",
      "[step: 7390] loss: 0.006731709931045771\n",
      "[step: 7391] loss: 0.006659328937530518\n",
      "[step: 7392] loss: 0.006645496468991041\n",
      "[step: 7393] loss: 0.006767039652913809\n",
      "[step: 7394] loss: 0.006833301391452551\n",
      "[step: 7395] loss: 0.006958744488656521\n",
      "[step: 7396] loss: 0.006838655564934015\n",
      "[step: 7397] loss: 0.006993522401899099\n",
      "[step: 7398] loss: 0.0067883823066949844\n",
      "[step: 7399] loss: 0.006851866375654936\n",
      "[step: 7400] loss: 0.0069115604273974895\n",
      "[step: 7401] loss: 0.00702873058617115\n",
      "[step: 7402] loss: 0.006897276267409325\n",
      "[step: 7403] loss: 0.006917349994182587\n",
      "[step: 7404] loss: 0.0069551290944218636\n",
      "[step: 7405] loss: 0.006877858657389879\n",
      "[step: 7406] loss: 0.006905486807227135\n",
      "[step: 7407] loss: 0.0068759736604988575\n",
      "[step: 7408] loss: 0.0068009765818715096\n",
      "[step: 7409] loss: 0.0068115717731416225\n",
      "[step: 7410] loss: 0.006783215794712305\n",
      "[step: 7411] loss: 0.006732889451086521\n",
      "[step: 7412] loss: 0.006695720832794905\n",
      "[step: 7413] loss: 0.006718174088746309\n",
      "[step: 7414] loss: 0.00666556553915143\n",
      "[step: 7415] loss: 0.006660724990069866\n",
      "[step: 7416] loss: 0.006636402569711208\n",
      "[step: 7417] loss: 0.006675612647086382\n",
      "[step: 7418] loss: 0.006602569483220577\n",
      "[step: 7419] loss: 0.006653689779341221\n",
      "[step: 7420] loss: 0.006644908804446459\n",
      "[step: 7421] loss: 0.006719493772834539\n",
      "[step: 7422] loss: 0.00659998320043087\n",
      "[step: 7423] loss: 0.006624739617109299\n",
      "[step: 7424] loss: 0.006633816286921501\n",
      "[step: 7425] loss: 0.0066307298839092255\n",
      "[step: 7426] loss: 0.006580940913408995\n",
      "[step: 7427] loss: 0.0065799676813185215\n",
      "[step: 7428] loss: 0.00661711348220706\n",
      "[step: 7429] loss: 0.006614098325371742\n",
      "[step: 7430] loss: 0.006536723580211401\n",
      "[step: 7431] loss: 0.006586283911019564\n",
      "[step: 7432] loss: 0.00661810114979744\n",
      "[step: 7433] loss: 0.006655978038907051\n",
      "[step: 7434] loss: 0.00657278997823596\n",
      "[step: 7435] loss: 0.006588860414922237\n",
      "[step: 7436] loss: 0.006585795897990465\n",
      "[step: 7437] loss: 0.006629038602113724\n",
      "[step: 7438] loss: 0.006520754192024469\n",
      "[step: 7439] loss: 0.006529779173433781\n",
      "[step: 7440] loss: 0.006573619320988655\n",
      "[step: 7441] loss: 0.006599538028240204\n",
      "[step: 7442] loss: 0.006631575059145689\n",
      "[step: 7443] loss: 0.006927915383130312\n",
      "[step: 7444] loss: 0.006957645993679762\n",
      "[step: 7445] loss: 0.006789236329495907\n",
      "[step: 7446] loss: 0.00730262603610754\n",
      "[step: 7447] loss: 0.0072745028883218765\n",
      "[step: 7448] loss: 0.007093607913702726\n",
      "[step: 7449] loss: 0.0071308808401227\n",
      "[step: 7450] loss: 0.0070481570437550545\n",
      "[step: 7451] loss: 0.007012186571955681\n",
      "[step: 7452] loss: 0.007096962071955204\n",
      "[step: 7453] loss: 0.006945316679775715\n",
      "[step: 7454] loss: 0.007141841109842062\n",
      "[step: 7455] loss: 0.006992598995566368\n",
      "[step: 7456] loss: 0.006778329610824585\n",
      "[step: 7457] loss: 0.006875794380903244\n",
      "[step: 7458] loss: 0.006824658252298832\n",
      "[step: 7459] loss: 0.0067053912207484245\n",
      "[step: 7460] loss: 0.0067316326312720776\n",
      "[step: 7461] loss: 0.00675760954618454\n",
      "[step: 7462] loss: 0.0066048745065927505\n",
      "[step: 7463] loss: 0.006697923876345158\n",
      "[step: 7464] loss: 0.006783435586839914\n",
      "[step: 7465] loss: 0.006679653190076351\n",
      "[step: 7466] loss: 0.006641046609729528\n",
      "[step: 7467] loss: 0.006800778675824404\n",
      "[step: 7468] loss: 0.006755444221198559\n",
      "[step: 7469] loss: 0.00675601651892066\n",
      "[step: 7470] loss: 0.006569625809788704\n",
      "[step: 7471] loss: 0.006736611947417259\n",
      "[step: 7472] loss: 0.0065863667987287045\n",
      "[step: 7473] loss: 0.006579320877790451\n",
      "[step: 7474] loss: 0.006603599991649389\n",
      "[step: 7475] loss: 0.0066160657443106174\n",
      "[step: 7476] loss: 0.0065126544795930386\n",
      "[step: 7477] loss: 0.006581232417374849\n",
      "[step: 7478] loss: 0.006529469508677721\n",
      "[step: 7479] loss: 0.0065345135517418385\n",
      "[step: 7480] loss: 0.00651408638805151\n",
      "[step: 7481] loss: 0.006549902725964785\n",
      "[step: 7482] loss: 0.006483519449830055\n",
      "[step: 7483] loss: 0.006503348238766193\n",
      "[step: 7484] loss: 0.0064760115928947926\n",
      "[step: 7485] loss: 0.006515209563076496\n",
      "[step: 7486] loss: 0.006450030021369457\n",
      "[step: 7487] loss: 0.006484662648290396\n",
      "[step: 7488] loss: 0.006470281630754471\n",
      "[step: 7489] loss: 0.006498078349977732\n",
      "[step: 7490] loss: 0.006470030173659325\n",
      "[step: 7491] loss: 0.0064595951698720455\n",
      "[step: 7492] loss: 0.006462505552917719\n",
      "[step: 7493] loss: 0.006419997662305832\n",
      "[step: 7494] loss: 0.006433578208088875\n",
      "[step: 7495] loss: 0.00642802519723773\n",
      "[step: 7496] loss: 0.0064157904125750065\n",
      "[step: 7497] loss: 0.006439082324504852\n",
      "[step: 7498] loss: 0.006410184781998396\n",
      "[step: 7499] loss: 0.006431936752051115\n",
      "[step: 7500] loss: 0.00644887937232852\n",
      "[step: 7501] loss: 0.00649295374751091\n",
      "[step: 7502] loss: 0.0066841403022408485\n",
      "[step: 7503] loss: 0.006718394812196493\n",
      "[step: 7504] loss: 0.006496105808764696\n",
      "[step: 7505] loss: 0.006551705300807953\n",
      "[step: 7506] loss: 0.006832665763795376\n",
      "[step: 7507] loss: 0.0065985675901174545\n",
      "[step: 7508] loss: 0.006608498748391867\n",
      "[step: 7509] loss: 0.006589232478290796\n",
      "[step: 7510] loss: 0.007192583288997412\n",
      "[step: 7511] loss: 0.00714153703302145\n",
      "[step: 7512] loss: 0.006604147143661976\n",
      "[step: 7513] loss: 0.007431684993207455\n",
      "[step: 7514] loss: 0.007900656200945377\n",
      "[step: 7515] loss: 0.008752369321882725\n",
      "[step: 7516] loss: 0.008184260688722134\n",
      "[step: 7517] loss: 0.008566056378185749\n",
      "[step: 7518] loss: 0.0085453437641263\n",
      "[step: 7519] loss: 0.008561619557440281\n",
      "[step: 7520] loss: 0.008253783918917179\n",
      "[step: 7521] loss: 0.007900900207459927\n",
      "[step: 7522] loss: 0.008140196092426777\n",
      "[step: 7523] loss: 0.007778785191476345\n",
      "[step: 7524] loss: 0.007903345860540867\n",
      "[step: 7525] loss: 0.007687954232096672\n",
      "[step: 7526] loss: 0.007926259189844131\n",
      "[step: 7527] loss: 0.007624865975230932\n",
      "[step: 7528] loss: 0.007521077059209347\n",
      "[step: 7529] loss: 0.007538399659097195\n",
      "[step: 7530] loss: 0.007562173064798117\n",
      "[step: 7531] loss: 0.007640861440449953\n",
      "[step: 7532] loss: 0.007483854424208403\n",
      "[step: 7533] loss: 0.00738254189491272\n",
      "[step: 7534] loss: 0.007262697443366051\n",
      "[step: 7535] loss: 0.007290087174624205\n",
      "[step: 7536] loss: 0.0072560738772153854\n",
      "[step: 7537] loss: 0.007126269396394491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 7538] loss: 0.007043095771223307\n",
      "[step: 7539] loss: 0.007032849360257387\n",
      "[step: 7540] loss: 0.006952265277504921\n",
      "[step: 7541] loss: 0.006949718110263348\n",
      "[step: 7542] loss: 0.006889824755489826\n",
      "[step: 7543] loss: 0.006905973888933659\n",
      "[step: 7544] loss: 0.006868564989417791\n",
      "[step: 7545] loss: 0.006808662787079811\n",
      "[step: 7546] loss: 0.006833240855485201\n",
      "[step: 7547] loss: 0.006829213351011276\n",
      "[step: 7548] loss: 0.006766089238226414\n",
      "[step: 7549] loss: 0.006792780943214893\n",
      "[step: 7550] loss: 0.006767818704247475\n",
      "[step: 7551] loss: 0.006760556250810623\n",
      "[step: 7552] loss: 0.006734270136803389\n",
      "[step: 7553] loss: 0.006724548991769552\n",
      "[step: 7554] loss: 0.006726499181240797\n",
      "[step: 7555] loss: 0.006704655941575766\n",
      "[step: 7556] loss: 0.006690177135169506\n",
      "[step: 7557] loss: 0.00665657315403223\n",
      "[step: 7558] loss: 0.006642000749707222\n",
      "[step: 7559] loss: 0.006617648061364889\n",
      "[step: 7560] loss: 0.0066148582845926285\n",
      "[step: 7561] loss: 0.006604406051337719\n",
      "[step: 7562] loss: 0.006593822967261076\n",
      "[step: 7563] loss: 0.006580406799912453\n",
      "[step: 7564] loss: 0.006565155927091837\n",
      "[step: 7565] loss: 0.006555229891091585\n",
      "[step: 7566] loss: 0.006543359719216824\n",
      "[step: 7567] loss: 0.006535471882671118\n",
      "[step: 7568] loss: 0.00652581499889493\n",
      "[step: 7569] loss: 0.0065194666385650635\n",
      "[step: 7570] loss: 0.006512484047561884\n",
      "[step: 7571] loss: 0.006504533346742392\n",
      "[step: 7572] loss: 0.006499261129647493\n",
      "[step: 7573] loss: 0.006498204544186592\n",
      "[step: 7574] loss: 0.006502192467451096\n",
      "[step: 7575] loss: 0.0064981780014932156\n",
      "[step: 7576] loss: 0.00647994689643383\n",
      "[step: 7577] loss: 0.0064573949202895164\n",
      "[step: 7578] loss: 0.006439170800149441\n",
      "[step: 7579] loss: 0.00642704963684082\n",
      "[step: 7580] loss: 0.006418933626264334\n",
      "[step: 7581] loss: 0.006417454220354557\n",
      "[step: 7582] loss: 0.006426815874874592\n",
      "[step: 7583] loss: 0.006426461506634951\n",
      "[step: 7584] loss: 0.006414810661226511\n",
      "[step: 7585] loss: 0.00639768410474062\n",
      "[step: 7586] loss: 0.006380958948284388\n",
      "[step: 7587] loss: 0.00636313809081912\n",
      "[step: 7588] loss: 0.0063482848927378654\n",
      "[step: 7589] loss: 0.006339287385344505\n",
      "[step: 7590] loss: 0.006334619130939245\n",
      "[step: 7591] loss: 0.006332808174192905\n",
      "[step: 7592] loss: 0.006331818178296089\n",
      "[step: 7593] loss: 0.006330737378448248\n",
      "[step: 7594] loss: 0.0063349283300340176\n",
      "[step: 7595] loss: 0.0063495934009552\n",
      "[step: 7596] loss: 0.006408069282770157\n",
      "[step: 7597] loss: 0.00661621754989028\n",
      "[step: 7598] loss: 0.006976973731070757\n",
      "[step: 7599] loss: 0.007856588810682297\n",
      "[step: 7600] loss: 0.006786293815821409\n",
      "[step: 7601] loss: 0.007442789152264595\n",
      "[step: 7602] loss: 0.009325575083494186\n",
      "[step: 7603] loss: 0.011274777352809906\n",
      "[step: 7604] loss: 0.011487645097076893\n",
      "[step: 7605] loss: 0.01187007687985897\n",
      "[step: 7606] loss: 0.01073775626718998\n",
      "[step: 7607] loss: 0.010483496822416782\n",
      "[step: 7608] loss: 0.010625945404171944\n",
      "[step: 7609] loss: 0.009853157214820385\n",
      "[step: 7610] loss: 0.009754291735589504\n",
      "[step: 7611] loss: 0.009300059638917446\n",
      "[step: 7612] loss: 0.009111857041716576\n",
      "[step: 7613] loss: 0.009253703989088535\n",
      "[step: 7614] loss: 0.009244647808372974\n",
      "[step: 7615] loss: 0.008999329060316086\n",
      "[step: 7616] loss: 0.008982837200164795\n",
      "[step: 7617] loss: 0.009062547236680984\n",
      "[step: 7618] loss: 0.00887249130755663\n",
      "[step: 7619] loss: 0.008742173202335835\n",
      "[step: 7620] loss: 0.008702208288013935\n",
      "[step: 7621] loss: 0.008670906536281109\n",
      "[step: 7622] loss: 0.008570890873670578\n",
      "[step: 7623] loss: 0.008493189699947834\n",
      "[step: 7624] loss: 0.008541509509086609\n",
      "[step: 7625] loss: 0.008528155274689198\n",
      "[step: 7626] loss: 0.008450212888419628\n",
      "[step: 7627] loss: 0.008428292348980904\n",
      "[step: 7628] loss: 0.008458111435174942\n",
      "[step: 7629] loss: 0.008440880104899406\n",
      "[step: 7630] loss: 0.008429138921201229\n",
      "[step: 7631] loss: 0.008351173251867294\n",
      "[step: 7632] loss: 0.008335525169968605\n",
      "[step: 7633] loss: 0.008332878351211548\n",
      "[step: 7634] loss: 0.008263543248176575\n",
      "[step: 7635] loss: 0.008222763426601887\n",
      "[step: 7636] loss: 0.008237753063440323\n",
      "[step: 7637] loss: 0.008219284005463123\n",
      "[step: 7638] loss: 0.008199479430913925\n",
      "[step: 7639] loss: 0.008200702257454395\n",
      "[step: 7640] loss: 0.008168869651854038\n",
      "[step: 7641] loss: 0.008143539540469646\n",
      "[step: 7642] loss: 0.008144881576299667\n",
      "[step: 7643] loss: 0.008125724270939827\n",
      "[step: 7644] loss: 0.00808911956846714\n",
      "[step: 7645] loss: 0.008073810487985611\n",
      "[step: 7646] loss: 0.008059012703597546\n",
      "[step: 7647] loss: 0.00803772360086441\n",
      "[step: 7648] loss: 0.00803709402680397\n",
      "[step: 7649] loss: 0.008106973953545094\n",
      "[step: 7650] loss: 0.008477290160953999\n",
      "[step: 7651] loss: 0.01080651581287384\n",
      "[step: 7652] loss: 0.008772492408752441\n",
      "[step: 7653] loss: 0.008367043919861317\n",
      "[step: 7654] loss: 0.00878141913563013\n",
      "[step: 7655] loss: 0.008143667131662369\n",
      "[step: 7656] loss: 0.00850276742130518\n",
      "[step: 7657] loss: 0.008228089660406113\n",
      "[step: 7658] loss: 0.008173946291208267\n",
      "[step: 7659] loss: 0.008197696879506111\n",
      "[step: 7660] loss: 0.008157573640346527\n",
      "[step: 7661] loss: 0.00824650190770626\n",
      "[step: 7662] loss: 0.008005022071301937\n",
      "[step: 7663] loss: 0.008132398128509521\n",
      "[step: 7664] loss: 0.00800334569066763\n",
      "[step: 7665] loss: 0.008043501526117325\n",
      "[step: 7666] loss: 0.007929631508886814\n",
      "[step: 7667] loss: 0.00793355330824852\n",
      "[step: 7668] loss: 0.007948712445795536\n",
      "[step: 7669] loss: 0.00789499282836914\n",
      "[step: 7670] loss: 0.007853510789573193\n",
      "[step: 7671] loss: 0.007826240733265877\n",
      "[step: 7672] loss: 0.007823256775736809\n",
      "[step: 7673] loss: 0.007774250581860542\n",
      "[step: 7674] loss: 0.007728144992142916\n",
      "[step: 7675] loss: 0.007753169629722834\n",
      "[step: 7676] loss: 0.007721645291894674\n",
      "[step: 7677] loss: 0.007693891879171133\n",
      "[step: 7678] loss: 0.007671157363802195\n",
      "[step: 7679] loss: 0.007656577974557877\n",
      "[step: 7680] loss: 0.0076473066583275795\n",
      "[step: 7681] loss: 0.007622944191098213\n",
      "[step: 7682] loss: 0.007606578059494495\n",
      "[step: 7683] loss: 0.007593211252242327\n",
      "[step: 7684] loss: 0.007585031446069479\n",
      "[step: 7685] loss: 0.007549232337623835\n",
      "[step: 7686] loss: 0.007549341302365065\n",
      "[step: 7687] loss: 0.007535179611295462\n",
      "[step: 7688] loss: 0.007521337829530239\n",
      "[step: 7689] loss: 0.007507370784878731\n",
      "[step: 7690] loss: 0.007493560668081045\n",
      "[step: 7691] loss: 0.007482793182134628\n",
      "[step: 7692] loss: 0.00747276283800602\n",
      "[step: 7693] loss: 0.007460745982825756\n",
      "[step: 7694] loss: 0.007457025349140167\n",
      "[step: 7695] loss: 0.0074401237070560455\n",
      "[step: 7696] loss: 0.007430776488035917\n",
      "[step: 7697] loss: 0.00742708332836628\n",
      "[step: 7698] loss: 0.007412542589008808\n",
      "[step: 7699] loss: 0.007406141608953476\n",
      "[step: 7700] loss: 0.007398027461022139\n",
      "[step: 7701] loss: 0.007387560326606035\n",
      "[step: 7702] loss: 0.007382205221801996\n",
      "[step: 7703] loss: 0.00737446965649724\n",
      "[step: 7704] loss: 0.007367238402366638\n",
      "[step: 7705] loss: 0.007360342424362898\n",
      "[step: 7706] loss: 0.00735293747857213\n",
      "[step: 7707] loss: 0.007347113452851772\n",
      "[step: 7708] loss: 0.0073401592671871185\n",
      "[step: 7709] loss: 0.0073336525820195675\n",
      "[step: 7710] loss: 0.007327839266508818\n",
      "[step: 7711] loss: 0.007321092765778303\n",
      "[step: 7712] loss: 0.007315357681363821\n",
      "[step: 7713] loss: 0.0073095629923045635\n",
      "[step: 7714] loss: 0.007303260732442141\n",
      "[step: 7715] loss: 0.007297627627849579\n",
      "[step: 7716] loss: 0.007292143069207668\n",
      "[step: 7717] loss: 0.00728629669174552\n",
      "[step: 7718] loss: 0.007280639372766018\n",
      "[step: 7719] loss: 0.007275196257978678\n",
      "[step: 7720] loss: 0.0072695245034992695\n",
      "[step: 7721] loss: 0.007263908162713051\n",
      "[step: 7722] loss: 0.007258391473442316\n",
      "[step: 7723] loss: 0.007252724375575781\n",
      "[step: 7724] loss: 0.007247337140142918\n",
      "[step: 7725] loss: 0.0072417231276631355\n",
      "[step: 7726] loss: 0.007236073724925518\n",
      "[step: 7727] loss: 0.0072306739166378975\n",
      "[step: 7728] loss: 0.007224974222481251\n",
      "[step: 7729] loss: 0.007219434715807438\n",
      "[step: 7730] loss: 0.0072138262912631035\n",
      "[step: 7731] loss: 0.00720809493213892\n",
      "[step: 7732] loss: 0.007202606648206711\n",
      "[step: 7733] loss: 0.007196805439889431\n",
      "[step: 7734] loss: 0.00719103030860424\n",
      "[step: 7735] loss: 0.007185380440205336\n",
      "[step: 7736] loss: 0.007179529871791601\n",
      "[step: 7737] loss: 0.007173649035394192\n",
      "[step: 7738] loss: 0.00716768205165863\n",
      "[step: 7739] loss: 0.007161704357713461\n",
      "[step: 7740] loss: 0.0071556707844138145\n",
      "[step: 7741] loss: 0.007149488665163517\n",
      "[step: 7742] loss: 0.007143249735236168\n",
      "[step: 7743] loss: 0.007136915344744921\n",
      "[step: 7744] loss: 0.007130463141947985\n",
      "[step: 7745] loss: 0.007123933639377356\n",
      "[step: 7746] loss: 0.007117331959307194\n",
      "[step: 7747] loss: 0.007110574282705784\n",
      "[step: 7748] loss: 0.007103721145540476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 7749] loss: 0.007096732035279274\n",
      "[step: 7750] loss: 0.007089630700647831\n",
      "[step: 7751] loss: 0.007082462776452303\n",
      "[step: 7752] loss: 0.0070751989260315895\n",
      "[step: 7753] loss: 0.007068097125738859\n",
      "[step: 7754] loss: 0.007061436306685209\n",
      "[step: 7755] loss: 0.0070572104305028915\n",
      "[step: 7756] loss: 0.007056231144815683\n",
      "[step: 7757] loss: 0.00707927206531167\n",
      "[step: 7758] loss: 0.00708669563755393\n",
      "[step: 7759] loss: 0.007176011335104704\n",
      "[step: 7760] loss: 0.007104273419827223\n",
      "[step: 7761] loss: 0.007060654927045107\n",
      "[step: 7762] loss: 0.007030989043414593\n",
      "[step: 7763] loss: 0.007003143895417452\n",
      "[step: 7764] loss: 0.007037135772407055\n",
      "[step: 7765] loss: 0.007129047065973282\n",
      "[step: 7766] loss: 0.007039446849375963\n",
      "[step: 7767] loss: 0.006988666485995054\n",
      "[step: 7768] loss: 0.007029199507087469\n",
      "[step: 7769] loss: 0.006991324480623007\n",
      "[step: 7770] loss: 0.007109617814421654\n",
      "[step: 7771] loss: 0.007651018910109997\n",
      "[step: 7772] loss: 0.007211780641227961\n",
      "[step: 7773] loss: 0.007247135974466801\n",
      "[step: 7774] loss: 0.007754254154860973\n",
      "[step: 7775] loss: 0.0072299400344491005\n",
      "[step: 7776] loss: 0.006955541670322418\n",
      "[step: 7777] loss: 0.007062653079628944\n",
      "[step: 7778] loss: 0.007158032152801752\n",
      "[step: 7779] loss: 0.00711692962795496\n",
      "[step: 7780] loss: 0.006958574056625366\n",
      "[step: 7781] loss: 0.006955911871045828\n",
      "[step: 7782] loss: 0.0071838353760540485\n",
      "[step: 7783] loss: 0.00843523908406496\n",
      "[step: 7784] loss: 0.009985269978642464\n",
      "[step: 7785] loss: 0.013380831107497215\n",
      "[step: 7786] loss: 0.023093998432159424\n",
      "[step: 7787] loss: 0.03296920657157898\n",
      "[step: 7788] loss: 0.03193162381649017\n",
      "[step: 7789] loss: 0.049940694123506546\n",
      "[step: 7790] loss: 0.04965510591864586\n",
      "[step: 7791] loss: 0.04699418321251869\n",
      "[step: 7792] loss: 0.0536874495446682\n",
      "[step: 7793] loss: 0.03603923320770264\n",
      "[step: 7794] loss: 0.030292218551039696\n",
      "[step: 7795] loss: 0.025177810341119766\n",
      "[step: 7796] loss: 0.021542787551879883\n",
      "[step: 7797] loss: 0.01605505682528019\n",
      "[step: 7798] loss: 0.018922388553619385\n",
      "[step: 7799] loss: 0.021377798169851303\n",
      "[step: 7800] loss: 0.018084829673171043\n",
      "[step: 7801] loss: 0.016425147652626038\n",
      "[step: 7802] loss: 0.019299130886793137\n",
      "[step: 7803] loss: 0.01610708236694336\n",
      "[step: 7804] loss: 0.016701286658644676\n",
      "[step: 7805] loss: 0.015457069501280785\n",
      "[step: 7806] loss: 0.014551222324371338\n",
      "[step: 7807] loss: 0.015231955796480179\n",
      "[step: 7808] loss: 0.013703534379601479\n",
      "[step: 7809] loss: 0.013350305147469044\n",
      "[step: 7810] loss: 0.01313047856092453\n",
      "[step: 7811] loss: 0.012172466143965721\n",
      "[step: 7812] loss: 0.012462764047086239\n",
      "[step: 7813] loss: 0.012085641734302044\n",
      "[step: 7814] loss: 0.011836323887109756\n",
      "[step: 7815] loss: 0.011641263030469418\n",
      "[step: 7816] loss: 0.011457656510174274\n",
      "[step: 7817] loss: 0.011264403350651264\n",
      "[step: 7818] loss: 0.011232131160795689\n",
      "[step: 7819] loss: 0.011399620212614536\n",
      "[step: 7820] loss: 0.011379329487681389\n",
      "[step: 7821] loss: 0.011333353817462921\n",
      "[step: 7822] loss: 0.011175399646162987\n",
      "[step: 7823] loss: 0.01120779849588871\n",
      "[step: 7824] loss: 0.011470425873994827\n",
      "[step: 7825] loss: 0.011488672345876694\n",
      "[step: 7826] loss: 0.011585119180381298\n",
      "[step: 7827] loss: 0.01147462148219347\n",
      "[step: 7828] loss: 0.011418147012591362\n",
      "[step: 7829] loss: 0.011302064172923565\n",
      "[step: 7830] loss: 0.011276956647634506\n",
      "[step: 7831] loss: 0.011283811181783676\n",
      "[step: 7832] loss: 0.01121104322373867\n",
      "[step: 7833] loss: 0.011085368692874908\n",
      "[step: 7834] loss: 0.011207803152501583\n",
      "[step: 7835] loss: 0.01126156561076641\n",
      "[step: 7836] loss: 0.011328200809657574\n",
      "[step: 7837] loss: 0.011200298555195332\n",
      "[step: 7838] loss: 0.0114036425948143\n",
      "[step: 7839] loss: 0.011412856169044971\n",
      "[step: 7840] loss: 0.011382686905562878\n",
      "[step: 7841] loss: 0.01136296708136797\n",
      "[step: 7842] loss: 0.011363346129655838\n",
      "[step: 7843] loss: 0.011341407895088196\n",
      "[step: 7844] loss: 0.01104918122291565\n",
      "[step: 7845] loss: 0.01115681417286396\n",
      "[step: 7846] loss: 0.011277957819402218\n",
      "[step: 7847] loss: 0.01125473901629448\n",
      "[step: 7848] loss: 0.011240650899708271\n",
      "[step: 7849] loss: 0.01124547328799963\n",
      "[step: 7850] loss: 0.011245224624872208\n",
      "[step: 7851] loss: 0.01124025508761406\n",
      "[step: 7852] loss: 0.011101268231868744\n",
      "[step: 7853] loss: 0.011246508918702602\n",
      "[step: 7854] loss: 0.011193069629371166\n",
      "[step: 7855] loss: 0.011201048269867897\n",
      "[step: 7856] loss: 0.011236390098929405\n",
      "[step: 7857] loss: 0.011241551488637924\n",
      "[step: 7858] loss: 0.011277822777628899\n",
      "[step: 7859] loss: 0.011306515894830227\n",
      "[step: 7860] loss: 0.011320044286549091\n",
      "[step: 7861] loss: 0.011307588778436184\n",
      "[step: 7862] loss: 0.011424487456679344\n",
      "[step: 7863] loss: 0.011408213526010513\n",
      "[step: 7864] loss: 0.01138435397297144\n",
      "[step: 7865] loss: 0.0113534489646554\n",
      "[step: 7866] loss: 0.011335978284478188\n",
      "[step: 7867] loss: 0.011325376108288765\n",
      "[step: 7868] loss: 0.011335251852869987\n",
      "[step: 7869] loss: 0.011354287154972553\n",
      "[step: 7870] loss: 0.011376990005373955\n",
      "[step: 7871] loss: 0.011392527259886265\n",
      "[step: 7872] loss: 0.011396591551601887\n",
      "[step: 7873] loss: 0.011392035521566868\n",
      "[step: 7874] loss: 0.011378580704331398\n",
      "[step: 7875] loss: 0.011357778683304787\n",
      "[step: 7876] loss: 0.011337000876665115\n",
      "[step: 7877] loss: 0.011320323683321476\n",
      "[step: 7878] loss: 0.011307480745017529\n",
      "[step: 7879] loss: 0.011299972422420979\n",
      "[step: 7880] loss: 0.011297961696982384\n",
      "[step: 7881] loss: 0.011297444812953472\n",
      "[step: 7882] loss: 0.011295484378933907\n",
      "[step: 7883] loss: 0.01129145361483097\n",
      "[step: 7884] loss: 0.01128407008945942\n",
      "[step: 7885] loss: 0.01127301063388586\n",
      "[step: 7886] loss: 0.011260517872869968\n",
      "[step: 7887] loss: 0.011248426511883736\n",
      "[step: 7888] loss: 0.011237389408051968\n",
      "[step: 7889] loss: 0.011228524148464203\n",
      "[step: 7890] loss: 0.011222286149859428\n",
      "[step: 7891] loss: 0.011217505671083927\n",
      "[step: 7892] loss: 0.01121312752366066\n",
      "[step: 7893] loss: 0.011208721436560154\n",
      "[step: 7894] loss: 0.011203497648239136\n",
      "[step: 7895] loss: 0.011197015643119812\n",
      "[step: 7896] loss: 0.01118987612426281\n",
      "[step: 7897] loss: 0.011182709597051144\n",
      "[step: 7898] loss: 0.01117578987032175\n",
      "[step: 7899] loss: 0.011169621720910072\n",
      "[step: 7900] loss: 0.011164555326104164\n",
      "[step: 7901] loss: 0.011160297319293022\n",
      "[step: 7902] loss: 0.011156459338963032\n",
      "[step: 7903] loss: 0.011152812279760838\n",
      "[step: 7904] loss: 0.011149034835398197\n",
      "[step: 7905] loss: 0.01114488672465086\n",
      "[step: 7906] loss: 0.011140508577227592\n",
      "[step: 7907] loss: 0.011136121116578579\n",
      "[step: 7908] loss: 0.01113184355199337\n",
      "[step: 7909] loss: 0.01112786028534174\n",
      "[step: 7910] loss: 0.011124322190880775\n",
      "[step: 7911] loss: 0.011121143586933613\n",
      "[step: 7912] loss: 0.01111817266792059\n",
      "[step: 7913] loss: 0.011115319095551968\n",
      "[step: 7914] loss: 0.011112462729215622\n",
      "[step: 7915] loss: 0.011109498329460621\n",
      "[step: 7916] loss: 0.011106464080512524\n",
      "[step: 7917] loss: 0.011103442870080471\n",
      "[step: 7918] loss: 0.01110048871487379\n",
      "[step: 7919] loss: 0.011097664013504982\n",
      "[step: 7920] loss: 0.011095021851360798\n",
      "[step: 7921] loss: 0.011092534288764\n",
      "[step: 7922] loss: 0.011090145446360111\n",
      "[step: 7923] loss: 0.011087805964052677\n",
      "[step: 7924] loss: 0.011085478588938713\n",
      "[step: 7925] loss: 0.011083131656050682\n",
      "[step: 7926] loss: 0.011080773547291756\n",
      "[step: 7927] loss: 0.011078442446887493\n",
      "[step: 7928] loss: 0.011076157912611961\n",
      "[step: 7929] loss: 0.011073941364884377\n",
      "[step: 7930] loss: 0.011071803979575634\n",
      "[step: 7931] loss: 0.011069739237427711\n",
      "[step: 7932] loss: 0.01106771919876337\n",
      "[step: 7933] loss: 0.011065724305808544\n",
      "[step: 7934] loss: 0.01106374990195036\n",
      "[step: 7935] loss: 0.011061775498092175\n",
      "[step: 7936] loss: 0.011059815064072609\n",
      "[step: 7937] loss: 0.011057883501052856\n",
      "[step: 7938] loss: 0.011055986396968365\n",
      "[step: 7939] loss: 0.011054133996367455\n",
      "[step: 7940] loss: 0.011052316054701805\n",
      "[step: 7941] loss: 0.011050541885197163\n",
      "[step: 7942] loss: 0.011048788204789162\n",
      "[step: 7943] loss: 0.011047055013477802\n",
      "[step: 7944] loss: 0.011045336723327637\n",
      "[step: 7945] loss: 0.011043629609048367\n",
      "[step: 7946] loss: 0.011041944846510887\n",
      "[step: 7947] loss: 0.01104028057307005\n",
      "[step: 7948] loss: 0.011038641445338726\n",
      "[step: 7949] loss: 0.01103703398257494\n",
      "[step: 7950] loss: 0.01103544607758522\n",
      "[step: 7951] loss: 0.011033881455659866\n",
      "[step: 7952] loss: 0.01103233266621828\n",
      "[step: 7953] loss: 0.011030802503228188\n",
      "[step: 7954] loss: 0.01102928351610899\n",
      "[step: 7955] loss: 0.01102778036147356\n",
      "[step: 7956] loss: 0.011026290245354176\n",
      "[step: 7957] loss: 0.01102482434362173\n",
      "[step: 7958] loss: 0.011023377068340778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 7959] loss: 0.01102194469422102\n",
      "[step: 7960] loss: 0.01102052628993988\n",
      "[step: 7961] loss: 0.011019126512110233\n",
      "[step: 7962] loss: 0.011017734184861183\n",
      "[step: 7963] loss: 0.011016360484063625\n",
      "[step: 7964] loss: 0.011014997027814388\n",
      "[step: 7965] loss: 0.011013645678758621\n",
      "[step: 7966] loss: 0.011012307368218899\n",
      "[step: 7967] loss: 0.011010981164872646\n",
      "[step: 7968] loss: 0.011009671725332737\n",
      "[step: 7969] loss: 0.011008370667696\n",
      "[step: 7970] loss: 0.011007077991962433\n",
      "[step: 7971] loss: 0.011005801148712635\n",
      "[step: 7972] loss: 0.011004531756043434\n",
      "[step: 7973] loss: 0.011003275401890278\n",
      "[step: 7974] loss: 0.011002026498317719\n",
      "[step: 7975] loss: 0.011000788770616055\n",
      "[step: 7976] loss: 0.010999560356140137\n",
      "[step: 7977] loss: 0.010998338460922241\n",
      "[step: 7978] loss: 0.010997130535542965\n",
      "[step: 7979] loss: 0.010995927266776562\n",
      "[step: 7980] loss: 0.010994736105203629\n",
      "[step: 7981] loss: 0.010993553325533867\n",
      "[step: 7982] loss: 0.010992374271154404\n",
      "[step: 7983] loss: 0.010991201736032963\n",
      "[step: 7984] loss: 0.010990040376782417\n",
      "[step: 7985] loss: 0.010988886468112469\n",
      "[step: 7986] loss: 0.010987736284732819\n",
      "[step: 7987] loss: 0.010986599139869213\n",
      "[step: 7988] loss: 0.010985463857650757\n",
      "[step: 7989] loss: 0.010984338819980621\n",
      "[step: 7990] loss: 0.010983212850987911\n",
      "[step: 7991] loss: 0.010982096195220947\n",
      "[step: 7992] loss: 0.01098098885267973\n",
      "[step: 7993] loss: 0.010979884304106236\n",
      "[step: 7994] loss: 0.010978787206113338\n",
      "[step: 7995] loss: 0.010977691039443016\n",
      "[step: 7996] loss: 0.01097660418599844\n",
      "[step: 7997] loss: 0.01097552478313446\n",
      "[step: 7998] loss: 0.010974443517625332\n",
      "[step: 7999] loss: 0.010973368771374226\n",
      "[step: 8000] loss: 0.010972299613058567\n",
      "[step: 8001] loss: 0.010971233248710632\n",
      "[step: 8002] loss: 0.010970174334943295\n",
      "[step: 8003] loss: 0.010969116352498531\n",
      "[step: 8004] loss: 0.010968062095344067\n",
      "[step: 8005] loss: 0.010967012494802475\n",
      "[step: 8006] loss: 0.010965965688228607\n",
      "[step: 8007] loss: 0.010964923538267612\n",
      "[step: 8008] loss: 0.010963882319629192\n",
      "[step: 8009] loss: 0.010962842963635921\n",
      "[step: 8010] loss: 0.010961809195578098\n",
      "[step: 8011] loss: 0.01096077635884285\n",
      "[step: 8012] loss: 0.0109597472473979\n",
      "[step: 8013] loss: 0.010958717204630375\n",
      "[step: 8014] loss: 0.010957690887153149\n",
      "[step: 8015] loss: 0.010956671088933945\n",
      "[step: 8016] loss: 0.010955644771456718\n",
      "[step: 8017] loss: 0.010954624973237514\n",
      "[step: 8018] loss: 0.010953606106340885\n",
      "[step: 8019] loss: 0.010952585376799107\n",
      "[step: 8020] loss: 0.010951575823128223\n",
      "[step: 8021] loss: 0.01095055602490902\n",
      "[step: 8022] loss: 0.01094953902065754\n",
      "[step: 8023] loss: 0.010948523879051208\n",
      "[step: 8024] loss: 0.010947509668767452\n",
      "[step: 8025] loss: 0.010946491733193398\n",
      "[step: 8026] loss: 0.010945476591587067\n",
      "[step: 8027] loss: 0.010944465175271034\n",
      "[step: 8028] loss: 0.010943448171019554\n",
      "[step: 8029] loss: 0.010942432098090649\n",
      "[step: 8030] loss: 0.01094141323119402\n",
      "[step: 8031] loss: 0.01094039436429739\n",
      "[step: 8032] loss: 0.010939376428723335\n",
      "[step: 8033] loss: 0.010938353836536407\n",
      "[step: 8034] loss: 0.01093733124434948\n",
      "[step: 8035] loss: 0.010936308652162552\n",
      "[step: 8036] loss: 0.010935281403362751\n",
      "[step: 8037] loss: 0.010934253223240376\n",
      "[step: 8038] loss: 0.010933222249150276\n",
      "[step: 8039] loss: 0.010932187549769878\n",
      "[step: 8040] loss: 0.010931151919066906\n",
      "[step: 8041] loss: 0.010930114425718784\n",
      "[step: 8042] loss: 0.010929068550467491\n",
      "[step: 8043] loss: 0.010928021743893623\n",
      "[step: 8044] loss: 0.010926976799964905\n",
      "[step: 8045] loss: 0.010925925336778164\n",
      "[step: 8046] loss: 0.010924864560365677\n",
      "[step: 8047] loss: 0.010923804715275764\n",
      "[step: 8048] loss: 0.010922739282250404\n",
      "[step: 8049] loss: 0.010921668261289597\n",
      "[step: 8050] loss: 0.01092059537768364\n",
      "[step: 8051] loss: 0.010919513180851936\n",
      "[step: 8052] loss: 0.010918429121375084\n",
      "[step: 8053] loss: 0.010917339473962784\n",
      "[step: 8054] loss: 0.01091624516993761\n",
      "[step: 8055] loss: 0.010915144346654415\n",
      "[step: 8056] loss: 0.010914035141468048\n",
      "[step: 8057] loss: 0.010912921279668808\n",
      "[step: 8058] loss: 0.010911802761256695\n",
      "[step: 8059] loss: 0.010910676792263985\n",
      "[step: 8060] loss: 0.010909545235335827\n",
      "[step: 8061] loss: 0.010908406227827072\n",
      "[step: 8062] loss: 0.010907260701060295\n",
      "[step: 8063] loss: 0.010906107723712921\n",
      "[step: 8064] loss: 0.0109049491584301\n",
      "[step: 8065] loss: 0.010903780348598957\n",
      "[step: 8066] loss: 0.01090260874480009\n",
      "[step: 8067] loss: 0.010901427827775478\n",
      "[step: 8068] loss: 0.010900242254137993\n",
      "[step: 8069] loss: 0.010899043641984463\n",
      "[step: 8070] loss: 0.010897841304540634\n",
      "[step: 8071] loss: 0.010896634310483932\n",
      "[step: 8072] loss: 0.010895421728491783\n",
      "[step: 8073] loss: 0.010894197039306164\n",
      "[step: 8074] loss: 0.010892967693507671\n",
      "[step: 8075] loss: 0.010891726240515709\n",
      "[step: 8076] loss: 0.01089048758149147\n",
      "[step: 8077] loss: 0.010889235883951187\n",
      "[step: 8078] loss: 0.010887982323765755\n",
      "[step: 8079] loss: 0.010886716656386852\n",
      "[step: 8080] loss: 0.010885451920330524\n",
      "[step: 8081] loss: 0.010884182527661324\n",
      "[step: 8082] loss: 0.010882904753088951\n",
      "[step: 8083] loss: 0.010881626047194004\n",
      "[step: 8084] loss: 0.010880340822041035\n",
      "[step: 8085] loss: 0.010879053734242916\n",
      "[step: 8086] loss: 0.010877765715122223\n",
      "[step: 8087] loss: 0.010876472108066082\n",
      "[step: 8088] loss: 0.010875175707042217\n",
      "[step: 8089] loss: 0.010873883962631226\n",
      "[step: 8090] loss: 0.010872594080865383\n",
      "[step: 8091] loss: 0.010871298611164093\n",
      "[step: 8092] loss: 0.010870005004107952\n",
      "[step: 8093] loss: 0.01086871512234211\n",
      "[step: 8094] loss: 0.010867427103221416\n",
      "[step: 8095] loss: 0.010866142809391022\n",
      "[step: 8096] loss: 0.010864867828786373\n",
      "[step: 8097] loss: 0.010863592848181725\n",
      "[step: 8098] loss: 0.010862323455512524\n",
      "[step: 8099] loss: 0.010861064307391644\n",
      "[step: 8100] loss: 0.010859807021915913\n",
      "[step: 8101] loss: 0.010858563706278801\n",
      "[step: 8102] loss: 0.010857324115931988\n",
      "[step: 8103] loss: 0.01085609756410122\n",
      "[step: 8104] loss: 0.010854878462851048\n",
      "[step: 8105] loss: 0.010853673331439495\n",
      "[step: 8106] loss: 0.010852471925318241\n",
      "[step: 8107] loss: 0.010851288214325905\n",
      "[step: 8108] loss: 0.010850110091269016\n",
      "[step: 8109] loss: 0.010848945006728172\n",
      "[step: 8110] loss: 0.010847791098058224\n",
      "[step: 8111] loss: 0.010846644639968872\n",
      "[step: 8112] loss: 0.010845513083040714\n",
      "[step: 8113] loss: 0.010844387114048004\n",
      "[step: 8114] loss: 0.010843274183571339\n",
      "[step: 8115] loss: 0.010842171497642994\n",
      "[step: 8116] loss: 0.010841076262295246\n",
      "[step: 8117] loss: 0.010839988477528095\n",
      "[step: 8118] loss: 0.010838910937309265\n",
      "[step: 8119] loss: 0.010837836191058159\n",
      "[step: 8120] loss: 0.010836774483323097\n",
      "[step: 8121] loss: 0.010835712775588036\n",
      "[step: 8122] loss: 0.010834654793143272\n",
      "[step: 8123] loss: 0.010833603329956532\n",
      "[step: 8124] loss: 0.010832556523382664\n",
      "[step: 8125] loss: 0.010831507854163647\n",
      "[step: 8126] loss: 0.010830465704202652\n",
      "[step: 8127] loss: 0.010829421691596508\n",
      "[step: 8128] loss: 0.010828379541635513\n",
      "[step: 8129] loss: 0.01082733552902937\n",
      "[step: 8130] loss: 0.010826293379068375\n",
      "[step: 8131] loss: 0.010825244709849358\n",
      "[step: 8132] loss: 0.010824199765920639\n",
      "[step: 8133] loss: 0.010823148302733898\n",
      "[step: 8134] loss: 0.010822098702192307\n",
      "[step: 8135] loss: 0.010821039788424969\n",
      "[step: 8136] loss: 0.010819981805980206\n",
      "[step: 8137] loss: 0.010818918235599995\n",
      "[step: 8138] loss: 0.01081785000860691\n",
      "[step: 8139] loss: 0.010816779918968678\n",
      "[step: 8140] loss: 0.010815710760653019\n",
      "[step: 8141] loss: 0.01081462949514389\n",
      "[step: 8142] loss: 0.010813544504344463\n",
      "[step: 8143] loss: 0.010812457650899887\n",
      "[step: 8144] loss: 0.010811361484229565\n",
      "[step: 8145] loss: 0.010810267180204391\n",
      "[step: 8146] loss: 0.010809161700308323\n",
      "[step: 8147] loss: 0.010808057151734829\n",
      "[step: 8148] loss: 0.010806944221258163\n",
      "[step: 8149] loss: 0.010805828496813774\n",
      "[step: 8150] loss: 0.010804703459143639\n",
      "[step: 8151] loss: 0.010803578421473503\n",
      "[step: 8152] loss: 0.01080244779586792\n",
      "[step: 8153] loss: 0.01080130971968174\n",
      "[step: 8154] loss: 0.010800166055560112\n",
      "[step: 8155] loss: 0.010799020528793335\n",
      "[step: 8156] loss: 0.010797874070703983\n",
      "[step: 8157] loss: 0.010796715505421162\n",
      "[step: 8158] loss: 0.010795554146170616\n",
      "[step: 8159] loss: 0.010794386267662048\n",
      "[step: 8160] loss: 0.010793213732540607\n",
      "[step: 8161] loss: 0.010792038403451443\n",
      "[step: 8162] loss: 0.010790856555104256\n",
      "[step: 8163] loss: 0.010789669118821621\n",
      "[step: 8164] loss: 0.010788476094603539\n",
      "[step: 8165] loss: 0.010787280276417732\n",
      "[step: 8166] loss: 0.01078607328236103\n",
      "[step: 8167] loss: 0.01078486256301403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 8168] loss: 0.010783650912344456\n",
      "[step: 8169] loss: 0.01078242901712656\n",
      "[step: 8170] loss: 0.010781202465295792\n",
      "[step: 8171] loss: 0.010779970325529575\n",
      "[step: 8172] loss: 0.01077873446047306\n",
      "[step: 8173] loss: 0.010777491144835949\n",
      "[step: 8174] loss: 0.01077624037861824\n",
      "[step: 8175] loss: 0.010774986818432808\n",
      "[step: 8176] loss: 0.010773724876344204\n",
      "[step: 8177] loss: 0.010772458277642727\n",
      "[step: 8178] loss: 0.010771186091005802\n",
      "[step: 8179] loss: 0.010769909247756004\n",
      "[step: 8180] loss: 0.01076862309128046\n",
      "[step: 8181] loss: 0.010767334140837193\n",
      "[step: 8182] loss: 0.010766037739813328\n",
      "[step: 8183] loss: 0.01076473668217659\n",
      "[step: 8184] loss: 0.010763428173959255\n",
      "[step: 8185] loss: 0.010762112215161324\n",
      "[step: 8186] loss: 0.010760796256363392\n",
      "[step: 8187] loss: 0.010759465396404266\n",
      "[step: 8188] loss: 0.010758132673799992\n",
      "[step: 8189] loss: 0.010756796225905418\n",
      "[step: 8190] loss: 0.010755449533462524\n",
      "[step: 8191] loss: 0.010754100978374481\n",
      "[step: 8192] loss: 0.010752741247415543\n",
      "[step: 8193] loss: 0.010751377791166306\n",
      "[step: 8194] loss: 0.010750006884336472\n",
      "[step: 8195] loss: 0.010748631320893764\n",
      "[step: 8196] loss: 0.010747249238193035\n",
      "[step: 8197] loss: 0.01074585784226656\n",
      "[step: 8198] loss: 0.01074446365237236\n",
      "[step: 8199] loss: 0.010743055492639542\n",
      "[step: 8200] loss: 0.010741649195551872\n",
      "[step: 8201] loss: 0.010740230791270733\n",
      "[step: 8202] loss: 0.010738808661699295\n",
      "[step: 8203] loss: 0.010737374424934387\n",
      "[step: 8204] loss: 0.01073593832552433\n",
      "[step: 8205] loss: 0.010734494775533676\n",
      "[step: 8206] loss: 0.010733040049672127\n",
      "[step: 8207] loss: 0.010731582529842854\n",
      "[step: 8208] loss: 0.010730115696787834\n",
      "[step: 8209] loss: 0.010728645138442516\n",
      "[step: 8210] loss: 0.010727161541581154\n",
      "[step: 8211] loss: 0.010725673288106918\n",
      "[step: 8212] loss: 0.010724175721406937\n",
      "[step: 8213] loss: 0.010722674429416656\n",
      "[step: 8214] loss: 0.010721161961555481\n",
      "[step: 8215] loss: 0.010719641111791134\n",
      "[step: 8216] loss: 0.01071811094880104\n",
      "[step: 8217] loss: 0.010716577991843224\n",
      "[step: 8218] loss: 0.010715031065046787\n",
      "[step: 8219] loss: 0.010713480412960052\n",
      "[step: 8220] loss: 0.010711918585002422\n",
      "[step: 8221] loss: 0.010710351169109344\n",
      "[step: 8222] loss: 0.010708771646022797\n",
      "[step: 8223] loss: 0.010707183741033077\n",
      "[step: 8224] loss: 0.010705591179430485\n",
      "[step: 8225] loss: 0.010703988373279572\n",
      "[step: 8226] loss: 0.010702378116548061\n",
      "[step: 8227] loss: 0.010700756683945656\n",
      "[step: 8228] loss: 0.01069912500679493\n",
      "[step: 8229] loss: 0.010697485879063606\n",
      "[step: 8230] loss: 0.010695836506783962\n",
      "[step: 8231] loss: 0.01069418154656887\n",
      "[step: 8232] loss: 0.01069251261651516\n",
      "[step: 8233] loss: 0.010690840892493725\n",
      "[step: 8234] loss: 0.010689156129956245\n",
      "[step: 8235] loss: 0.010687461122870445\n",
      "[step: 8236] loss: 0.010685761459171772\n",
      "[step: 8237] loss: 0.01068404782563448\n",
      "[step: 8238] loss: 0.010682325810194016\n",
      "[step: 8239] loss: 0.010680598206818104\n",
      "[step: 8240] loss: 0.010678858496248722\n",
      "[step: 8241] loss: 0.010677110403776169\n",
      "[step: 8242] loss: 0.010675353929400444\n",
      "[step: 8243] loss: 0.010673590935766697\n",
      "[step: 8244] loss: 0.010671812109649181\n",
      "[step: 8245] loss: 0.010670033283531666\n",
      "[step: 8246] loss: 0.010668240487575531\n",
      "[step: 8247] loss: 0.01066643837839365\n",
      "[step: 8248] loss: 0.010664627887308598\n",
      "[step: 8249] loss: 0.010662805289030075\n",
      "[step: 8250] loss: 0.010660980828106403\n",
      "[step: 8251] loss: 0.01065914612263441\n",
      "[step: 8252] loss: 0.010657301172614098\n",
      "[step: 8253] loss: 0.010655451565980911\n",
      "[step: 8254] loss: 0.010653592646121979\n",
      "[step: 8255] loss: 0.010651723481714725\n",
      "[step: 8256] loss: 0.010649850592017174\n",
      "[step: 8257] loss: 0.01064796932041645\n",
      "[step: 8258] loss: 0.010646081529557705\n",
      "[step: 8259] loss: 0.010644183494150639\n",
      "[step: 8260] loss: 0.010642279870808125\n",
      "[step: 8261] loss: 0.010640370659530163\n",
      "[step: 8262] loss: 0.010638454928994179\n",
      "[step: 8263] loss: 0.010636529885232449\n",
      "[step: 8264] loss: 0.01063459925353527\n",
      "[step: 8265] loss: 0.010632665827870369\n",
      "[step: 8266] loss: 0.01063072495162487\n",
      "[step: 8267] loss: 0.010628780350089073\n",
      "[step: 8268] loss: 0.010626832954585552\n",
      "[step: 8269] loss: 0.010624873451888561\n",
      "[step: 8270] loss: 0.010622912086546421\n",
      "[step: 8271] loss: 0.010620946995913982\n",
      "[step: 8272] loss: 0.010618980042636395\n",
      "[step: 8273] loss: 0.010617010295391083\n",
      "[step: 8274] loss: 0.010615037754178047\n",
      "[step: 8275] loss: 0.01061305683106184\n",
      "[step: 8276] loss: 0.010611075907945633\n",
      "[step: 8277] loss: 0.010609092190861702\n",
      "[step: 8278] loss: 0.010607103817164898\n",
      "[step: 8279] loss: 0.010605117306113243\n",
      "[step: 8280] loss: 0.010603128932416439\n",
      "[step: 8281] loss: 0.010601136833429337\n",
      "[step: 8282] loss: 0.010599149391055107\n",
      "[step: 8283] loss: 0.010597152635455132\n",
      "[step: 8284] loss: 0.010595162399113178\n",
      "[step: 8285] loss: 0.010593168437480927\n",
      "[step: 8286] loss: 0.010591178201138973\n",
      "[step: 8287] loss: 0.01058918610215187\n",
      "[step: 8288] loss: 0.010587198659777641\n",
      "[step: 8289] loss: 0.010585211217403412\n",
      "[step: 8290] loss: 0.010583221912384033\n",
      "[step: 8291] loss: 0.010581239126622677\n",
      "[step: 8292] loss: 0.010579257272183895\n",
      "[step: 8293] loss: 0.010577278211712837\n",
      "[step: 8294] loss: 0.010575302876532078\n",
      "[step: 8295] loss: 0.010573330335319042\n",
      "[step: 8296] loss: 0.01057136245071888\n",
      "[step: 8297] loss: 0.010569390840828419\n",
      "[step: 8298] loss: 0.010567432269454002\n",
      "[step: 8299] loss: 0.01056547649204731\n",
      "[step: 8300] loss: 0.010563524439930916\n",
      "[step: 8301] loss: 0.010561577044427395\n",
      "[step: 8302] loss: 0.010559630580246449\n",
      "[step: 8303] loss: 0.010557694360613823\n",
      "[step: 8304] loss: 0.010555766522884369\n",
      "[step: 8305] loss: 0.01055383961647749\n",
      "[step: 8306] loss: 0.010551922023296356\n",
      "[step: 8307] loss: 0.010550007224082947\n",
      "[step: 8308] loss: 0.010548100806772709\n",
      "[step: 8309] loss: 0.01054619811475277\n",
      "[step: 8310] loss: 0.010544304735958576\n",
      "[step: 8311] loss: 0.01054241880774498\n",
      "[step: 8312] loss: 0.01054054033011198\n",
      "[step: 8313] loss: 0.010538668371737003\n",
      "[step: 8314] loss: 0.010536803863942623\n",
      "[step: 8315] loss: 0.01053494494408369\n",
      "[step: 8316] loss: 0.010533090680837631\n",
      "[step: 8317] loss: 0.010531247593462467\n",
      "[step: 8318] loss: 0.010529408231377602\n",
      "[step: 8319] loss: 0.010527582839131355\n",
      "[step: 8320] loss: 0.010525760240852833\n",
      "[step: 8321] loss: 0.010523946024477482\n",
      "[step: 8322] loss: 0.010522139258682728\n",
      "[step: 8323] loss: 0.010520339012145996\n",
      "[step: 8324] loss: 0.010518542490899563\n",
      "[step: 8325] loss: 0.010516759939491749\n",
      "[step: 8326] loss: 0.01051497831940651\n",
      "[step: 8327] loss: 0.010513212531805038\n",
      "[step: 8328] loss: 0.010511443950235844\n",
      "[step: 8329] loss: 0.01050968375056982\n",
      "[step: 8330] loss: 0.010507934726774693\n",
      "[step: 8331] loss: 0.010506188496947289\n",
      "[step: 8332] loss: 0.010504450649023056\n",
      "[step: 8333] loss: 0.01050272025167942\n",
      "[step: 8334] loss: 0.010500991716980934\n",
      "[step: 8335] loss: 0.010499273426830769\n",
      "[step: 8336] loss: 0.010497557930648327\n",
      "[step: 8337] loss: 0.010495847091078758\n",
      "[step: 8338] loss: 0.010494143702089787\n",
      "[step: 8339] loss: 0.01049244124442339\n",
      "[step: 8340] loss: 0.010490749962627888\n",
      "[step: 8341] loss: 0.010489062406122684\n",
      "[step: 8342] loss: 0.010487381368875504\n",
      "[step: 8343] loss: 0.010485698468983173\n",
      "[step: 8344] loss: 0.010484025813639164\n",
      "[step: 8345] loss: 0.010482355952262878\n",
      "[step: 8346] loss: 0.010480686090886593\n",
      "[step: 8347] loss: 0.010479025542736053\n",
      "[step: 8348] loss: 0.01047736406326294\n",
      "[step: 8349] loss: 0.010475708171725273\n",
      "[step: 8350] loss: 0.010474053211510181\n",
      "[step: 8351] loss: 0.010472402907907963\n",
      "[step: 8352] loss: 0.01047075167298317\n",
      "[step: 8353] loss: 0.010469107888638973\n",
      "[step: 8354] loss: 0.010467465966939926\n",
      "[step: 8355] loss: 0.010465825907886028\n",
      "[step: 8356] loss: 0.01046418771147728\n",
      "[step: 8357] loss: 0.010462546721100807\n",
      "[step: 8358] loss: 0.010460911318659782\n",
      "[step: 8359] loss: 0.010459279641509056\n",
      "[step: 8360] loss: 0.010457645170390606\n",
      "[step: 8361] loss: 0.010456014424562454\n",
      "[step: 8362] loss: 0.010454386472702026\n",
      "[step: 8363] loss: 0.010452751070261002\n",
      "[step: 8364] loss: 0.010451124049723148\n",
      "[step: 8365] loss: 0.010449495166540146\n",
      "[step: 8366] loss: 0.010447868146002293\n",
      "[step: 8367] loss: 0.010446243919432163\n",
      "[step: 8368] loss: 0.010444610379636288\n",
      "[step: 8369] loss: 0.010442984290421009\n",
      "[step: 8370] loss: 0.010441356338560581\n",
      "[step: 8371] loss: 0.010439729318022728\n",
      "[step: 8372] loss: 0.010438098572194576\n",
      "[step: 8373] loss: 0.010436469689011574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 8374] loss: 0.010434839874505997\n",
      "[step: 8375] loss: 0.010433207266032696\n",
      "[step: 8376] loss: 0.010431578382849693\n",
      "[step: 8377] loss: 0.010429943911731243\n",
      "[step: 8378] loss: 0.010428309440612793\n",
      "[step: 8379] loss: 0.010426674038171768\n",
      "[step: 8380] loss: 0.01042503584176302\n",
      "[step: 8381] loss: 0.010423397645354271\n",
      "[step: 8382] loss: 0.010421755723655224\n",
      "[step: 8383] loss: 0.010420115664601326\n",
      "[step: 8384] loss: 0.01041847188025713\n",
      "[step: 8385] loss: 0.010416829027235508\n",
      "[step: 8386] loss: 0.010415179654955864\n",
      "[step: 8387] loss: 0.010413531213998795\n",
      "[step: 8388] loss: 0.010411878116428852\n",
      "[step: 8389] loss: 0.010410227812826633\n",
      "[step: 8390] loss: 0.010408570989966393\n",
      "[step: 8391] loss: 0.010406912304461002\n",
      "[step: 8392] loss: 0.010405255481600761\n",
      "[step: 8393] loss: 0.010403593070805073\n",
      "[step: 8394] loss: 0.01040192972868681\n",
      "[step: 8395] loss: 0.010400262661278248\n",
      "[step: 8396] loss: 0.010398589074611664\n",
      "[step: 8397] loss: 0.010396920144557953\n",
      "[step: 8398] loss: 0.010395245626568794\n",
      "[step: 8399] loss: 0.010393567383289337\n",
      "[step: 8400] loss: 0.010391890071332455\n",
      "[step: 8401] loss: 0.010390202514827251\n",
      "[step: 8402] loss: 0.01038852147758007\n",
      "[step: 8403] loss: 0.010386829264461994\n",
      "[step: 8404] loss: 0.010385141707956791\n",
      "[step: 8405] loss: 0.01038344856351614\n",
      "[step: 8406] loss: 0.010381747037172318\n",
      "[step: 8407] loss: 0.010380050167441368\n",
      "[step: 8408] loss: 0.010378346778452396\n",
      "[step: 8409] loss: 0.010376637801527977\n",
      "[step: 8410] loss: 0.010374934412539005\n",
      "[step: 8411] loss: 0.010373220779001713\n",
      "[step: 8412] loss: 0.01037150714546442\n",
      "[step: 8413] loss: 0.010369791649281979\n",
      "[step: 8414] loss: 0.01036807056516409\n",
      "[step: 8415] loss: 0.010366345755755901\n",
      "[step: 8416] loss: 0.010364620946347713\n",
      "[step: 8417] loss: 0.010362892411649227\n",
      "[step: 8418] loss: 0.010361159220337868\n",
      "[step: 8419] loss: 0.010359426029026508\n",
      "[step: 8420] loss: 0.01035768911242485\n",
      "[step: 8421] loss: 0.010355950333178043\n",
      "[step: 8422] loss: 0.01035420410335064\n",
      "[step: 8423] loss: 0.010352459736168385\n",
      "[step: 8424] loss: 0.010350707918405533\n",
      "[step: 8425] loss: 0.01034895796328783\n",
      "[step: 8426] loss: 0.010347206145524979\n",
      "[step: 8427] loss: 0.010345454327762127\n",
      "[step: 8428] loss: 0.01034368947148323\n",
      "[step: 8429] loss: 0.010341930203139782\n",
      "[step: 8430] loss: 0.01034016627818346\n",
      "[step: 8431] loss: 0.010338399559259415\n",
      "[step: 8432] loss: 0.01033662911504507\n",
      "[step: 8433] loss: 0.010334860533475876\n",
      "[step: 8434] loss: 0.010333088226616383\n",
      "[step: 8435] loss: 0.010331313125789165\n",
      "[step: 8436] loss: 0.010329531505703926\n",
      "[step: 8437] loss: 0.01032775454223156\n",
      "[step: 8438] loss: 0.010325971990823746\n",
      "[step: 8439] loss: 0.010324185714125633\n",
      "[step: 8440] loss: 0.010322397574782372\n",
      "[step: 8441] loss: 0.010320611298084259\n",
      "[step: 8442] loss: 0.010318819433450699\n",
      "[step: 8443] loss: 0.010317032225430012\n",
      "[step: 8444] loss: 0.010315231047570705\n",
      "[step: 8445] loss: 0.010313436388969421\n",
      "[step: 8446] loss: 0.010311638005077839\n",
      "[step: 8447] loss: 0.010309838689863682\n",
      "[step: 8448] loss: 0.010308035649359226\n",
      "[step: 8449] loss: 0.010306237265467644\n",
      "[step: 8450] loss: 0.01030442863702774\n",
      "[step: 8451] loss: 0.010302623733878136\n",
      "[step: 8452] loss: 0.010300814174115658\n",
      "[step: 8453] loss: 0.010299009270966053\n",
      "[step: 8454] loss: 0.010297195054590702\n",
      "[step: 8455] loss: 0.010295380838215351\n",
      "[step: 8456] loss: 0.010293562896549702\n",
      "[step: 8457] loss: 0.01029174868017435\n",
      "[step: 8458] loss: 0.010289928875863552\n",
      "[step: 8459] loss: 0.010288112796843052\n",
      "[step: 8460] loss: 0.01028628833591938\n",
      "[step: 8461] loss: 0.010284469462931156\n",
      "[step: 8462] loss: 0.01028264407068491\n",
      "[step: 8463] loss: 0.010280819609761238\n",
      "[step: 8464] loss: 0.010278991423547268\n",
      "[step: 8465] loss: 0.010277154855430126\n",
      "[step: 8466] loss: 0.010275324806571007\n",
      "[step: 8467] loss: 0.010273493826389313\n",
      "[step: 8468] loss: 0.010271655395627022\n",
      "[step: 8469] loss: 0.010269821621477604\n",
      "[step: 8470] loss: 0.010267980396747589\n",
      "[step: 8471] loss: 0.010266139172017574\n",
      "[step: 8472] loss: 0.010264294221997261\n",
      "[step: 8473] loss: 0.0102624436840415\n",
      "[step: 8474] loss: 0.010260594077408314\n",
      "[step: 8475] loss: 0.010258742608129978\n",
      "[step: 8476] loss: 0.01025688461959362\n",
      "[step: 8477] loss: 0.010255028493702412\n",
      "[step: 8478] loss: 0.010253164917230606\n",
      "[step: 8479] loss: 0.010251297615468502\n",
      "[step: 8480] loss: 0.010249430313706398\n",
      "[step: 8481] loss: 0.010247557424008846\n",
      "[step: 8482] loss: 0.010245679877698421\n",
      "[step: 8483] loss: 0.010243793949484825\n",
      "[step: 8484] loss: 0.01024190615862608\n",
      "[step: 8485] loss: 0.010240016505122185\n",
      "[step: 8486] loss: 0.010238118469715118\n",
      "[step: 8487] loss: 0.010236214846372604\n",
      "[step: 8488] loss: 0.010234306566417217\n",
      "[step: 8489] loss: 0.010232391767203808\n",
      "[step: 8490] loss: 0.01023047138005495\n",
      "[step: 8491] loss: 0.010228539817035198\n",
      "[step: 8492] loss: 0.010226606391370296\n",
      "[step: 8493] loss: 0.010224658995866776\n",
      "[step: 8494] loss: 0.010222706012427807\n",
      "[step: 8495] loss: 0.010220745578408241\n",
      "[step: 8496] loss: 0.010218774899840355\n",
      "[step: 8497] loss: 0.010216793976724148\n",
      "[step: 8498] loss: 0.010214805603027344\n",
      "[step: 8499] loss: 0.01021280512213707\n",
      "[step: 8500] loss: 0.010210791602730751\n",
      "[step: 8501] loss: 0.010208765976130962\n",
      "[step: 8502] loss: 0.010206727311015129\n",
      "[step: 8503] loss: 0.0102046774700284\n",
      "[step: 8504] loss: 0.010202614590525627\n",
      "[step: 8505] loss: 0.010200533084571362\n",
      "[step: 8506] loss: 0.01019844226539135\n",
      "[step: 8507] loss: 0.010196330025792122\n",
      "[step: 8508] loss: 0.010194206610321999\n",
      "[step: 8509] loss: 0.010192063637077808\n",
      "[step: 8510] loss: 0.010189899243414402\n",
      "[step: 8511] loss: 0.010187720879912376\n",
      "[step: 8512] loss: 0.01018552202731371\n",
      "[step: 8513] loss: 0.010183298960328102\n",
      "[step: 8514] loss: 0.010181053541600704\n",
      "[step: 8515] loss: 0.010178790427744389\n",
      "[step: 8516] loss: 0.010176497511565685\n",
      "[step: 8517] loss: 0.01017418596893549\n",
      "[step: 8518] loss: 0.010171846486628056\n",
      "[step: 8519] loss: 0.010169475339353085\n",
      "[step: 8520] loss: 0.010167082771658897\n",
      "[step: 8521] loss: 0.010164660401642323\n",
      "[step: 8522] loss: 0.010162205435335636\n",
      "[step: 8523] loss: 0.010159719735383987\n",
      "[step: 8524] loss: 0.010157202370464802\n",
      "[step: 8525] loss: 0.01015465334057808\n",
      "[step: 8526] loss: 0.010152065195143223\n",
      "[step: 8527] loss: 0.010149441659450531\n",
      "[step: 8528] loss: 0.010146785527467728\n",
      "[step: 8529] loss: 0.010144079104065895\n",
      "[step: 8530] loss: 0.010141342878341675\n",
      "[step: 8531] loss: 0.01013855915516615\n",
      "[step: 8532] loss: 0.010135732591152191\n",
      "[step: 8533] loss: 0.010132859461009502\n",
      "[step: 8534] loss: 0.010129936039447784\n",
      "[step: 8535] loss: 0.010126967914402485\n",
      "[step: 8536] loss: 0.01012395229190588\n",
      "[step: 8537] loss: 0.010120876133441925\n",
      "[step: 8538] loss: 0.010117750614881516\n",
      "[step: 8539] loss: 0.01011456735432148\n",
      "[step: 8540] loss: 0.010111324489116669\n",
      "[step: 8541] loss: 0.010108026675879955\n",
      "[step: 8542] loss: 0.010104660876095295\n",
      "[step: 8543] loss: 0.010101232677698135\n",
      "[step: 8544] loss: 0.010097736492753029\n",
      "[step: 8545] loss: 0.010094174183905125\n",
      "[step: 8546] loss: 0.010090533643960953\n",
      "[step: 8547] loss: 0.010086825117468834\n",
      "[step: 8548] loss: 0.010083041153848171\n",
      "[step: 8549] loss: 0.010079173371195793\n",
      "[step: 8550] loss: 0.010075231082737446\n",
      "[step: 8551] loss: 0.010071196593344212\n",
      "[step: 8552] loss: 0.010067081078886986\n",
      "[step: 8553] loss: 0.010062877088785172\n",
      "[step: 8554] loss: 0.010058573447167873\n",
      "[step: 8555] loss: 0.01005418598651886\n",
      "[step: 8556] loss: 0.010049687698483467\n",
      "[step: 8557] loss: 0.010045094415545464\n",
      "[step: 8558] loss: 0.010040391236543655\n",
      "[step: 8559] loss: 0.010035582818090916\n",
      "[step: 8560] loss: 0.01003066636621952\n",
      "[step: 8561] loss: 0.010025636292994022\n",
      "[step: 8562] loss: 0.010020487010478973\n",
      "[step: 8563] loss: 0.010015211999416351\n",
      "[step: 8564] loss: 0.010009817779064178\n",
      "[step: 8565] loss: 0.01000429317355156\n",
      "[step: 8566] loss: 0.009998634457588196\n",
      "[step: 8567] loss: 0.009992845356464386\n",
      "[step: 8568] loss: 0.009986920282244682\n",
      "[step: 8569] loss: 0.00998084619641304\n",
      "[step: 8570] loss: 0.009974630549550056\n",
      "[step: 8571] loss: 0.009968268685042858\n",
      "[step: 8572] loss: 0.009961752220988274\n",
      "[step: 8573] loss: 0.009955085813999176\n",
      "[step: 8574] loss: 0.00994825828820467\n",
      "[step: 8575] loss: 0.00994127057492733\n",
      "[step: 8576] loss: 0.009934120811522007\n",
      "[step: 8577] loss: 0.009926806204020977\n",
      "[step: 8578] loss: 0.009919323027133942\n",
      "[step: 8579] loss: 0.009911676868796349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 8580] loss: 0.009903854690492153\n",
      "[step: 8581] loss: 0.009895866736769676\n",
      "[step: 8582] loss: 0.00988770741969347\n",
      "[step: 8583] loss: 0.009879379533231258\n",
      "[step: 8584] loss: 0.009870870038866997\n",
      "[step: 8585] loss: 0.009862201288342476\n",
      "[step: 8586] loss: 0.009853361174464226\n",
      "[step: 8587] loss: 0.009844357147812843\n",
      "[step: 8588] loss: 0.009835192933678627\n",
      "[step: 8589] loss: 0.009825874119997025\n",
      "[step: 8590] loss: 0.009816399775445461\n",
      "[step: 8591] loss: 0.009806782007217407\n",
      "[step: 8592] loss: 0.009797025471925735\n",
      "[step: 8593] loss: 0.009787137620151043\n",
      "[step: 8594] loss: 0.00977712869644165\n",
      "[step: 8595] loss: 0.009767008014023304\n",
      "[step: 8596] loss: 0.009756785817444324\n",
      "[step: 8597] loss: 0.009746472351253033\n",
      "[step: 8598] loss: 0.009736089035868645\n",
      "[step: 8599] loss: 0.009725643321871758\n",
      "[step: 8600] loss: 0.009715143591165543\n",
      "[step: 8601] loss: 0.009704611264169216\n",
      "[step: 8602] loss: 0.00969406496733427\n",
      "[step: 8603] loss: 0.009683518670499325\n",
      "[step: 8604] loss: 0.0096729826182127\n",
      "[step: 8605] loss: 0.009662478230893612\n",
      "[step: 8606] loss: 0.009652025066316128\n",
      "[step: 8607] loss: 0.009641634300351143\n",
      "[step: 8608] loss: 0.00963133480399847\n",
      "[step: 8609] loss: 0.009621192701160908\n",
      "[step: 8610] loss: 0.009611495770514011\n",
      "[step: 8611] loss: 0.00960405170917511\n",
      "[step: 8612] loss: 0.009610486216843128\n",
      "[step: 8613] loss: 0.00971295963972807\n",
      "[step: 8614] loss: 0.010176006704568863\n",
      "[step: 8615] loss: 0.011735150590538979\n",
      "[step: 8616] loss: 0.010296139866113663\n",
      "[step: 8617] loss: 0.012105870991945267\n",
      "[step: 8618] loss: 0.011091572232544422\n",
      "[step: 8619] loss: 0.011980497278273106\n",
      "[step: 8620] loss: 0.010392872616648674\n",
      "[step: 8621] loss: 0.01175648719072342\n",
      "[step: 8622] loss: 0.01066284254193306\n",
      "[step: 8623] loss: 0.011512459255754948\n",
      "[step: 8624] loss: 0.011701468378305435\n",
      "[step: 8625] loss: 0.010899144224822521\n",
      "[step: 8626] loss: 0.010427342727780342\n",
      "[step: 8627] loss: 0.010741145350039005\n",
      "[step: 8628] loss: 0.010192346759140491\n",
      "[step: 8629] loss: 0.010480638593435287\n",
      "[step: 8630] loss: 0.010136113502085209\n",
      "[step: 8631] loss: 0.010176289826631546\n",
      "[step: 8632] loss: 0.010370783507823944\n",
      "[step: 8633] loss: 0.00997404009103775\n",
      "[step: 8634] loss: 0.010052318684756756\n",
      "[step: 8635] loss: 0.01017181109637022\n",
      "[step: 8636] loss: 0.009911371394991875\n",
      "[step: 8637] loss: 0.010047498159110546\n",
      "[step: 8638] loss: 0.009834333322942257\n",
      "[step: 8639] loss: 0.0099227549508214\n",
      "[step: 8640] loss: 0.009936151094734669\n",
      "[step: 8641] loss: 0.00973141472786665\n",
      "[step: 8642] loss: 0.009893229231238365\n",
      "[step: 8643] loss: 0.009796352125704288\n",
      "[step: 8644] loss: 0.009783300571143627\n",
      "[step: 8645] loss: 0.009789940901100636\n",
      "[step: 8646] loss: 0.00973599310964346\n",
      "[step: 8647] loss: 0.009742110036313534\n",
      "[step: 8648] loss: 0.009695863351225853\n",
      "[step: 8649] loss: 0.009689943864941597\n",
      "[step: 8650] loss: 0.009682372212409973\n",
      "[step: 8651] loss: 0.009639076888561249\n",
      "[step: 8652] loss: 0.009640399366617203\n",
      "[step: 8653] loss: 0.009626536630094051\n",
      "[step: 8654] loss: 0.009593603201210499\n",
      "[step: 8655] loss: 0.009589612483978271\n",
      "[step: 8656] loss: 0.009578430093824863\n",
      "[step: 8657] loss: 0.009539670310914516\n",
      "[step: 8658] loss: 0.009552860632538795\n",
      "[step: 8659] loss: 0.009518940933048725\n",
      "[step: 8660] loss: 0.00952191837131977\n",
      "[step: 8661] loss: 0.00949910283088684\n",
      "[step: 8662] loss: 0.009499835781753063\n",
      "[step: 8663] loss: 0.009471476078033447\n",
      "[step: 8664] loss: 0.009478289633989334\n",
      "[step: 8665] loss: 0.009451806545257568\n",
      "[step: 8666] loss: 0.009454122744500637\n",
      "[step: 8667] loss: 0.009437423199415207\n",
      "[step: 8668] loss: 0.009430713951587677\n",
      "[step: 8669] loss: 0.009423663839697838\n",
      "[step: 8670] loss: 0.009412501007318497\n",
      "[step: 8671] loss: 0.009408487938344479\n",
      "[step: 8672] loss: 0.00939495675265789\n",
      "[step: 8673] loss: 0.009395869448781013\n",
      "[step: 8674] loss: 0.009382416494190693\n",
      "[step: 8675] loss: 0.009384531527757645\n",
      "[step: 8676] loss: 0.009373366832733154\n",
      "[step: 8677] loss: 0.009370388463139534\n",
      "[step: 8678] loss: 0.009365443140268326\n",
      "[step: 8679] loss: 0.009361608885228634\n",
      "[step: 8680] loss: 0.009353776462376118\n",
      "[step: 8681] loss: 0.009354786016047001\n",
      "[step: 8682] loss: 0.009347590617835522\n",
      "[step: 8683] loss: 0.009344186633825302\n",
      "[step: 8684] loss: 0.009341462515294552\n",
      "[step: 8685] loss: 0.009339284151792526\n",
      "[step: 8686] loss: 0.009332657791674137\n",
      "[step: 8687] loss: 0.009331041015684605\n",
      "[step: 8688] loss: 0.009329201653599739\n",
      "[step: 8689] loss: 0.009324360638856888\n",
      "[step: 8690] loss: 0.009320915676653385\n",
      "[step: 8691] loss: 0.00931913685053587\n",
      "[step: 8692] loss: 0.00931669119745493\n",
      "[step: 8693] loss: 0.009312424808740616\n",
      "[step: 8694] loss: 0.009309916757047176\n",
      "[step: 8695] loss: 0.00930753443390131\n",
      "[step: 8696] loss: 0.009305681101977825\n",
      "[step: 8697] loss: 0.009301815181970596\n",
      "[step: 8698] loss: 0.009299425408244133\n",
      "[step: 8699] loss: 0.009296877309679985\n",
      "[step: 8700] loss: 0.009295014664530754\n",
      "[step: 8701] loss: 0.00929180532693863\n",
      "[step: 8702] loss: 0.009289299137890339\n",
      "[step: 8703] loss: 0.009286576882004738\n",
      "[step: 8704] loss: 0.009284712374210358\n",
      "[step: 8705] loss: 0.009281928651034832\n",
      "[step: 8706] loss: 0.009279390797019005\n",
      "[step: 8707] loss: 0.009276491589844227\n",
      "[step: 8708] loss: 0.009274222888052464\n",
      "[step: 8709] loss: 0.009271703660488129\n",
      "[step: 8710] loss: 0.009269223548471928\n",
      "[step: 8711] loss: 0.009266428649425507\n",
      "[step: 8712] loss: 0.009263941086828709\n",
      "[step: 8713] loss: 0.00926156248897314\n",
      "[step: 8714] loss: 0.009259376674890518\n",
      "[step: 8715] loss: 0.009257083758711815\n",
      "[step: 8716] loss: 0.009254810400307178\n",
      "[step: 8717] loss: 0.009252573363482952\n",
      "[step: 8718] loss: 0.009250402450561523\n",
      "[step: 8719] loss: 0.009248249232769012\n",
      "[step: 8720] loss: 0.009245993569493294\n",
      "[step: 8721] loss: 0.009243778884410858\n",
      "[step: 8722] loss: 0.009241599589586258\n",
      "[step: 8723] loss: 0.009239626117050648\n",
      "[step: 8724] loss: 0.009237684309482574\n",
      "[step: 8725] loss: 0.009235820733010769\n",
      "[step: 8726] loss: 0.00923388171941042\n",
      "[step: 8727] loss: 0.009231999516487122\n",
      "[step: 8728] loss: 0.009230092167854309\n",
      "[step: 8729] loss: 0.009228277951478958\n",
      "[step: 8730] loss: 0.009226441383361816\n",
      "[step: 8731] loss: 0.009224671870470047\n",
      "[step: 8732] loss: 0.009222892113029957\n",
      "[step: 8733] loss: 0.00922120176255703\n",
      "[step: 8734] loss: 0.009219524450600147\n",
      "[step: 8735] loss: 0.009217900224030018\n",
      "[step: 8736] loss: 0.009216261096298695\n",
      "[step: 8737] loss: 0.00921463593840599\n",
      "[step: 8738] loss: 0.009213004261255264\n",
      "[step: 8739] loss: 0.009211404249072075\n",
      "[step: 8740] loss: 0.009209828451275826\n",
      "[step: 8741] loss: 0.009208278730511665\n",
      "[step: 8742] loss: 0.009206743910908699\n",
      "[step: 8743] loss: 0.009205215610563755\n",
      "[step: 8744] loss: 0.009203691966831684\n",
      "[step: 8745] loss: 0.009202180430293083\n",
      "[step: 8746] loss: 0.009200677275657654\n",
      "[step: 8747] loss: 0.009199176914989948\n",
      "[step: 8748] loss: 0.00919769424945116\n",
      "[step: 8749] loss: 0.009196216240525246\n",
      "[step: 8750] loss: 0.009194741025567055\n",
      "[step: 8751] loss: 0.009193269535899162\n",
      "[step: 8752] loss: 0.009191805496811867\n",
      "[step: 8753] loss: 0.009190346114337444\n",
      "[step: 8754] loss: 0.00918890256434679\n",
      "[step: 8755] loss: 0.009187453426420689\n",
      "[step: 8756] loss: 0.009186021983623505\n",
      "[step: 8757] loss: 0.009184586815536022\n",
      "[step: 8758] loss: 0.00918316375464201\n",
      "[step: 8759] loss: 0.009181741625070572\n",
      "[step: 8760] loss: 0.009180326946079731\n",
      "[step: 8761] loss: 0.009178916923701763\n",
      "[step: 8762] loss: 0.009177511557936668\n",
      "[step: 8763] loss: 0.009176110848784447\n",
      "[step: 8764] loss: 0.009174712002277374\n",
      "[step: 8765] loss: 0.009173322468996048\n",
      "[step: 8766] loss: 0.009171937592327595\n",
      "[step: 8767] loss: 0.009170557372272015\n",
      "[step: 8768] loss: 0.009169179014861584\n",
      "[step: 8769] loss: 0.009167809039354324\n",
      "[step: 8770] loss: 0.009166444651782513\n",
      "[step: 8771] loss: 0.009165100753307343\n",
      "[step: 8772] loss: 0.00916377454996109\n",
      "[step: 8773] loss: 0.009162495844066143\n",
      "[step: 8774] loss: 0.009161274880170822\n",
      "[step: 8775] loss: 0.009160229936242104\n",
      "[step: 8776] loss: 0.00915937963873148\n",
      "[step: 8777] loss: 0.009159247390925884\n",
      "[step: 8778] loss: 0.009159663692116737\n",
      "[step: 8779] loss: 0.00916336476802826\n",
      "[step: 8780] loss: 0.009167137555778027\n",
      "[step: 8781] loss: 0.009184964932501316\n",
      "[step: 8782] loss: 0.009185729548335075\n",
      "[step: 8783] loss: 0.009222457185387611\n",
      "[step: 8784] loss: 0.009183652698993683\n",
      "[step: 8785] loss: 0.009180476889014244\n",
      "[step: 8786] loss: 0.009158250875771046\n",
      "[step: 8787] loss: 0.009150425903499126\n",
      "[step: 8788] loss: 0.009144410490989685\n",
      "[step: 8789] loss: 0.00914135854691267\n",
      "[step: 8790] loss: 0.009139739908277988\n",
      "[step: 8791] loss: 0.009139186702668667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 8792] loss: 0.009139599278569221\n",
      "[step: 8793] loss: 0.00914050079882145\n",
      "[step: 8794] loss: 0.009144661016762257\n",
      "[step: 8795] loss: 0.009147174656391144\n",
      "[step: 8796] loss: 0.009160911664366722\n",
      "[step: 8797] loss: 0.009158551692962646\n",
      "[step: 8798] loss: 0.009179892018437386\n",
      "[step: 8799] loss: 0.009157514199614525\n",
      "[step: 8800] loss: 0.009160120040178299\n",
      "[step: 8801] loss: 0.009142064489424229\n",
      "[step: 8802] loss: 0.009138957597315311\n",
      "[step: 8803] loss: 0.00913144275546074\n",
      "[step: 8804] loss: 0.00913016963750124\n",
      "[step: 8805] loss: 0.00912705808877945\n",
      "[step: 8806] loss: 0.00912820640951395\n",
      "[step: 8807] loss: 0.009126926772296429\n",
      "[step: 8808] loss: 0.0091323833912611\n",
      "[step: 8809] loss: 0.009132461622357368\n",
      "[step: 8810] loss: 0.009148264303803444\n",
      "[step: 8811] loss: 0.009142259135842323\n",
      "[step: 8812] loss: 0.00916430726647377\n",
      "[step: 8813] loss: 0.009140178561210632\n",
      "[step: 8814] loss: 0.009145138785243034\n",
      "[step: 8815] loss: 0.009125900454819202\n",
      "[step: 8816] loss: 0.009125142358243465\n",
      "[step: 8817] loss: 0.009116949513554573\n",
      "[step: 8818] loss: 0.009119506925344467\n",
      "[step: 8819] loss: 0.00911602471023798\n",
      "[step: 8820] loss: 0.009124750271439552\n",
      "[step: 8821] loss: 0.009121336974203587\n",
      "[step: 8822] loss: 0.009138914756476879\n",
      "[step: 8823] loss: 0.009126775898039341\n",
      "[step: 8824] loss: 0.009144681505858898\n",
      "[step: 8825] loss: 0.00912135187536478\n",
      "[step: 8826] loss: 0.009125083684921265\n",
      "[step: 8827] loss: 0.009108366444706917\n",
      "[step: 8828] loss: 0.009108345955610275\n",
      "[step: 8829] loss: 0.00910085067152977\n",
      "[step: 8830] loss: 0.009104771539568901\n",
      "[step: 8831] loss: 0.00910191796720028\n",
      "[step: 8832] loss: 0.00911493506282568\n",
      "[step: 8833] loss: 0.009110253304243088\n",
      "[step: 8834] loss: 0.009134018793702126\n",
      "[step: 8835] loss: 0.00911355298012495\n",
      "[step: 8836] loss: 0.00912872888147831\n",
      "[step: 8837] loss: 0.009102976880967617\n",
      "[step: 8838] loss: 0.009104822762310505\n",
      "[step: 8839] loss: 0.009090806357562542\n",
      "[step: 8840] loss: 0.009093251079320908\n",
      "[step: 8841] loss: 0.009087281301617622\n",
      "[step: 8842] loss: 0.009097148664295673\n",
      "[step: 8843] loss: 0.009093686938285828\n",
      "[step: 8844] loss: 0.009117480367422104\n",
      "[step: 8845] loss: 0.009101253934204578\n",
      "[step: 8846] loss: 0.009123600088059902\n",
      "[step: 8847] loss: 0.00909276120364666\n",
      "[step: 8848] loss: 0.009095568209886551\n",
      "[step: 8849] loss: 0.009078131057322025\n",
      "[step: 8850] loss: 0.009079995565116405\n",
      "[step: 8851] loss: 0.009073043242096901\n",
      "[step: 8852] loss: 0.009082593955099583\n",
      "[step: 8853] loss: 0.009079310111701488\n",
      "[step: 8854] loss: 0.009104540571570396\n",
      "[step: 8855] loss: 0.009089057333767414\n",
      "[step: 8856] loss: 0.00911689456552267\n",
      "[step: 8857] loss: 0.009080948308110237\n",
      "[step: 8858] loss: 0.009083757176995277\n",
      "[step: 8859] loss: 0.009064042940735817\n",
      "[step: 8860] loss: 0.009065156802535057\n",
      "[step: 8861] loss: 0.009057975374162197\n",
      "[step: 8862] loss: 0.00906750001013279\n",
      "[step: 8863] loss: 0.009065081365406513\n",
      "[step: 8864] loss: 0.009092978201806545\n",
      "[step: 8865] loss: 0.009076457470655441\n",
      "[step: 8866] loss: 0.00910702720284462\n",
      "[step: 8867] loss: 0.009066966362297535\n",
      "[step: 8868] loss: 0.009068738669157028\n",
      "[step: 8869] loss: 0.009048922918736935\n",
      "[step: 8870] loss: 0.009050523862242699\n",
      "[step: 8871] loss: 0.009043821133673191\n",
      "[step: 8872] loss: 0.009055488742887974\n",
      "[step: 8873] loss: 0.00905293133109808\n",
      "[step: 8874] loss: 0.00908543448895216\n",
      "[step: 8875] loss: 0.009063046425580978\n",
      "[step: 8876] loss: 0.00909064244478941\n",
      "[step: 8877] loss: 0.00905025564134121\n",
      "[step: 8878] loss: 0.009051027707755566\n",
      "[step: 8879] loss: 0.009032979607582092\n",
      "[step: 8880] loss: 0.00903588067740202\n",
      "[step: 8881] loss: 0.009029987268149853\n",
      "[step: 8882] loss: 0.009045062586665154\n",
      "[step: 8883] loss: 0.009041093289852142\n",
      "[step: 8884] loss: 0.009077223017811775\n",
      "[step: 8885] loss: 0.009047266095876694\n",
      "[step: 8886] loss: 0.009067842736840248\n",
      "[step: 8887] loss: 0.009031831286847591\n",
      "[step: 8888] loss: 0.009033549576997757\n",
      "[step: 8889] loss: 0.009017892181873322\n",
      "[step: 8890] loss: 0.009023946709930897\n",
      "[step: 8891] loss: 0.009017937816679478\n",
      "[step: 8892] loss: 0.009038074873387814\n",
      "[step: 8893] loss: 0.00902868527919054\n",
      "[step: 8894] loss: 0.009063142351806164\n",
      "[step: 8895] loss: 0.009027990512549877\n",
      "[step: 8896] loss: 0.009040323086082935\n",
      "[step: 8897] loss: 0.00901192706078291\n",
      "[step: 8898] loss: 0.009015732444822788\n",
      "[step: 8899] loss: 0.009002980776131153\n",
      "[step: 8900] loss: 0.009013738483190536\n",
      "[step: 8901] loss: 0.009006500244140625\n",
      "[step: 8902] loss: 0.009031928144395351\n",
      "[step: 8903] loss: 0.009013836272060871\n",
      "[step: 8904] loss: 0.009041054174304008\n",
      "[step: 8905] loss: 0.009006869047880173\n",
      "[step: 8906] loss: 0.009015417657792568\n",
      "[step: 8907] loss: 0.008992955088615417\n",
      "[step: 8908] loss: 0.008999367244541645\n",
      "[step: 8909] loss: 0.008988035842776299\n",
      "[step: 8910] loss: 0.0090030487626791\n",
      "[step: 8911] loss: 0.008992871269583702\n",
      "[step: 8912] loss: 0.009019812569022179\n",
      "[step: 8913] loss: 0.008995706215500832\n",
      "[step: 8914] loss: 0.009016081690788269\n",
      "[step: 8915] loss: 0.008985725231468678\n",
      "[step: 8916] loss: 0.008993486873805523\n",
      "[step: 8917] loss: 0.008974690921604633\n",
      "[step: 8918] loss: 0.008983872830867767\n",
      "[step: 8919] loss: 0.008972578682005405\n",
      "[step: 8920] loss: 0.008990971371531487\n",
      "[step: 8921] loss: 0.008976894430816174\n",
      "[step: 8922] loss: 0.00900231022387743\n",
      "[step: 8923] loss: 0.008975488133728504\n",
      "[step: 8924] loss: 0.008991153910756111\n",
      "[step: 8925] loss: 0.008964688517153263\n",
      "[step: 8926] loss: 0.008972861804068089\n",
      "[step: 8927] loss: 0.008956174366176128\n",
      "[step: 8928] loss: 0.008967683650553226\n",
      "[step: 8929] loss: 0.008955653756856918\n",
      "[step: 8930] loss: 0.00897603016346693\n",
      "[step: 8931] loss: 0.008958686143159866\n",
      "[step: 8932] loss: 0.008981880731880665\n",
      "[step: 8933] loss: 0.008954423479735851\n",
      "[step: 8934] loss: 0.008967514149844646\n",
      "[step: 8935] loss: 0.008943543769419193\n",
      "[step: 8936] loss: 0.008952181786298752\n",
      "[step: 8937] loss: 0.008936634287238121\n",
      "[step: 8938] loss: 0.008949795737862587\n",
      "[step: 8939] loss: 0.008937012404203415\n",
      "[step: 8940] loss: 0.008958524093031883\n",
      "[step: 8941] loss: 0.008938922546803951\n",
      "[step: 8942] loss: 0.008960404433310032\n",
      "[step: 8943] loss: 0.008932805620133877\n",
      "[step: 8944] loss: 0.008944427594542503\n",
      "[step: 8945] loss: 0.00892175454646349\n",
      "[step: 8946] loss: 0.008930540643632412\n",
      "[step: 8947] loss: 0.008915667422115803\n",
      "[step: 8948] loss: 0.008929858915507793\n",
      "[step: 8949] loss: 0.008916747756302357\n",
      "[step: 8950] loss: 0.00893936026841402\n",
      "[step: 8951] loss: 0.008918248116970062\n",
      "[step: 8952] loss: 0.008939022198319435\n",
      "[step: 8953] loss: 0.008910702541470528\n",
      "[step: 8954] loss: 0.0089211231097579\n",
      "[step: 8955] loss: 0.008898858912289143\n",
      "[step: 8956] loss: 0.008907283656299114\n",
      "[step: 8957] loss: 0.008893045596778393\n",
      "[step: 8958] loss: 0.008907822892069817\n",
      "[step: 8959] loss: 0.008895119652152061\n",
      "[step: 8960] loss: 0.008919485844671726\n",
      "[step: 8961] loss: 0.008897244930267334\n",
      "[step: 8962] loss: 0.008918453939259052\n",
      "[step: 8963] loss: 0.008888334967195988\n",
      "[step: 8964] loss: 0.008897372521460056\n",
      "[step: 8965] loss: 0.008874639868736267\n",
      "[step: 8966] loss: 0.008881996385753155\n",
      "[step: 8967] loss: 0.008868533186614513\n",
      "[step: 8968] loss: 0.008883303962647915\n",
      "[step: 8969] loss: 0.008872227743268013\n",
      "[step: 8970] loss: 0.00889971386641264\n",
      "[step: 8971] loss: 0.008876821026206017\n",
      "[step: 8972] loss: 0.008900275453925133\n",
      "[step: 8973] loss: 0.008866561576724052\n",
      "[step: 8974] loss: 0.008873336017131805\n",
      "[step: 8975] loss: 0.008848946541547775\n",
      "[step: 8976] loss: 0.008854122832417488\n",
      "[step: 8977] loss: 0.008841386996209621\n",
      "[step: 8978] loss: 0.008854641579091549\n",
      "[step: 8979] loss: 0.008847523480653763\n",
      "[step: 8980] loss: 0.008879981003701687\n",
      "[step: 8981] loss: 0.00885866116732359\n",
      "[step: 8982] loss: 0.008889573626220226\n",
      "[step: 8983] loss: 0.008848030120134354\n",
      "[step: 8984] loss: 0.008848333731293678\n",
      "[step: 8985] loss: 0.008820764720439911\n",
      "[step: 8986] loss: 0.00882197730243206\n",
      "[step: 8987] loss: 0.008809253573417664\n",
      "[step: 8988] loss: 0.008817591704428196\n",
      "[step: 8989] loss: 0.008818083442747593\n",
      "[step: 8990] loss: 0.008853914216160774\n",
      "[step: 8991] loss: 0.008845885284245014\n",
      "[step: 8992] loss: 0.0089116794988513\n",
      "[step: 8993] loss: 0.008836411871016026\n",
      "[step: 8994] loss: 0.008798375725746155\n",
      "[step: 8995] loss: 0.008774809539318085\n",
      "[step: 8996] loss: 0.008770217187702656\n",
      "[step: 8997] loss: 0.008761243894696236\n",
      "[step: 8998] loss: 0.008756265044212341\n",
      "[step: 8999] loss: 0.008755764923989773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 9000] loss: 0.008759350515902042\n",
      "[step: 9001] loss: 0.008781078271567822\n",
      "[step: 9002] loss: 0.008877231739461422\n",
      "[step: 9003] loss: 0.008901878260076046\n",
      "[step: 9004] loss: 0.009141522459685802\n",
      "[step: 9005] loss: 0.008997604250907898\n",
      "[step: 9006] loss: 0.009232147596776485\n",
      "[step: 9007] loss: 0.011179040186107159\n",
      "[step: 9008] loss: 0.011351129971444607\n",
      "[step: 9009] loss: 0.010229436680674553\n",
      "[step: 9010] loss: 0.01088065654039383\n",
      "[step: 9011] loss: 0.027711134403944016\n",
      "[step: 9012] loss: 0.026116477325558662\n",
      "[step: 9013] loss: 0.01703125238418579\n",
      "[step: 9014] loss: 0.015127546153962612\n",
      "[step: 9015] loss: 0.013606562279164791\n",
      "[step: 9016] loss: 0.013302810490131378\n",
      "[step: 9017] loss: 0.014707164838910103\n",
      "[step: 9018] loss: 0.014117520302534103\n",
      "[step: 9019] loss: 0.014354372397065163\n",
      "[step: 9020] loss: 0.013633504509925842\n",
      "[step: 9021] loss: 0.013025553897023201\n",
      "[step: 9022] loss: 0.013006343506276608\n",
      "[step: 9023] loss: 0.012368143536150455\n",
      "[step: 9024] loss: 0.012720090337097645\n",
      "[step: 9025] loss: 0.012301699258387089\n",
      "[step: 9026] loss: 0.012496992014348507\n",
      "[step: 9027] loss: 0.012458445504307747\n",
      "[step: 9028] loss: 0.012548607774078846\n",
      "[step: 9029] loss: 0.012675043195486069\n",
      "[step: 9030] loss: 0.012365052476525307\n",
      "[step: 9031] loss: 0.012400256469845772\n",
      "[step: 9032] loss: 0.01206834614276886\n",
      "[step: 9033] loss: 0.012119287624955177\n",
      "[step: 9034] loss: 0.011945021338760853\n",
      "[step: 9035] loss: 0.011887989938259125\n",
      "[step: 9036] loss: 0.011857673525810242\n",
      "[step: 9037] loss: 0.011738508939743042\n",
      "[step: 9038] loss: 0.011830965988337994\n",
      "[step: 9039] loss: 0.011759034357964993\n",
      "[step: 9040] loss: 0.011772148311138153\n",
      "[step: 9041] loss: 0.011727927252650261\n",
      "[step: 9042] loss: 0.011658858507871628\n",
      "[step: 9043] loss: 0.011684389784932137\n",
      "[step: 9044] loss: 0.01162050012499094\n",
      "[step: 9045] loss: 0.011632192879915237\n",
      "[step: 9046] loss: 0.01159929670393467\n",
      "[step: 9047] loss: 0.011555016972124577\n",
      "[step: 9048] loss: 0.01155040506273508\n",
      "[step: 9049] loss: 0.011508299969136715\n",
      "[step: 9050] loss: 0.011530016548931599\n",
      "[step: 9051] loss: 0.011505244299769402\n",
      "[step: 9052] loss: 0.011469229124486446\n",
      "[step: 9053] loss: 0.01146687287837267\n",
      "[step: 9054] loss: 0.011442586779594421\n",
      "[step: 9055] loss: 0.011440313421189785\n",
      "[step: 9056] loss: 0.01141304336488247\n",
      "[step: 9057] loss: 0.011389504186809063\n",
      "[step: 9058] loss: 0.01138300634920597\n",
      "[step: 9059] loss: 0.01136519480496645\n",
      "[step: 9060] loss: 0.011359614320099354\n",
      "[step: 9061] loss: 0.01133776642382145\n",
      "[step: 9062] loss: 0.011320243589580059\n",
      "[step: 9063] loss: 0.01131192035973072\n",
      "[step: 9064] loss: 0.011294159106910229\n",
      "[step: 9065] loss: 0.011284036561846733\n",
      "[step: 9066] loss: 0.011265129782259464\n",
      "[step: 9067] loss: 0.01124926470220089\n",
      "[step: 9068] loss: 0.011241094209253788\n",
      "[step: 9069] loss: 0.011226917617022991\n",
      "[step: 9070] loss: 0.011215346865355968\n",
      "[step: 9071] loss: 0.011200889945030212\n",
      "[step: 9072] loss: 0.01119057647883892\n",
      "[step: 9073] loss: 0.011181358247995377\n",
      "[step: 9074] loss: 0.011166014708578587\n",
      "[step: 9075] loss: 0.011155218817293644\n",
      "[step: 9076] loss: 0.011144276708364487\n",
      "[step: 9077] loss: 0.01113321352750063\n",
      "[step: 9078] loss: 0.011121799238026142\n",
      "[step: 9079] loss: 0.011109668761491776\n",
      "[step: 9080] loss: 0.011100736446678638\n",
      "[step: 9081] loss: 0.011089663952589035\n",
      "[step: 9082] loss: 0.011078245006501675\n",
      "[step: 9083] loss: 0.011067832820117474\n",
      "[step: 9084] loss: 0.011056941002607346\n",
      "[step: 9085] loss: 0.011046887375414371\n",
      "[step: 9086] loss: 0.0110354945063591\n",
      "[step: 9087] loss: 0.011024948209524155\n",
      "[step: 9088] loss: 0.011015353724360466\n",
      "[step: 9089] loss: 0.011004822328686714\n",
      "[step: 9090] loss: 0.010994833894073963\n",
      "[step: 9091] loss: 0.010984903201460838\n",
      "[step: 9092] loss: 0.010975494980812073\n",
      "[step: 9093] loss: 0.010965932160615921\n",
      "[step: 9094] loss: 0.010956238023936749\n",
      "[step: 9095] loss: 0.010947566479444504\n",
      "[step: 9096] loss: 0.010938720777630806\n",
      "[step: 9097] loss: 0.010930100455880165\n",
      "[step: 9098] loss: 0.010921941138803959\n",
      "[step: 9099] loss: 0.01091401744633913\n",
      "[step: 9100] loss: 0.01090641412883997\n",
      "[step: 9101] loss: 0.010898763313889503\n",
      "[step: 9102] loss: 0.010891656391322613\n",
      "[step: 9103] loss: 0.010884791612625122\n",
      "[step: 9104] loss: 0.010877955704927444\n",
      "[step: 9105] loss: 0.010871579870581627\n",
      "[step: 9106] loss: 0.010865393094718456\n",
      "[step: 9107] loss: 0.010859363712370396\n",
      "[step: 9108] loss: 0.010853467509150505\n",
      "[step: 9109] loss: 0.010847818106412888\n",
      "[step: 9110] loss: 0.010842286050319672\n",
      "[step: 9111] loss: 0.010836780071258545\n",
      "[step: 9112] loss: 0.010831544175744057\n",
      "[step: 9113] loss: 0.010826347395777702\n",
      "[step: 9114] loss: 0.010821216739714146\n",
      "[step: 9115] loss: 0.010816222056746483\n",
      "[step: 9116] loss: 0.010811254382133484\n",
      "[step: 9117] loss: 0.01080634631216526\n",
      "[step: 9118] loss: 0.010801490396261215\n",
      "[step: 9119] loss: 0.010796725749969482\n",
      "[step: 9120] loss: 0.010791964828968048\n",
      "[step: 9121] loss: 0.010787264443933964\n",
      "[step: 9122] loss: 0.010782638564705849\n",
      "[step: 9123] loss: 0.010777999646961689\n",
      "[step: 9124] loss: 0.010773438960313797\n",
      "[step: 9125] loss: 0.010768920183181763\n",
      "[step: 9126] loss: 0.010764433071017265\n",
      "[step: 9127] loss: 0.010760020464658737\n",
      "[step: 9128] loss: 0.010755651630461216\n",
      "[step: 9129] loss: 0.010751321911811829\n",
      "[step: 9130] loss: 0.01074705459177494\n",
      "[step: 9131] loss: 0.010742838494479656\n",
      "[step: 9132] loss: 0.01073866244405508\n",
      "[step: 9133] loss: 0.010734555311501026\n",
      "[step: 9134] loss: 0.010730483569204807\n",
      "[step: 9135] loss: 0.010726461187005043\n",
      "[step: 9136] loss: 0.010722490958869457\n",
      "[step: 9137] loss: 0.010718549601733685\n",
      "[step: 9138] loss: 0.010714670643210411\n",
      "[step: 9139] loss: 0.0107108224183321\n",
      "[step: 9140] loss: 0.010707016102969646\n",
      "[step: 9141] loss: 0.01070325169712305\n",
      "[step: 9142] loss: 0.01069952454417944\n",
      "[step: 9143] loss: 0.010695824399590492\n",
      "[step: 9144] loss: 0.010692164301872253\n",
      "[step: 9145] loss: 0.010688532143831253\n",
      "[step: 9146] loss: 0.010684927925467491\n",
      "[step: 9147] loss: 0.010681351646780968\n",
      "[step: 9148] loss: 0.010677806101739407\n",
      "[step: 9149] loss: 0.010674288496375084\n",
      "[step: 9150] loss: 0.010670795105397701\n",
      "[step: 9151] loss: 0.010667324997484684\n",
      "[step: 9152] loss: 0.01066388376057148\n",
      "[step: 9153] loss: 0.010660464875400066\n",
      "[step: 9154] loss: 0.010657074861228466\n",
      "[step: 9155] loss: 0.01065369974821806\n",
      "[step: 9156] loss: 0.010650355368852615\n",
      "[step: 9157] loss: 0.010647032409906387\n",
      "[step: 9158] loss: 0.010643732734024525\n",
      "[step: 9159] loss: 0.010640452615916729\n",
      "[step: 9160] loss: 0.010637198574841022\n",
      "[step: 9161] loss: 0.01063397154211998\n",
      "[step: 9162] loss: 0.010630762204527855\n",
      "[step: 9163] loss: 0.010627573356032372\n",
      "[step: 9164] loss: 0.010624411515891552\n",
      "[step: 9165] loss: 0.010621262714266777\n",
      "[step: 9166] loss: 0.010618144646286964\n",
      "[step: 9167] loss: 0.010615040548145771\n",
      "[step: 9168] loss: 0.010611958801746368\n",
      "[step: 9169] loss: 0.010608900338411331\n",
      "[step: 9170] loss: 0.010605854913592339\n",
      "[step: 9171] loss: 0.01060283463448286\n",
      "[step: 9172] loss: 0.010599832981824875\n",
      "[step: 9173] loss: 0.010596846230328083\n",
      "[step: 9174] loss: 0.010593882761895657\n",
      "[step: 9175] loss: 0.010590935125946999\n",
      "[step: 9176] loss: 0.010588008910417557\n",
      "[step: 9177] loss: 0.010585101321339607\n",
      "[step: 9178] loss: 0.010582211427390575\n",
      "[step: 9179] loss: 0.010579333640635014\n",
      "[step: 9180] loss: 0.010576478205621243\n",
      "[step: 9181] loss: 0.010573637671768665\n",
      "[step: 9182] loss: 0.010570816695690155\n",
      "[step: 9183] loss: 0.010568016208708286\n",
      "[step: 9184] loss: 0.010565231554210186\n",
      "[step: 9185] loss: 0.010562460869550705\n",
      "[step: 9186] loss: 0.010559708811342716\n",
      "[step: 9187] loss: 0.010556974448263645\n",
      "[step: 9188] loss: 0.010554255917668343\n",
      "[step: 9189] loss: 0.010551552288234234\n",
      "[step: 9190] loss: 0.010548869147896767\n",
      "[step: 9191] loss: 0.010546199977397919\n",
      "[step: 9192] loss: 0.010543547570705414\n",
      "[step: 9193] loss: 0.010540910065174103\n",
      "[step: 9194] loss: 0.010538285598158836\n",
      "[step: 9195] loss: 0.010535682551562786\n",
      "[step: 9196] loss: 0.010533090680837631\n",
      "[step: 9197] loss: 0.01053051557391882\n",
      "[step: 9198] loss: 0.01052795723080635\n",
      "[step: 9199] loss: 0.010525406338274479\n",
      "[step: 9200] loss: 0.010522878728806973\n",
      "[step: 9201] loss: 0.01052036415785551\n",
      "[step: 9202] loss: 0.010517857037484646\n",
      "[step: 9203] loss: 0.010515372268855572\n",
      "[step: 9204] loss: 0.010512899607419968\n",
      "[step: 9205] loss: 0.010510435327887535\n",
      "[step: 9206] loss: 0.010507984086871147\n",
      "[step: 9207] loss: 0.010505550540983677\n",
      "[step: 9208] loss: 0.010503130033612251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 9209] loss: 0.010500717908143997\n",
      "[step: 9210] loss: 0.010498318821191788\n",
      "[step: 9211] loss: 0.010495933704078197\n",
      "[step: 9212] loss: 0.010493559762835503\n",
      "[step: 9213] loss: 0.01049119420349598\n",
      "[step: 9214] loss: 0.010488842613995075\n",
      "[step: 9215] loss: 0.010486497543752193\n",
      "[step: 9216] loss: 0.01048416830599308\n",
      "[step: 9217] loss: 0.01048184372484684\n",
      "[step: 9218] loss: 0.010479534044861794\n",
      "[step: 9219] loss: 0.010477231815457344\n",
      "[step: 9220] loss: 0.010474937036633492\n",
      "[step: 9221] loss: 0.010472653433680534\n",
      "[step: 9222] loss: 0.010470372624695301\n",
      "[step: 9223] loss: 0.010468104854226112\n",
      "[step: 9224] loss: 0.010465841740369797\n",
      "[step: 9225] loss: 0.010463590733706951\n",
      "[step: 9226] loss: 0.01046134252101183\n",
      "[step: 9227] loss: 0.010459103621542454\n",
      "[step: 9228] loss: 0.010456868447363377\n",
      "[step: 9229] loss: 0.010454640723764896\n",
      "[step: 9230] loss: 0.01045241765677929\n",
      "[step: 9231] loss: 0.010450197383761406\n",
      "[step: 9232] loss: 0.010447988286614418\n",
      "[step: 9233] loss: 0.010445778258144855\n",
      "[step: 9234] loss: 0.010443574748933315\n",
      "[step: 9235] loss: 0.010441373102366924\n",
      "[step: 9236] loss: 0.010439176112413406\n",
      "[step: 9237] loss: 0.010436983779072762\n",
      "[step: 9238] loss: 0.010434792377054691\n",
      "[step: 9239] loss: 0.010432600043714046\n",
      "[step: 9240] loss: 0.010430415160953999\n",
      "[step: 9241] loss: 0.010428226552903652\n",
      "[step: 9242] loss: 0.01042604073882103\n",
      "[step: 9243] loss: 0.010423857718706131\n",
      "[step: 9244] loss: 0.010421673767268658\n",
      "[step: 9245] loss: 0.010419490747153759\n",
      "[step: 9246] loss: 0.01041730958968401\n",
      "[step: 9247] loss: 0.01041511818766594\n",
      "[step: 9248] loss: 0.010412932373583317\n",
      "[step: 9249] loss: 0.01041074562817812\n",
      "[step: 9250] loss: 0.010408561676740646\n",
      "[step: 9251] loss: 0.010406364686787128\n",
      "[step: 9252] loss: 0.010404168628156185\n",
      "[step: 9253] loss: 0.010401972569525242\n",
      "[step: 9254] loss: 0.010399773716926575\n",
      "[step: 9255] loss: 0.010397573001682758\n",
      "[step: 9256] loss: 0.010395361110568047\n",
      "[step: 9257] loss: 0.010393152944743633\n",
      "[step: 9258] loss: 0.0103909308090806\n",
      "[step: 9259] loss: 0.010388712398707867\n",
      "[step: 9260] loss: 0.010386483743786812\n",
      "[step: 9261] loss: 0.010384252294898033\n",
      "[step: 9262] loss: 0.010382009670138359\n",
      "[step: 9263] loss: 0.010379765182733536\n",
      "[step: 9264] loss: 0.010377512313425541\n",
      "[step: 9265] loss: 0.010375252924859524\n",
      "[step: 9266] loss: 0.010372985154390335\n",
      "[step: 9267] loss: 0.010370709002017975\n",
      "[step: 9268] loss: 0.010368424467742443\n",
      "[step: 9269] loss: 0.010366133414208889\n",
      "[step: 9270] loss: 0.010363832116127014\n",
      "[step: 9271] loss: 0.010361520573496819\n",
      "[step: 9272] loss: 0.010359196923673153\n",
      "[step: 9273] loss: 0.010356868617236614\n",
      "[step: 9274] loss: 0.010354523546993732\n",
      "[step: 9275] loss: 0.010352171957492828\n",
      "[step: 9276] loss: 0.010349808260798454\n",
      "[step: 9277] loss: 0.010347431525588036\n",
      "[step: 9278] loss: 0.010345039889216423\n",
      "[step: 9279] loss: 0.010342634283006191\n",
      "[step: 9280] loss: 0.010340221226215363\n",
      "[step: 9281] loss: 0.01033779513090849\n",
      "[step: 9282] loss: 0.010335352271795273\n",
      "[step: 9283] loss: 0.010332895442843437\n",
      "[step: 9284] loss: 0.010330422781407833\n",
      "[step: 9285] loss: 0.010327939875423908\n",
      "[step: 9286] loss: 0.010325436480343342\n",
      "[step: 9287] loss: 0.010322925634682178\n",
      "[step: 9288] loss: 0.010320397093892097\n",
      "[step: 9289] loss: 0.010317874141037464\n",
      "[step: 9290] loss: 0.010315380990505219\n",
      "[step: 9291] loss: 0.010312940925359726\n",
      "[step: 9292] loss: 0.010310836136341095\n",
      "[step: 9293] loss: 0.01030877884477377\n",
      "[step: 9294] loss: 0.010308574885129929\n",
      "[step: 9295] loss: 0.010303856804966927\n",
      "[step: 9296] loss: 0.010301229543983936\n",
      "[step: 9297] loss: 0.010297614149749279\n",
      "[step: 9298] loss: 0.010295215994119644\n",
      "[step: 9299] loss: 0.010292618535459042\n",
      "[step: 9300] loss: 0.01029206532984972\n",
      "[step: 9301] loss: 0.010287348181009293\n",
      "[step: 9302] loss: 0.01028476096689701\n",
      "[step: 9303] loss: 0.010281062684953213\n",
      "[step: 9304] loss: 0.010279146023094654\n",
      "[step: 9305] loss: 0.010276388376951218\n",
      "[step: 9306] loss: 0.010277265682816505\n",
      "[step: 9307] loss: 0.01026885025203228\n",
      "[step: 9308] loss: 0.010270493105053902\n",
      "[step: 9309] loss: 0.010281503200531006\n",
      "[step: 9310] loss: 0.010295258834958076\n",
      "[step: 9311] loss: 0.010497934184968472\n",
      "[step: 9312] loss: 0.010332771576941013\n",
      "[step: 9313] loss: 0.010389629751443863\n",
      "[step: 9314] loss: 0.010403013788163662\n",
      "[step: 9315] loss: 0.01040466595441103\n",
      "[step: 9316] loss: 0.010419394820928574\n",
      "[step: 9317] loss: 0.010435434989631176\n",
      "[step: 9318] loss: 0.010386611334979534\n",
      "[step: 9319] loss: 0.010410098358988762\n",
      "[step: 9320] loss: 0.010337517596781254\n",
      "[step: 9321] loss: 0.010346843861043453\n",
      "[step: 9322] loss: 0.01030375249683857\n",
      "[step: 9323] loss: 0.010296369902789593\n",
      "[step: 9324] loss: 0.010292347520589828\n",
      "[step: 9325] loss: 0.010274702683091164\n",
      "[step: 9326] loss: 0.010283054783940315\n",
      "[step: 9327] loss: 0.010266650468111038\n",
      "[step: 9328] loss: 0.010267657227814198\n",
      "[step: 9329] loss: 0.010261865332722664\n",
      "[step: 9330] loss: 0.010251102037727833\n",
      "[step: 9331] loss: 0.010252324864268303\n",
      "[step: 9332] loss: 0.01023829635232687\n",
      "[step: 9333] loss: 0.010241318494081497\n",
      "[step: 9334] loss: 0.010229295119643211\n",
      "[step: 9335] loss: 0.010228213854134083\n",
      "[step: 9336] loss: 0.01021984126418829\n",
      "[step: 9337] loss: 0.010215849615633488\n",
      "[step: 9338] loss: 0.010210549458861351\n",
      "[step: 9339] loss: 0.010206242091953754\n",
      "[step: 9340] loss: 0.010200888849794865\n",
      "[step: 9341] loss: 0.010198603384196758\n",
      "[step: 9342] loss: 0.01019100472331047\n",
      "[step: 9343] loss: 0.010190148837864399\n",
      "[step: 9344] loss: 0.010181167162954807\n",
      "[step: 9345] loss: 0.010179571807384491\n",
      "[step: 9346] loss: 0.010171188972890377\n",
      "[step: 9347] loss: 0.010168005712330341\n",
      "[step: 9348] loss: 0.010161296464502811\n",
      "[step: 9349] loss: 0.010156741365790367\n",
      "[step: 9350] loss: 0.010151028633117676\n",
      "[step: 9351] loss: 0.010145892389118671\n",
      "[step: 9352] loss: 0.01014066394418478\n",
      "[step: 9353] loss: 0.01013534888625145\n",
      "[step: 9354] loss: 0.010130312293767929\n",
      "[step: 9355] loss: 0.010124759748578072\n",
      "[step: 9356] loss: 0.01012024562805891\n",
      "[step: 9357] loss: 0.010114415548741817\n",
      "[step: 9358] loss: 0.010109938681125641\n",
      "[step: 9359] loss: 0.010104084387421608\n",
      "[step: 9360] loss: 0.010099011473357677\n",
      "[step: 9361] loss: 0.010093684308230877\n",
      "[step: 9362] loss: 0.0100876335054636\n",
      "[step: 9363] loss: 0.010082792490720749\n",
      "[step: 9364] loss: 0.010076339356601238\n",
      "[step: 9365] loss: 0.010071276687085629\n",
      "[step: 9366] loss: 0.010065226815640926\n",
      "[step: 9367] loss: 0.010059337131679058\n",
      "[step: 9368] loss: 0.010053776204586029\n",
      "[step: 9369] loss: 0.010047546587884426\n",
      "[step: 9370] loss: 0.010041601024568081\n",
      "[step: 9371] loss: 0.01003571879118681\n",
      "[step: 9372] loss: 0.0100295040756464\n",
      "[step: 9373] loss: 0.010023267939686775\n",
      "[step: 9374] loss: 0.01001741737127304\n",
      "[step: 9375] loss: 0.010010945610702038\n",
      "[step: 9376] loss: 0.010004481300711632\n",
      "[step: 9377] loss: 0.009998001158237457\n",
      "[step: 9378] loss: 0.009991631843149662\n",
      "[step: 9379] loss: 0.009984892792999744\n",
      "[step: 9380] loss: 0.00997815653681755\n",
      "[step: 9381] loss: 0.009971458464860916\n",
      "[step: 9382] loss: 0.009964820928871632\n",
      "[step: 9383] loss: 0.009958080016076565\n",
      "[step: 9384] loss: 0.009951186366379261\n",
      "[step: 9385] loss: 0.009944268502295017\n",
      "[step: 9386] loss: 0.009937307797372341\n",
      "[step: 9387] loss: 0.009930414147675037\n",
      "[step: 9388] loss: 0.009923499077558517\n",
      "[step: 9389] loss: 0.009916734881699085\n",
      "[step: 9390] loss: 0.00991029106080532\n",
      "[step: 9391] loss: 0.00990527868270874\n",
      "[step: 9392] loss: 0.009905369952321053\n",
      "[step: 9393] loss: 0.00992740597575903\n",
      "[step: 9394] loss: 0.010029890574514866\n",
      "[step: 9395] loss: 0.010380174964666367\n",
      "[step: 9396] loss: 0.01090478990226984\n",
      "[step: 9397] loss: 0.010825781151652336\n",
      "[step: 9398] loss: 0.010887542739510536\n",
      "[step: 9399] loss: 0.010757610201835632\n",
      "[step: 9400] loss: 0.010318828746676445\n",
      "[step: 9401] loss: 0.010621602647006512\n",
      "[step: 9402] loss: 0.010448240675032139\n",
      "[step: 9403] loss: 0.010532776825129986\n",
      "[step: 9404] loss: 0.010350162163376808\n",
      "[step: 9405] loss: 0.01012459583580494\n",
      "[step: 9406] loss: 0.01034662127494812\n",
      "[step: 9407] loss: 0.010136239230632782\n",
      "[step: 9408] loss: 0.010213075205683708\n",
      "[step: 9409] loss: 0.010026887059211731\n",
      "[step: 9410] loss: 0.010210268199443817\n",
      "[step: 9411] loss: 0.01012637373059988\n",
      "[step: 9412] loss: 0.010050329379737377\n",
      "[step: 9413] loss: 0.010008434765040874\n",
      "[step: 9414] loss: 0.01001269742846489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 9415] loss: 0.010038802400231361\n",
      "[step: 9416] loss: 0.009915373288094997\n",
      "[step: 9417] loss: 0.010001344606280327\n",
      "[step: 9418] loss: 0.0099415872246027\n",
      "[step: 9419] loss: 0.009973304346203804\n",
      "[step: 9420] loss: 0.009884655475616455\n",
      "[step: 9421] loss: 0.009926311671733856\n",
      "[step: 9422] loss: 0.009905513375997543\n",
      "[step: 9423] loss: 0.009900287725031376\n",
      "[step: 9424] loss: 0.009881814010441303\n",
      "[step: 9425] loss: 0.009882431477308273\n",
      "[step: 9426] loss: 0.009883913211524487\n",
      "[step: 9427] loss: 0.009836569428443909\n",
      "[step: 9428] loss: 0.009843533858656883\n",
      "[step: 9429] loss: 0.009823670610785484\n",
      "[step: 9430] loss: 0.009831623174250126\n",
      "[step: 9431] loss: 0.009802116081118584\n",
      "[step: 9432] loss: 0.009822540916502476\n",
      "[step: 9433] loss: 0.009798004291951656\n",
      "[step: 9434] loss: 0.00979576539248228\n",
      "[step: 9435] loss: 0.009780362248420715\n",
      "[step: 9436] loss: 0.009781286120414734\n",
      "[step: 9437] loss: 0.00976698100566864\n",
      "[step: 9438] loss: 0.009762151166796684\n",
      "[step: 9439] loss: 0.009763892740011215\n",
      "[step: 9440] loss: 0.009753087535500526\n",
      "[step: 9441] loss: 0.009749719873070717\n",
      "[step: 9442] loss: 0.009740946814417839\n",
      "[step: 9443] loss: 0.00973773468285799\n",
      "[step: 9444] loss: 0.009723822586238384\n",
      "[step: 9445] loss: 0.009723895229399204\n",
      "[step: 9446] loss: 0.009714850224554539\n",
      "[step: 9447] loss: 0.009710819460451603\n",
      "[step: 9448] loss: 0.009703786112368107\n",
      "[step: 9449] loss: 0.00970129668712616\n",
      "[step: 9450] loss: 0.009691847488284111\n",
      "[step: 9451] loss: 0.009688134305179119\n",
      "[step: 9452] loss: 0.009682318195700645\n",
      "[step: 9453] loss: 0.009676313027739525\n",
      "[step: 9454] loss: 0.009671438485383987\n",
      "[step: 9455] loss: 0.00966660212725401\n",
      "[step: 9456] loss: 0.009660445153713226\n",
      "[step: 9457] loss: 0.009655303321778774\n",
      "[step: 9458] loss: 0.00965060107409954\n",
      "[step: 9459] loss: 0.00964420847594738\n",
      "[step: 9460] loss: 0.009639948606491089\n",
      "[step: 9461] loss: 0.00963413156569004\n",
      "[step: 9462] loss: 0.009628883562982082\n",
      "[step: 9463] loss: 0.00962354987859726\n",
      "[step: 9464] loss: 0.00961898360401392\n",
      "[step: 9465] loss: 0.009613750502467155\n",
      "[step: 9466] loss: 0.009611032903194427\n",
      "[step: 9467] loss: 0.009611403569579124\n",
      "[step: 9468] loss: 0.009622585959732533\n",
      "[step: 9469] loss: 0.00969143956899643\n",
      "[step: 9470] loss: 0.009664206765592098\n",
      "[step: 9471] loss: 0.00972495973110199\n",
      "[step: 9472] loss: 0.00959537923336029\n",
      "[step: 9473] loss: 0.009680607356131077\n",
      "[step: 9474] loss: 0.010148958303034306\n",
      "[step: 9475] loss: 0.009691968560218811\n",
      "[step: 9476] loss: 0.010240067727863789\n",
      "[step: 9477] loss: 0.011524359695613384\n",
      "[step: 9478] loss: 0.011416953057050705\n",
      "[step: 9479] loss: 0.01022277306765318\n",
      "[step: 9480] loss: 0.013074155896902084\n",
      "[step: 9481] loss: 0.017121462151408195\n",
      "[step: 9482] loss: 0.017810899764299393\n",
      "[step: 9483] loss: 0.01330780889838934\n",
      "[step: 9484] loss: 0.012788893654942513\n",
      "[step: 9485] loss: 0.012485144659876823\n",
      "[step: 9486] loss: 0.011873273178935051\n",
      "[step: 9487] loss: 0.01210765726864338\n",
      "[step: 9488] loss: 0.012500442564487457\n",
      "[step: 9489] loss: 0.011587820015847683\n",
      "[step: 9490] loss: 0.012271355837583542\n",
      "[step: 9491] loss: 0.01171137299388647\n",
      "[step: 9492] loss: 0.011549192480742931\n",
      "[step: 9493] loss: 0.011782343499362469\n",
      "[step: 9494] loss: 0.011191122233867645\n",
      "[step: 9495] loss: 0.011577395722270012\n",
      "[step: 9496] loss: 0.011402767151594162\n",
      "[step: 9497] loss: 0.011097224429249763\n",
      "[step: 9498] loss: 0.011353732086718082\n",
      "[step: 9499] loss: 0.010908124037086964\n",
      "[step: 9500] loss: 0.010903162881731987\n",
      "[step: 9501] loss: 0.010821730829775333\n",
      "[step: 9502] loss: 0.01061178371310234\n",
      "[step: 9503] loss: 0.010745013132691383\n",
      "[step: 9504] loss: 0.01053514052182436\n",
      "[step: 9505] loss: 0.010576806031167507\n",
      "[step: 9506] loss: 0.010487629100680351\n",
      "[step: 9507] loss: 0.010354719124734402\n",
      "[step: 9508] loss: 0.010442613624036312\n",
      "[step: 9509] loss: 0.010289637371897697\n",
      "[step: 9510] loss: 0.010403970256447792\n",
      "[step: 9511] loss: 0.010293247178196907\n",
      "[step: 9512] loss: 0.010297807864844799\n",
      "[step: 9513] loss: 0.010288141667842865\n",
      "[step: 9514] loss: 0.010246609337627888\n",
      "[step: 9515] loss: 0.0102829085662961\n",
      "[step: 9516] loss: 0.01020106952637434\n",
      "[step: 9517] loss: 0.010229053907096386\n",
      "[step: 9518] loss: 0.010159694589674473\n",
      "[step: 9519] loss: 0.010174425318837166\n",
      "[step: 9520] loss: 0.010143899358808994\n",
      "[step: 9521] loss: 0.01010619755834341\n",
      "[step: 9522] loss: 0.010085610672831535\n",
      "[step: 9523] loss: 0.010029192082583904\n",
      "[step: 9524] loss: 0.01002719160169363\n",
      "[step: 9525] loss: 0.00996728427708149\n",
      "[step: 9526] loss: 0.009950380772352219\n",
      "[step: 9527] loss: 0.009916296228766441\n",
      "[step: 9528] loss: 0.009898080490529537\n",
      "[step: 9529] loss: 0.009891600348055363\n",
      "[step: 9530] loss: 0.009871115908026695\n",
      "[step: 9531] loss: 0.009854594245553017\n",
      "[step: 9532] loss: 0.009817834012210369\n",
      "[step: 9533] loss: 0.009821996092796326\n",
      "[step: 9534] loss: 0.009796234779059887\n",
      "[step: 9535] loss: 0.009781125001609325\n",
      "[step: 9536] loss: 0.009763581678271294\n",
      "[step: 9537] loss: 0.009749011136591434\n",
      "[step: 9538] loss: 0.009739119559526443\n",
      "[step: 9539] loss: 0.009717433713376522\n",
      "[step: 9540] loss: 0.009705261327326298\n",
      "[step: 9541] loss: 0.00967965554445982\n",
      "[step: 9542] loss: 0.009669091552495956\n",
      "[step: 9543] loss: 0.00964810885488987\n",
      "[step: 9544] loss: 0.009628375060856342\n",
      "[step: 9545] loss: 0.009616728872060776\n",
      "[step: 9546] loss: 0.00960132572799921\n",
      "[step: 9547] loss: 0.009595116600394249\n",
      "[step: 9548] loss: 0.009581138379871845\n",
      "[step: 9549] loss: 0.009599252603948116\n",
      "[step: 9550] loss: 0.009530737064778805\n",
      "[step: 9551] loss: 0.009563807398080826\n",
      "[step: 9552] loss: 0.009734378196299076\n",
      "[step: 9553] loss: 0.009660948999226093\n",
      "[step: 9554] loss: 0.009627985768020153\n",
      "[step: 9555] loss: 0.009515603072941303\n",
      "[step: 9556] loss: 0.009507359005510807\n",
      "[step: 9557] loss: 0.009626930579543114\n",
      "[step: 9558] loss: 0.009830047376453876\n",
      "[step: 9559] loss: 0.00964734610170126\n",
      "[step: 9560] loss: 0.009910187683999538\n",
      "[step: 9561] loss: 0.00996956042945385\n",
      "[step: 9562] loss: 0.01015262957662344\n",
      "[step: 9563] loss: 0.009782751090824604\n",
      "[step: 9564] loss: 0.010233080014586449\n",
      "[step: 9565] loss: 0.009689802303910255\n",
      "[step: 9566] loss: 0.009967719204723835\n",
      "[step: 9567] loss: 0.009785241447389126\n",
      "[step: 9568] loss: 0.009906618855893612\n",
      "[step: 9569] loss: 0.009826297871768475\n",
      "[step: 9570] loss: 0.009797322563827038\n",
      "[step: 9571] loss: 0.00979814026504755\n",
      "[step: 9572] loss: 0.009663019329309464\n",
      "[step: 9573] loss: 0.009653179906308651\n",
      "[step: 9574] loss: 0.009711185470223427\n",
      "[step: 9575] loss: 0.009601057507097721\n",
      "[step: 9576] loss: 0.009670292027294636\n",
      "[step: 9577] loss: 0.009462770074605942\n",
      "[step: 9578] loss: 0.009586342610418797\n",
      "[step: 9579] loss: 0.009516955353319645\n",
      "[step: 9580] loss: 0.009553002193570137\n",
      "[step: 9581] loss: 0.009480577893555164\n",
      "[step: 9582] loss: 0.00945481937378645\n",
      "[step: 9583] loss: 0.009513498283922672\n",
      "[step: 9584] loss: 0.00942170899361372\n",
      "[step: 9585] loss: 0.009423048235476017\n",
      "[step: 9586] loss: 0.009377514943480492\n",
      "[step: 9587] loss: 0.009406115859746933\n",
      "[step: 9588] loss: 0.009370129555463791\n",
      "[step: 9589] loss: 0.00930783525109291\n",
      "[step: 9590] loss: 0.009333426132798195\n",
      "[step: 9591] loss: 0.009361084550619125\n",
      "[step: 9592] loss: 0.009281815029680729\n",
      "[step: 9593] loss: 0.009346559643745422\n",
      "[step: 9594] loss: 0.009370485320687294\n",
      "[step: 9595] loss: 0.009250445291399956\n",
      "[step: 9596] loss: 0.00951869785785675\n",
      "[step: 9597] loss: 0.009846867993474007\n",
      "[step: 9598] loss: 0.009476788341999054\n",
      "[step: 9599] loss: 0.009764089249074459\n",
      "[step: 9600] loss: 0.009388165548443794\n",
      "[step: 9601] loss: 0.009512577205896378\n",
      "[step: 9602] loss: 0.009314848110079765\n",
      "[step: 9603] loss: 0.009603982791304588\n",
      "[step: 9604] loss: 0.010152382776141167\n",
      "[step: 9605] loss: 0.009628737345337868\n",
      "[step: 9606] loss: 0.009962937794625759\n",
      "[step: 9607] loss: 0.009261677041649818\n",
      "[step: 9608] loss: 0.009741977788507938\n",
      "[step: 9609] loss: 0.009225265122950077\n",
      "[step: 9610] loss: 0.009738735854625702\n",
      "[step: 9611] loss: 0.009490243159234524\n",
      "[step: 9612] loss: 0.009659750387072563\n",
      "[step: 9613] loss: 0.009349524043500423\n",
      "[step: 9614] loss: 0.00954370480030775\n",
      "[step: 9615] loss: 0.00954778678715229\n",
      "[step: 9616] loss: 0.00973941758275032\n",
      "[step: 9617] loss: 0.009325530380010605\n",
      "[step: 9618] loss: 0.009854518808424473\n",
      "[step: 9619] loss: 0.010800454765558243\n",
      "[step: 9620] loss: 0.010543270967900753\n",
      "[step: 9621] loss: 0.010023334994912148\n",
      "[step: 9622] loss: 0.009943569079041481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 9623] loss: 0.011738196946680546\n",
      "[step: 9624] loss: 0.010630429722368717\n",
      "[step: 9625] loss: 0.010234697721898556\n",
      "[step: 9626] loss: 0.011562285013496876\n",
      "[step: 9627] loss: 0.011506232433021069\n",
      "[step: 9628] loss: 0.011012193746864796\n",
      "[step: 9629] loss: 0.010797310620546341\n",
      "[step: 9630] loss: 0.011004768311977386\n",
      "[step: 9631] loss: 0.01119736023247242\n",
      "[step: 9632] loss: 0.011004823260009289\n",
      "[step: 9633] loss: 0.010739272460341454\n",
      "[step: 9634] loss: 0.010404621250927448\n",
      "[step: 9635] loss: 0.010153462179005146\n",
      "[step: 9636] loss: 0.010245909914374352\n",
      "[step: 9637] loss: 0.010446456260979176\n",
      "[step: 9638] loss: 0.010493434965610504\n",
      "[step: 9639] loss: 0.010432692244648933\n",
      "[step: 9640] loss: 0.010239114053547382\n",
      "[step: 9641] loss: 0.00994876492768526\n",
      "[step: 9642] loss: 0.009952962398529053\n",
      "[step: 9643] loss: 0.010099147446453571\n",
      "[step: 9644] loss: 0.01014886423945427\n",
      "[step: 9645] loss: 0.010003558360040188\n",
      "[step: 9646] loss: 0.009983948431909084\n",
      "[step: 9647] loss: 0.010007362812757492\n",
      "[step: 9648] loss: 0.009984699077904224\n",
      "[step: 9649] loss: 0.009936830960214138\n",
      "[step: 9650] loss: 0.009902598336338997\n",
      "[step: 9651] loss: 0.009928219020366669\n",
      "[step: 9652] loss: 0.009899420663714409\n",
      "[step: 9653] loss: 0.009882324375212193\n",
      "[step: 9654] loss: 0.009848464280366898\n",
      "[step: 9655] loss: 0.009818539023399353\n",
      "[step: 9656] loss: 0.009818905033171177\n",
      "[step: 9657] loss: 0.009844735264778137\n",
      "[step: 9658] loss: 0.009805811569094658\n",
      "[step: 9659] loss: 0.009779417887330055\n",
      "[step: 9660] loss: 0.009792168624699116\n",
      "[step: 9661] loss: 0.009766105562448502\n",
      "[step: 9662] loss: 0.00975888967514038\n",
      "[step: 9663] loss: 0.009759737178683281\n",
      "[step: 9664] loss: 0.009734290651977062\n",
      "[step: 9665] loss: 0.009739112108945847\n",
      "[step: 9666] loss: 0.009724345989525318\n",
      "[step: 9667] loss: 0.009704144671559334\n",
      "[step: 9668] loss: 0.009711804799735546\n",
      "[step: 9669] loss: 0.009693420492112637\n",
      "[step: 9670] loss: 0.009692271240055561\n",
      "[step: 9671] loss: 0.009675581008195877\n",
      "[step: 9672] loss: 0.00968090444803238\n",
      "[step: 9673] loss: 0.009664737619459629\n",
      "[step: 9674] loss: 0.0096613559871912\n",
      "[step: 9675] loss: 0.00965251587331295\n",
      "[step: 9676] loss: 0.009647871367633343\n",
      "[step: 9677] loss: 0.00963969249278307\n",
      "[step: 9678] loss: 0.009638212621212006\n",
      "[step: 9679] loss: 0.009626543149352074\n",
      "[step: 9680] loss: 0.009618124924600124\n",
      "[step: 9681] loss: 0.00961109809577465\n",
      "[step: 9682] loss: 0.009606652893126011\n",
      "[step: 9683] loss: 0.009599640034139156\n",
      "[step: 9684] loss: 0.009596816264092922\n",
      "[step: 9685] loss: 0.00958744902163744\n",
      "[step: 9686] loss: 0.009582019411027431\n",
      "[step: 9687] loss: 0.009574850089848042\n",
      "[step: 9688] loss: 0.009570162743330002\n",
      "[step: 9689] loss: 0.009561733342707157\n",
      "[step: 9690] loss: 0.009558231569826603\n",
      "[step: 9691] loss: 0.009550126269459724\n",
      "[step: 9692] loss: 0.009544707834720612\n",
      "[step: 9693] loss: 0.009538098238408566\n",
      "[step: 9694] loss: 0.009531039744615555\n",
      "[step: 9695] loss: 0.009525667876005173\n",
      "[step: 9696] loss: 0.009519004262983799\n",
      "[step: 9697] loss: 0.009513760916888714\n",
      "[step: 9698] loss: 0.009506339207291603\n",
      "[step: 9699] loss: 0.009500770829617977\n",
      "[step: 9700] loss: 0.00949426181614399\n",
      "[step: 9701] loss: 0.009487761184573174\n",
      "[step: 9702] loss: 0.009482056833803654\n",
      "[step: 9703] loss: 0.00947494525462389\n",
      "[step: 9704] loss: 0.009468997828662395\n",
      "[step: 9705] loss: 0.009462960995733738\n",
      "[step: 9706] loss: 0.009456281550228596\n",
      "[step: 9707] loss: 0.009450521320104599\n",
      "[step: 9708] loss: 0.009444188326597214\n",
      "[step: 9709] loss: 0.00943789817392826\n",
      "[step: 9710] loss: 0.009432083927094936\n",
      "[step: 9711] loss: 0.009425616823136806\n",
      "[step: 9712] loss: 0.009419655427336693\n",
      "[step: 9713] loss: 0.00941381324082613\n",
      "[step: 9714] loss: 0.009407592937350273\n",
      "[step: 9715] loss: 0.009401485323905945\n",
      "[step: 9716] loss: 0.009395652450621128\n",
      "[step: 9717] loss: 0.009389619342982769\n",
      "[step: 9718] loss: 0.009383614175021648\n",
      "[step: 9719] loss: 0.009377812966704369\n",
      "[step: 9720] loss: 0.009371908381581306\n",
      "[step: 9721] loss: 0.009365875273942947\n",
      "[step: 9722] loss: 0.009360067546367645\n",
      "[step: 9723] loss: 0.009354259818792343\n",
      "[step: 9724] loss: 0.009348277933895588\n",
      "[step: 9725] loss: 0.009342408739030361\n",
      "[step: 9726] loss: 0.009336592629551888\n",
      "[step: 9727] loss: 0.009330776520073414\n",
      "[step: 9728] loss: 0.009324881248176098\n",
      "[step: 9729] loss: 0.009318973869085312\n",
      "[step: 9730] loss: 0.009313168935477734\n",
      "[step: 9731] loss: 0.009307348169386387\n",
      "[step: 9732] loss: 0.009301483631134033\n",
      "[step: 9733] loss: 0.009295589290559292\n",
      "[step: 9734] loss: 0.009289705194532871\n",
      "[step: 9735] loss: 0.009283842518925667\n",
      "[step: 9736] loss: 0.009277984499931335\n",
      "[step: 9737] loss: 0.009272152557969093\n",
      "[step: 9738] loss: 0.009266325272619724\n",
      "[step: 9739] loss: 0.009260504506528378\n",
      "[step: 9740] loss: 0.009254733100533485\n",
      "[step: 9741] loss: 0.009249028749763966\n",
      "[step: 9742] loss: 0.009243610315024853\n",
      "[step: 9743] loss: 0.00923863984644413\n",
      "[step: 9744] loss: 0.009236150421202183\n",
      "[step: 9745] loss: 0.009235811419785023\n",
      "[step: 9746] loss: 0.009256364777684212\n",
      "[step: 9747] loss: 0.009242601692676544\n",
      "[step: 9748] loss: 0.009262696839869022\n",
      "[step: 9749] loss: 0.009209084324538708\n",
      "[step: 9750] loss: 0.009213907644152641\n",
      "[step: 9751] loss: 0.009334759786725044\n",
      "[step: 9752] loss: 0.009312896989285946\n",
      "[step: 9753] loss: 0.009428194724023342\n",
      "[step: 9754] loss: 0.009320310316979885\n",
      "[step: 9755] loss: 0.009543832391500473\n",
      "[step: 9756] loss: 0.009773338213562965\n",
      "[step: 9757] loss: 0.009399407543241978\n",
      "[step: 9758] loss: 0.009746133349835873\n",
      "[step: 9759] loss: 0.009432840161025524\n",
      "[step: 9760] loss: 0.009526888839900494\n",
      "[step: 9761] loss: 0.009333808906376362\n",
      "[step: 9762] loss: 0.009627420455217361\n",
      "[step: 9763] loss: 0.009446966461837292\n",
      "[step: 9764] loss: 0.009519046172499657\n",
      "[step: 9765] loss: 0.009368172846734524\n",
      "[step: 9766] loss: 0.009261145256459713\n",
      "[step: 9767] loss: 0.009265084750950336\n",
      "[step: 9768] loss: 0.009227199479937553\n",
      "[step: 9769] loss: 0.009313412010669708\n",
      "[step: 9770] loss: 0.009319044649600983\n",
      "[step: 9771] loss: 0.009226915426552296\n",
      "[step: 9772] loss: 0.009296663105487823\n",
      "[step: 9773] loss: 0.009325714781880379\n",
      "[step: 9774] loss: 0.009318978525698185\n",
      "[step: 9775] loss: 0.009246439673006535\n",
      "[step: 9776] loss: 0.009244730696082115\n",
      "[step: 9777] loss: 0.00916959811002016\n",
      "[step: 9778] loss: 0.009187241084873676\n",
      "[step: 9779] loss: 0.009233043529093266\n",
      "[step: 9780] loss: 0.009126081131398678\n",
      "[step: 9781] loss: 0.009262487292289734\n",
      "[step: 9782] loss: 0.009337742812931538\n",
      "[step: 9783] loss: 0.009156768210232258\n",
      "[step: 9784] loss: 0.009352977387607098\n",
      "[step: 9785] loss: 0.009302525781095028\n",
      "[step: 9786] loss: 0.009261254221200943\n",
      "[step: 9787] loss: 0.009300313889980316\n",
      "[step: 9788] loss: 0.00911677721887827\n",
      "[step: 9789] loss: 0.009115620516240597\n",
      "[step: 9790] loss: 0.009195134043693542\n",
      "[step: 9791] loss: 0.009174779988825321\n",
      "[step: 9792] loss: 0.009135765954852104\n",
      "[step: 9793] loss: 0.009173397906124592\n",
      "[step: 9794] loss: 0.009057151153683662\n",
      "[step: 9795] loss: 0.009080938994884491\n",
      "[step: 9796] loss: 0.009129500947892666\n",
      "[step: 9797] loss: 0.00917633343487978\n",
      "[step: 9798] loss: 0.009073842316865921\n",
      "[step: 9799] loss: 0.009279240854084492\n",
      "[step: 9800] loss: 0.009407556615769863\n",
      "[step: 9801] loss: 0.009464339353144169\n",
      "[step: 9802] loss: 0.009312145411968231\n",
      "[step: 9803] loss: 0.009365118108689785\n",
      "[step: 9804] loss: 0.009722537361085415\n",
      "[step: 9805] loss: 0.009755821898579597\n",
      "[step: 9806] loss: 0.009187759831547737\n",
      "[step: 9807] loss: 0.009237958118319511\n",
      "[step: 9808] loss: 0.009877460077404976\n",
      "[step: 9809] loss: 0.009756744839251041\n",
      "[step: 9810] loss: 0.009331094101071358\n",
      "[step: 9811] loss: 0.009833204559981823\n",
      "[step: 9812] loss: 0.009725412353873253\n",
      "[step: 9813] loss: 0.009496239945292473\n",
      "[step: 9814] loss: 0.009664476849138737\n",
      "[step: 9815] loss: 0.009596516378223896\n",
      "[step: 9816] loss: 0.00949239544570446\n",
      "[step: 9817] loss: 0.009475252591073513\n",
      "[step: 9818] loss: 0.009406449273228645\n",
      "[step: 9819] loss: 0.009321784600615501\n",
      "[step: 9820] loss: 0.009492545388638973\n",
      "[step: 9821] loss: 0.009188506752252579\n",
      "[step: 9822] loss: 0.009421685710549355\n",
      "[step: 9823] loss: 0.009365802630782127\n",
      "[step: 9824] loss: 0.009253124706447124\n",
      "[step: 9825] loss: 0.009274007752537727\n",
      "[step: 9826] loss: 0.009112951345741749\n",
      "[step: 9827] loss: 0.009271041490137577\n",
      "[step: 9828] loss: 0.009152967482805252\n",
      "[step: 9829] loss: 0.009211331605911255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 9830] loss: 0.009108081459999084\n",
      "[step: 9831] loss: 0.009061555378139019\n",
      "[step: 9832] loss: 0.009103366173803806\n",
      "[step: 9833] loss: 0.009083697572350502\n",
      "[step: 9834] loss: 0.009066402912139893\n",
      "[step: 9835] loss: 0.008990832604467869\n",
      "[step: 9836] loss: 0.009044984355568886\n",
      "[step: 9837] loss: 0.009010440669953823\n",
      "[step: 9838] loss: 0.009037967771291733\n",
      "[step: 9839] loss: 0.008980363607406616\n",
      "[step: 9840] loss: 0.008988345973193645\n",
      "[step: 9841] loss: 0.008968329057097435\n",
      "[step: 9842] loss: 0.008982421830296516\n",
      "[step: 9843] loss: 0.008936611004173756\n",
      "[step: 9844] loss: 0.008961204439401627\n",
      "[step: 9845] loss: 0.00893311481922865\n",
      "[step: 9846] loss: 0.008935796096920967\n",
      "[step: 9847] loss: 0.008900502696633339\n",
      "[step: 9848] loss: 0.00891677476465702\n",
      "[step: 9849] loss: 0.008899522013962269\n",
      "[step: 9850] loss: 0.008887370117008686\n",
      "[step: 9851] loss: 0.00888810120522976\n",
      "[step: 9852] loss: 0.008865734562277794\n",
      "[step: 9853] loss: 0.008872248232364655\n",
      "[step: 9854] loss: 0.008850714191794395\n",
      "[step: 9855] loss: 0.008853506296873093\n",
      "[step: 9856] loss: 0.008854980580508709\n",
      "[step: 9857] loss: 0.008835936896502972\n",
      "[step: 9858] loss: 0.008845018222928047\n",
      "[step: 9859] loss: 0.00882715079933405\n",
      "[step: 9860] loss: 0.008818347938358784\n",
      "[step: 9861] loss: 0.00881176721304655\n",
      "[step: 9862] loss: 0.00879146158695221\n",
      "[step: 9863] loss: 0.00878671184182167\n",
      "[step: 9864] loss: 0.008773226290941238\n",
      "[step: 9865] loss: 0.008766806684434414\n",
      "[step: 9866] loss: 0.008754630573093891\n",
      "[step: 9867] loss: 0.008752448484301567\n",
      "[step: 9868] loss: 0.008743557147681713\n",
      "[step: 9869] loss: 0.008740261197090149\n",
      "[step: 9870] loss: 0.00872950628399849\n",
      "[step: 9871] loss: 0.008718856610357761\n",
      "[step: 9872] loss: 0.00870835967361927\n",
      "[step: 9873] loss: 0.008701970800757408\n",
      "[step: 9874] loss: 0.008694027550518513\n",
      "[step: 9875] loss: 0.008684300817549229\n",
      "[step: 9876] loss: 0.008674204349517822\n",
      "[step: 9877] loss: 0.008668188005685806\n",
      "[step: 9878] loss: 0.008659644983708858\n",
      "[step: 9879] loss: 0.00865095853805542\n",
      "[step: 9880] loss: 0.008643725886940956\n",
      "[step: 9881] loss: 0.00863830465823412\n",
      "[step: 9882] loss: 0.008633497171103954\n",
      "[step: 9883] loss: 0.008625100366771221\n",
      "[step: 9884] loss: 0.008619347587227821\n",
      "[step: 9885] loss: 0.008615819737315178\n",
      "[step: 9886] loss: 0.008613753132522106\n",
      "[step: 9887] loss: 0.00861397571861744\n",
      "[step: 9888] loss: 0.008633611723780632\n",
      "[step: 9889] loss: 0.008665494620800018\n",
      "[step: 9890] loss: 0.008714357390999794\n",
      "[step: 9891] loss: 0.008750775828957558\n",
      "[step: 9892] loss: 0.008753146044909954\n",
      "[step: 9893] loss: 0.008700239472091198\n",
      "[step: 9894] loss: 0.008642195723950863\n",
      "[step: 9895] loss: 0.008653347380459309\n",
      "[step: 9896] loss: 0.008642866276204586\n",
      "[step: 9897] loss: 0.008573292754590511\n",
      "[step: 9898] loss: 0.00856601633131504\n",
      "[step: 9899] loss: 0.00855339877307415\n",
      "[step: 9900] loss: 0.008565924130380154\n",
      "[step: 9901] loss: 0.008601265028119087\n",
      "[step: 9902] loss: 0.008583705872297287\n",
      "[step: 9903] loss: 0.008566425181925297\n",
      "[step: 9904] loss: 0.00858872290700674\n",
      "[step: 9905] loss: 0.008611285127699375\n",
      "[step: 9906] loss: 0.008540509268641472\n",
      "[step: 9907] loss: 0.00857599824666977\n",
      "[step: 9908] loss: 0.008715225383639336\n",
      "[step: 9909] loss: 0.008771920576691628\n",
      "[step: 9910] loss: 0.008736792020499706\n",
      "[step: 9911] loss: 0.008760601282119751\n",
      "[step: 9912] loss: 0.008860823698341846\n",
      "[step: 9913] loss: 0.008718050085008144\n",
      "[step: 9914] loss: 0.008939026854932308\n",
      "[step: 9915] loss: 0.008988110348582268\n",
      "[step: 9916] loss: 0.008788461796939373\n",
      "[step: 9917] loss: 0.008849074132740498\n",
      "[step: 9918] loss: 0.009414460510015488\n",
      "[step: 9919] loss: 0.009104625321924686\n",
      "[step: 9920] loss: 0.008596090599894524\n",
      "[step: 9921] loss: 0.00980179850012064\n",
      "[step: 9922] loss: 0.009676586836576462\n",
      "[step: 9923] loss: 0.010139028541743755\n",
      "[step: 9924] loss: 0.009906175546348095\n",
      "[step: 9925] loss: 0.0098146116361022\n",
      "[step: 9926] loss: 0.009925970807671547\n",
      "[step: 9927] loss: 0.010047534480690956\n",
      "[step: 9928] loss: 0.009482311084866524\n",
      "[step: 9929] loss: 0.009583035483956337\n",
      "[step: 9930] loss: 0.009416294284164906\n",
      "[step: 9931] loss: 0.009431002661585808\n",
      "[step: 9932] loss: 0.009413298219442368\n",
      "[step: 9933] loss: 0.009510627947747707\n",
      "[step: 9934] loss: 0.009477945975959301\n",
      "[step: 9935] loss: 0.00943733099848032\n",
      "[step: 9936] loss: 0.009387572295963764\n",
      "[step: 9937] loss: 0.009357864037156105\n",
      "[step: 9938] loss: 0.009260800667107105\n",
      "[step: 9939] loss: 0.00929911620914936\n",
      "[step: 9940] loss: 0.009138878434896469\n",
      "[step: 9941] loss: 0.009131410159170628\n",
      "[step: 9942] loss: 0.00912067387253046\n",
      "[step: 9943] loss: 0.009078631177544594\n",
      "[step: 9944] loss: 0.009039380587637424\n",
      "[step: 9945] loss: 0.009012504480779171\n",
      "[step: 9946] loss: 0.008990313857793808\n",
      "[step: 9947] loss: 0.00897720456123352\n",
      "[step: 9948] loss: 0.008898751810193062\n",
      "[step: 9949] loss: 0.008859202265739441\n",
      "[step: 9950] loss: 0.008851977996528149\n",
      "[step: 9951] loss: 0.008840094320476055\n",
      "[step: 9952] loss: 0.008806427009403706\n",
      "[step: 9953] loss: 0.008957462385296822\n",
      "[step: 9954] loss: 0.009191901423037052\n",
      "[step: 9955] loss: 0.009538045153021812\n",
      "[step: 9956] loss: 0.009304885752499104\n",
      "[step: 9957] loss: 0.009244292043149471\n",
      "[step: 9958] loss: 0.009333638474345207\n",
      "[step: 9959] loss: 0.009349219501018524\n",
      "[step: 9960] loss: 0.009290908463299274\n",
      "[step: 9961] loss: 0.009189841337502003\n",
      "[step: 9962] loss: 0.00924152135848999\n",
      "[step: 9963] loss: 0.009125172160565853\n",
      "[step: 9964] loss: 0.00909800548106432\n",
      "[step: 9965] loss: 0.00904568750411272\n",
      "[step: 9966] loss: 0.009073605760931969\n",
      "[step: 9967] loss: 0.009082673117518425\n",
      "[step: 9968] loss: 0.009082269854843616\n",
      "[step: 9969] loss: 0.009082804434001446\n",
      "[step: 9970] loss: 0.009036831557750702\n",
      "[step: 9971] loss: 0.009015320800244808\n",
      "[step: 9972] loss: 0.008966505527496338\n",
      "[step: 9973] loss: 0.008944188244640827\n",
      "[step: 9974] loss: 0.008904078975319862\n",
      "[step: 9975] loss: 0.008892765268683434\n",
      "[step: 9976] loss: 0.008865885436534882\n",
      "[step: 9977] loss: 0.008855200372636318\n",
      "[step: 9978] loss: 0.008839761838316917\n",
      "[step: 9979] loss: 0.008815154433250427\n",
      "[step: 9980] loss: 0.008803335018455982\n",
      "[step: 9981] loss: 0.008789880201220512\n",
      "[step: 9982] loss: 0.008772727102041245\n",
      "[step: 9983] loss: 0.008796097710728645\n",
      "[step: 9984] loss: 0.008787786588072777\n",
      "[step: 9985] loss: 0.00876978412270546\n",
      "[step: 9986] loss: 0.008749520406126976\n",
      "[step: 9987] loss: 0.008740187622606754\n",
      "[step: 9988] loss: 0.008728967979550362\n",
      "[step: 9989] loss: 0.00872874353080988\n",
      "[step: 9990] loss: 0.008709598332643509\n",
      "[step: 9991] loss: 0.008702860213816166\n",
      "[step: 9992] loss: 0.008697145618498325\n",
      "[step: 9993] loss: 0.008677332662045956\n",
      "[step: 9994] loss: 0.008663943037390709\n",
      "[step: 9995] loss: 0.008655070327222347\n",
      "[step: 9996] loss: 0.008646014146506786\n",
      "[step: 9997] loss: 0.008637930266559124\n",
      "[step: 9998] loss: 0.008631684817373753\n",
      "[step: 9999] loss: 0.008617219515144825\n",
      "[step: 10000] loss: 0.008605229668319225\n",
      "[step: 10001] loss: 0.00860506296157837\n",
      "[step: 10002] loss: 0.008597631938755512\n",
      "[step: 10003] loss: 0.008582581765949726\n",
      "[step: 10004] loss: 0.00858386605978012\n",
      "[step: 10005] loss: 0.008579242043197155\n",
      "[step: 10006] loss: 0.008564841002225876\n",
      "[step: 10007] loss: 0.008559604175388813\n",
      "[step: 10008] loss: 0.00855775736272335\n",
      "[step: 10009] loss: 0.008541559800505638\n",
      "[step: 10010] loss: 0.008538225665688515\n",
      "[step: 10011] loss: 0.008537815883755684\n",
      "[step: 10012] loss: 0.008522601798176765\n",
      "[step: 10013] loss: 0.008516713976860046\n",
      "[step: 10014] loss: 0.008517280220985413\n",
      "[step: 10015] loss: 0.008501742966473103\n",
      "[step: 10016] loss: 0.008492249995470047\n",
      "[step: 10017] loss: 0.00849269237369299\n",
      "[step: 10018] loss: 0.008481358177959919\n",
      "[step: 10019] loss: 0.008472833782434464\n",
      "[step: 10020] loss: 0.008468851447105408\n",
      "[step: 10021] loss: 0.008462312631309032\n",
      "[step: 10022] loss: 0.008455069735646248\n",
      "[step: 10023] loss: 0.008448039181530476\n",
      "[step: 10024] loss: 0.008443089202046394\n",
      "[step: 10025] loss: 0.008437109179794788\n",
      "[step: 10026] loss: 0.008430278860032558\n",
      "[step: 10027] loss: 0.008423835970461369\n",
      "[step: 10028] loss: 0.008417828008532524\n",
      "[step: 10029] loss: 0.008412052877247334\n",
      "[step: 10030] loss: 0.008406363427639008\n",
      "[step: 10031] loss: 0.008401227183640003\n",
      "[step: 10032] loss: 0.008394752629101276\n",
      "[step: 10033] loss: 0.008388387970626354\n",
      "[step: 10034] loss: 0.008383420296013355\n",
      "[step: 10035] loss: 0.00837845541536808\n",
      "[step: 10036] loss: 0.008373222313821316\n",
      "[step: 10037] loss: 0.008367183618247509\n",
      "[step: 10038] loss: 0.008361337706446648\n",
      "[step: 10039] loss: 0.008356411010026932\n",
      "[step: 10040] loss: 0.008351323194801807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 10041] loss: 0.008346081711351871\n",
      "[step: 10042] loss: 0.008340814150869846\n",
      "[step: 10043] loss: 0.008335672318935394\n",
      "[step: 10044] loss: 0.008330803364515305\n",
      "[step: 10045] loss: 0.008325549773871899\n",
      "[step: 10046] loss: 0.00832018069922924\n",
      "[step: 10047] loss: 0.0083150714635849\n",
      "[step: 10048] loss: 0.008310111239552498\n",
      "[step: 10049] loss: 0.008305149152874947\n",
      "[step: 10050] loss: 0.008300205692648888\n",
      "[step: 10051] loss: 0.008295190520584583\n",
      "[step: 10052] loss: 0.008290325291454792\n",
      "[step: 10053] loss: 0.00828564167022705\n",
      "[step: 10054] loss: 0.008280837908387184\n",
      "[step: 10055] loss: 0.008276102133095264\n",
      "[step: 10056] loss: 0.008271496742963791\n",
      "[step: 10057] loss: 0.00826717633754015\n",
      "[step: 10058] loss: 0.008263230323791504\n",
      "[step: 10059] loss: 0.00826046522706747\n",
      "[step: 10060] loss: 0.008258635178208351\n",
      "[step: 10061] loss: 0.008262778632342815\n",
      "[step: 10062] loss: 0.00826779194176197\n",
      "[step: 10063] loss: 0.008299652487039566\n",
      "[step: 10064] loss: 0.008305751718580723\n",
      "[step: 10065] loss: 0.008380331099033356\n",
      "[step: 10066] loss: 0.008323075249791145\n",
      "[step: 10067] loss: 0.008317572064697742\n",
      "[step: 10068] loss: 0.008268001489341259\n",
      "[step: 10069] loss: 0.008233551867306232\n",
      "[step: 10070] loss: 0.008210466243326664\n",
      "[step: 10071] loss: 0.008204230107367039\n",
      "[step: 10072] loss: 0.008207133039832115\n",
      "[step: 10073] loss: 0.008209986612200737\n",
      "[step: 10074] loss: 0.008213719353079796\n",
      "[step: 10075] loss: 0.008224038407206535\n",
      "[step: 10076] loss: 0.008276362903416157\n",
      "[step: 10077] loss: 0.008249959908425808\n",
      "[step: 10078] loss: 0.00826517678797245\n",
      "[step: 10079] loss: 0.008211260661482811\n",
      "[step: 10080] loss: 0.008184452541172504\n",
      "[step: 10081] loss: 0.008171742781996727\n",
      "[step: 10082] loss: 0.008164397440850735\n",
      "[step: 10083] loss: 0.008174036629498005\n",
      "[step: 10084] loss: 0.008196323178708553\n",
      "[step: 10085] loss: 0.008284284733235836\n",
      "[step: 10086] loss: 0.0082905488088727\n",
      "[step: 10087] loss: 0.00835471786558628\n",
      "[step: 10088] loss: 0.00827212817966938\n",
      "[step: 10089] loss: 0.008207280188798904\n",
      "[step: 10090] loss: 0.008151780813932419\n",
      "[step: 10091] loss: 0.008163287304341793\n",
      "[step: 10092] loss: 0.008258668705821037\n",
      "[step: 10093] loss: 0.008275097236037254\n",
      "[step: 10094] loss: 0.008314302191138268\n",
      "[step: 10095] loss: 0.008254535496234894\n",
      "[step: 10096] loss: 0.00816816184669733\n",
      "[step: 10097] loss: 0.008123192004859447\n",
      "[step: 10098] loss: 0.008109589107334614\n",
      "[step: 10099] loss: 0.008142963983118534\n",
      "[step: 10100] loss: 0.008153649978339672\n",
      "[step: 10101] loss: 0.008126148022711277\n",
      "[step: 10102] loss: 0.008106924593448639\n",
      "[step: 10103] loss: 0.008090684190392494\n",
      "[step: 10104] loss: 0.00809323787689209\n",
      "[step: 10105] loss: 0.008088122121989727\n",
      "[step: 10106] loss: 0.008087566122412682\n",
      "[step: 10107] loss: 0.008076136000454426\n",
      "[step: 10108] loss: 0.00806780718266964\n",
      "[step: 10109] loss: 0.008071991614997387\n",
      "[step: 10110] loss: 0.00806425791233778\n",
      "[step: 10111] loss: 0.008065329864621162\n",
      "[step: 10112] loss: 0.008052755147218704\n",
      "[step: 10113] loss: 0.008045990020036697\n",
      "[step: 10114] loss: 0.008031127043068409\n",
      "[step: 10115] loss: 0.008025501854717731\n",
      "[step: 10116] loss: 0.008021233603358269\n",
      "[step: 10117] loss: 0.00801937934011221\n",
      "[step: 10118] loss: 0.00802153255790472\n",
      "[step: 10119] loss: 0.008013402111828327\n",
      "[step: 10120] loss: 0.00801200233399868\n",
      "[step: 10121] loss: 0.00800346489995718\n",
      "[step: 10122] loss: 0.007997609674930573\n",
      "[step: 10123] loss: 0.007988405413925648\n",
      "[step: 10124] loss: 0.007986964657902718\n",
      "[step: 10125] loss: 0.007982295006513596\n",
      "[step: 10126] loss: 0.007984457537531853\n",
      "[step: 10127] loss: 0.007995815947651863\n",
      "[step: 10128] loss: 0.008026103489100933\n",
      "[step: 10129] loss: 0.008078603073954582\n",
      "[step: 10130] loss: 0.008268576115369797\n",
      "[step: 10131] loss: 0.008241266943514347\n",
      "[step: 10132] loss: 0.008343355730175972\n",
      "[step: 10133] loss: 0.00830625370144844\n",
      "[step: 10134] loss: 0.008352775126695633\n",
      "[step: 10135] loss: 0.008565536700189114\n",
      "[step: 10136] loss: 0.00900234840810299\n",
      "[step: 10137] loss: 0.00936159398406744\n",
      "[step: 10138] loss: 0.009655985049903393\n",
      "[step: 10139] loss: 0.010826134122908115\n",
      "[step: 10140] loss: 0.009947019629180431\n",
      "[step: 10141] loss: 0.024325957521796227\n",
      "[step: 10142] loss: 0.014360531233251095\n",
      "[step: 10143] loss: 0.014157522469758987\n",
      "[step: 10144] loss: 0.014118155464529991\n",
      "[step: 10145] loss: 0.012233896180987358\n",
      "[step: 10146] loss: 0.013484574854373932\n",
      "[step: 10147] loss: 0.013887668959796429\n",
      "[step: 10148] loss: 0.011667697690427303\n",
      "[step: 10149] loss: 0.012334109283983707\n",
      "[step: 10150] loss: 0.01206200011074543\n",
      "[step: 10151] loss: 0.013072799891233444\n",
      "[step: 10152] loss: 0.011932635679841042\n",
      "[step: 10153] loss: 0.01202451903373003\n",
      "[step: 10154] loss: 0.011150062084197998\n",
      "[step: 10155] loss: 0.011644426733255386\n",
      "[step: 10156] loss: 0.011147798970341682\n",
      "[step: 10157] loss: 0.011689353734254837\n",
      "[step: 10158] loss: 0.011242236942052841\n",
      "[step: 10159] loss: 0.011485149152576923\n",
      "[step: 10160] loss: 0.010985909961163998\n",
      "[step: 10161] loss: 0.011102493852376938\n",
      "[step: 10162] loss: 0.010775957256555557\n",
      "[step: 10163] loss: 0.010921423323452473\n",
      "[step: 10164] loss: 0.010767467319965363\n",
      "[step: 10165] loss: 0.010897979140281677\n",
      "[step: 10166] loss: 0.010803325101733208\n",
      "[step: 10167] loss: 0.010846665129065514\n",
      "[step: 10168] loss: 0.010752270929515362\n",
      "[step: 10169] loss: 0.010702871717512608\n",
      "[step: 10170] loss: 0.01058696024119854\n",
      "[step: 10171] loss: 0.010493234731256962\n",
      "[step: 10172] loss: 0.010400932282209396\n",
      "[step: 10173] loss: 0.01033227238804102\n",
      "[step: 10174] loss: 0.010318635031580925\n",
      "[step: 10175] loss: 0.010297564789652824\n",
      "[step: 10176] loss: 0.01030802633613348\n",
      "[step: 10177] loss: 0.010216119699180126\n",
      "[step: 10178] loss: 0.010148836299777031\n",
      "[step: 10179] loss: 0.010043206624686718\n",
      "[step: 10180] loss: 0.010037176311016083\n",
      "[step: 10181] loss: 0.010011183097958565\n",
      "[step: 10182] loss: 0.010033082216978073\n",
      "[step: 10183] loss: 0.00996576901525259\n",
      "[step: 10184] loss: 0.009909302927553654\n",
      "[step: 10185] loss: 0.009846375323832035\n",
      "[step: 10186] loss: 0.009877770207822323\n",
      "[step: 10187] loss: 0.009862671606242657\n",
      "[step: 10188] loss: 0.009821906685829163\n",
      "[step: 10189] loss: 0.009757750667631626\n",
      "[step: 10190] loss: 0.00977550819516182\n",
      "[step: 10191] loss: 0.009764952585101128\n",
      "[step: 10192] loss: 0.00973503291606903\n",
      "[step: 10193] loss: 0.009693835861980915\n",
      "[step: 10194] loss: 0.009696747176349163\n",
      "[step: 10195] loss: 0.009693292900919914\n",
      "[step: 10196] loss: 0.009655396454036236\n",
      "[step: 10197] loss: 0.00964012648910284\n",
      "[step: 10198] loss: 0.009631949476897717\n",
      "[step: 10199] loss: 0.009617256931960583\n",
      "[step: 10200] loss: 0.009582746773958206\n",
      "[step: 10201] loss: 0.009576383046805859\n",
      "[step: 10202] loss: 0.00955927837640047\n",
      "[step: 10203] loss: 0.009542608633637428\n",
      "[step: 10204] loss: 0.009517184458673\n",
      "[step: 10205] loss: 0.00951228104531765\n",
      "[step: 10206] loss: 0.009491292759776115\n",
      "[step: 10207] loss: 0.009470498189330101\n",
      "[step: 10208] loss: 0.009454646147787571\n",
      "[step: 10209] loss: 0.009440436959266663\n",
      "[step: 10210] loss: 0.009422226808965206\n",
      "[step: 10211] loss: 0.009400795213878155\n",
      "[step: 10212] loss: 0.009390324354171753\n",
      "[step: 10213] loss: 0.009372901171445847\n",
      "[step: 10214] loss: 0.009356318973004818\n",
      "[step: 10215] loss: 0.009339750744402409\n",
      "[step: 10216] loss: 0.009328027255833149\n",
      "[step: 10217] loss: 0.009312739595770836\n",
      "[step: 10218] loss: 0.009296986274421215\n",
      "[step: 10219] loss: 0.009285573847591877\n",
      "[step: 10220] loss: 0.009272970259189606\n",
      "[step: 10221] loss: 0.009259945712983608\n",
      "[step: 10222] loss: 0.009246889501810074\n",
      "[step: 10223] loss: 0.009236816316843033\n",
      "[step: 10224] loss: 0.009224740788340569\n",
      "[step: 10225] loss: 0.009213428944349289\n",
      "[step: 10226] loss: 0.009202895686030388\n",
      "[step: 10227] loss: 0.009193194098770618\n",
      "[step: 10228] loss: 0.00918265338987112\n",
      "[step: 10229] loss: 0.009172427468001842\n",
      "[step: 10230] loss: 0.009163525886833668\n",
      "[step: 10231] loss: 0.009153058752417564\n",
      "[step: 10232] loss: 0.009143664501607418\n",
      "[step: 10233] loss: 0.009134195744991302\n",
      "[step: 10234] loss: 0.009124626405537128\n",
      "[step: 10235] loss: 0.009115077555179596\n",
      "[step: 10236] loss: 0.009105991572141647\n",
      "[step: 10237] loss: 0.009096775203943253\n",
      "[step: 10238] loss: 0.009087422862648964\n",
      "[step: 10239] loss: 0.00907884631305933\n",
      "[step: 10240] loss: 0.009069784544408321\n",
      "[step: 10241] loss: 0.009060749784111977\n",
      "[step: 10242] loss: 0.009051778353750706\n",
      "[step: 10243] loss: 0.009042737074196339\n",
      "[step: 10244] loss: 0.009033163078129292\n",
      "[step: 10245] loss: 0.009023685939610004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 10246] loss: 0.009014204144477844\n",
      "[step: 10247] loss: 0.009004264138638973\n",
      "[step: 10248] loss: 0.008994375355541706\n",
      "[step: 10249] loss: 0.00898441020399332\n",
      "[step: 10250] loss: 0.008974259719252586\n",
      "[step: 10251] loss: 0.008963930420577526\n",
      "[step: 10252] loss: 0.008953703567385674\n",
      "[step: 10253] loss: 0.008943254128098488\n",
      "[step: 10254] loss: 0.008932743221521378\n",
      "[step: 10255] loss: 0.008922232314944267\n",
      "[step: 10256] loss: 0.00891158077865839\n",
      "[step: 10257] loss: 0.008900885470211506\n",
      "[step: 10258] loss: 0.008890161290764809\n",
      "[step: 10259] loss: 0.008879395201802254\n",
      "[step: 10260] loss: 0.008868573233485222\n",
      "[step: 10261] loss: 0.008857804350554943\n",
      "[step: 10262] loss: 0.008846912533044815\n",
      "[step: 10263] loss: 0.008835975080728531\n",
      "[step: 10264] loss: 0.008824953809380531\n",
      "[step: 10265] loss: 0.008813772350549698\n",
      "[step: 10266] loss: 0.008802453055977821\n",
      "[step: 10267] loss: 0.008790983818471432\n",
      "[step: 10268] loss: 0.008779358118772507\n",
      "[step: 10269] loss: 0.008767549879848957\n",
      "[step: 10270] loss: 0.008755589835345745\n",
      "[step: 10271] loss: 0.008743427693843842\n",
      "[step: 10272] loss: 0.008731096051633358\n",
      "[step: 10273] loss: 0.008718549273908138\n",
      "[step: 10274] loss: 0.008705796673893929\n",
      "[step: 10275] loss: 0.008692827075719833\n",
      "[step: 10276] loss: 0.008679618127644062\n",
      "[step: 10277] loss: 0.00866618100553751\n",
      "[step: 10278] loss: 0.0086525222286582\n",
      "[step: 10279] loss: 0.008638648316264153\n",
      "[step: 10280] loss: 0.008624561131000519\n",
      "[step: 10281] loss: 0.008610289543867111\n",
      "[step: 10282] loss: 0.008595835417509079\n",
      "[step: 10283] loss: 0.00858120433986187\n",
      "[step: 10284] loss: 0.008566404692828655\n",
      "[step: 10285] loss: 0.008551428094506264\n",
      "[step: 10286] loss: 0.008536295965313911\n",
      "[step: 10287] loss: 0.008521036244928837\n",
      "[step: 10288] loss: 0.008505833335220814\n",
      "[step: 10289] loss: 0.00849141925573349\n",
      "[step: 10290] loss: 0.00848148763179779\n",
      "[step: 10291] loss: 0.008498402312397957\n",
      "[step: 10292] loss: 0.008661556988954544\n",
      "[step: 10293] loss: 0.009717628359794617\n",
      "[step: 10294] loss: 0.009658660739660263\n",
      "[step: 10295] loss: 0.010260541923344135\n",
      "[step: 10296] loss: 0.008828317746520042\n",
      "[step: 10297] loss: 0.011305118910968304\n",
      "[step: 10298] loss: 0.011044603772461414\n",
      "[step: 10299] loss: 0.011005472391843796\n",
      "[step: 10300] loss: 0.009618195705115795\n",
      "[step: 10301] loss: 0.01060193870216608\n",
      "[step: 10302] loss: 0.00952361524105072\n",
      "[step: 10303] loss: 0.009381341747939587\n",
      "[step: 10304] loss: 0.009985570795834064\n",
      "[step: 10305] loss: 0.009015955030918121\n",
      "[step: 10306] loss: 0.009992684237658978\n",
      "[step: 10307] loss: 0.008926473557949066\n",
      "[step: 10308] loss: 0.009436272084712982\n",
      "[step: 10309] loss: 0.009272060357034206\n",
      "[step: 10310] loss: 0.009362056851387024\n",
      "[step: 10311] loss: 0.00899211224168539\n",
      "[step: 10312] loss: 0.009270992130041122\n",
      "[step: 10313] loss: 0.008881653659045696\n",
      "[step: 10314] loss: 0.00916398223489523\n",
      "[step: 10315] loss: 0.008942152373492718\n",
      "[step: 10316] loss: 0.008927148766815662\n",
      "[step: 10317] loss: 0.008819807320833206\n",
      "[step: 10318] loss: 0.009001807309687138\n",
      "[step: 10319] loss: 0.008757668547332287\n",
      "[step: 10320] loss: 0.008825804106891155\n",
      "[step: 10321] loss: 0.008650693111121655\n",
      "[step: 10322] loss: 0.008792922832071781\n",
      "[step: 10323] loss: 0.008620420470833778\n",
      "[step: 10324] loss: 0.008640818297863007\n",
      "[step: 10325] loss: 0.008568783290684223\n",
      "[step: 10326] loss: 0.008588485419750214\n",
      "[step: 10327] loss: 0.008495362475514412\n",
      "[step: 10328] loss: 0.008472195826470852\n",
      "[step: 10329] loss: 0.008442448452115059\n",
      "[step: 10330] loss: 0.008455077186226845\n",
      "[step: 10331] loss: 0.008390404284000397\n",
      "[step: 10332] loss: 0.008382799103856087\n",
      "[step: 10333] loss: 0.00839326623827219\n",
      "[step: 10334] loss: 0.00834551453590393\n",
      "[step: 10335] loss: 0.008310803212225437\n",
      "[step: 10336] loss: 0.008326869457960129\n",
      "[step: 10337] loss: 0.00830116868019104\n",
      "[step: 10338] loss: 0.008298113010823727\n",
      "[step: 10339] loss: 0.008316123858094215\n",
      "[step: 10340] loss: 0.008362806402146816\n",
      "[step: 10341] loss: 0.008368242532014847\n",
      "[step: 10342] loss: 0.00844041258096695\n",
      "[step: 10343] loss: 0.008774041198194027\n",
      "[step: 10344] loss: 0.008844149298965931\n",
      "[step: 10345] loss: 0.008643397130072117\n",
      "[step: 10346] loss: 0.008208857849240303\n",
      "[step: 10347] loss: 0.008326389826834202\n",
      "[step: 10348] loss: 0.008428998291492462\n",
      "[step: 10349] loss: 0.008505935780704021\n",
      "[step: 10350] loss: 0.008268148638308048\n",
      "[step: 10351] loss: 0.008355682715773582\n",
      "[step: 10352] loss: 0.008588014170527458\n",
      "[step: 10353] loss: 0.008748451247811317\n",
      "[step: 10354] loss: 0.008299306035041809\n",
      "[step: 10355] loss: 0.008476889692246914\n",
      "[step: 10356] loss: 0.00899238046258688\n",
      "[step: 10357] loss: 0.008349304087460041\n",
      "[step: 10358] loss: 0.00843546912074089\n",
      "[step: 10359] loss: 0.008578180335462093\n",
      "[step: 10360] loss: 0.008199947886168957\n",
      "[step: 10361] loss: 0.008574729785323143\n",
      "[step: 10362] loss: 0.008213462308049202\n",
      "[step: 10363] loss: 0.00833365973085165\n",
      "[step: 10364] loss: 0.008410911075770855\n",
      "[step: 10365] loss: 0.008218441158533096\n",
      "[step: 10366] loss: 0.00829150527715683\n",
      "[step: 10367] loss: 0.008346743881702423\n",
      "[step: 10368] loss: 0.00823007058352232\n",
      "[step: 10369] loss: 0.008176088333129883\n",
      "[step: 10370] loss: 0.008236045949161053\n",
      "[step: 10371] loss: 0.008339356631040573\n",
      "[step: 10372] loss: 0.008181563578546047\n",
      "[step: 10373] loss: 0.008081374689936638\n",
      "[step: 10374] loss: 0.00829867273569107\n",
      "[step: 10375] loss: 0.008202431723475456\n",
      "[step: 10376] loss: 0.00806841067969799\n",
      "[step: 10377] loss: 0.00816004816442728\n",
      "[step: 10378] loss: 0.00807355809956789\n",
      "[step: 10379] loss: 0.008171397261321545\n",
      "[step: 10380] loss: 0.008132376708090305\n",
      "[step: 10381] loss: 0.007991502061486244\n",
      "[step: 10382] loss: 0.008068691939115524\n",
      "[step: 10383] loss: 0.007951226085424423\n",
      "[step: 10384] loss: 0.008037972263991833\n",
      "[step: 10385] loss: 0.007941002026200294\n",
      "[step: 10386] loss: 0.007978660054504871\n",
      "[step: 10387] loss: 0.007974013686180115\n",
      "[step: 10388] loss: 0.008003110997378826\n",
      "[step: 10389] loss: 0.008275728672742844\n",
      "[step: 10390] loss: 0.008758898824453354\n",
      "[step: 10391] loss: 0.008863088674843311\n",
      "[step: 10392] loss: 0.008503655903041363\n",
      "[step: 10393] loss: 0.008205913938581944\n",
      "[step: 10394] loss: 0.008448662236332893\n",
      "[step: 10395] loss: 0.008135111071169376\n",
      "[step: 10396] loss: 0.008282070979475975\n",
      "[step: 10397] loss: 0.008046440780162811\n",
      "[step: 10398] loss: 0.008049730211496353\n",
      "[step: 10399] loss: 0.008107949048280716\n",
      "[step: 10400] loss: 0.007993370294570923\n",
      "[step: 10401] loss: 0.008194584399461746\n",
      "[step: 10402] loss: 0.008233706466853619\n",
      "[step: 10403] loss: 0.00820761639624834\n",
      "[step: 10404] loss: 0.00804819818586111\n",
      "[step: 10405] loss: 0.008395392447710037\n",
      "[step: 10406] loss: 0.008764171972870827\n",
      "[step: 10407] loss: 0.008524986915290356\n",
      "[step: 10408] loss: 0.008275813423097134\n",
      "[step: 10409] loss: 0.00859551690518856\n",
      "[step: 10410] loss: 0.008373070508241653\n",
      "[step: 10411] loss: 0.008023508824408054\n",
      "[step: 10412] loss: 0.008635231293737888\n",
      "[step: 10413] loss: 0.00808463804423809\n",
      "[step: 10414] loss: 0.008143628016114235\n",
      "[step: 10415] loss: 0.008248912170529366\n",
      "[step: 10416] loss: 0.008239073678851128\n",
      "[step: 10417] loss: 0.007953948341310024\n",
      "[step: 10418] loss: 0.008234677836298943\n",
      "[step: 10419] loss: 0.008270024321973324\n",
      "[step: 10420] loss: 0.007995559833943844\n",
      "[step: 10421] loss: 0.008093335665762424\n",
      "[step: 10422] loss: 0.008250086568295956\n",
      "[step: 10423] loss: 0.008043241687119007\n",
      "[step: 10424] loss: 0.007873743772506714\n",
      "[step: 10425] loss: 0.008036300539970398\n",
      "[step: 10426] loss: 0.007987495511770248\n",
      "[step: 10427] loss: 0.007901690900325775\n",
      "[step: 10428] loss: 0.007817861624062061\n",
      "[step: 10429] loss: 0.00787294376641512\n",
      "[step: 10430] loss: 0.00793121475726366\n",
      "[step: 10431] loss: 0.007947615347802639\n",
      "[step: 10432] loss: 0.007960190065205097\n",
      "[step: 10433] loss: 0.007907708175480366\n",
      "[step: 10434] loss: 0.007868572138249874\n",
      "[step: 10435] loss: 0.00779260229319334\n",
      "[step: 10436] loss: 0.00779177900403738\n",
      "[step: 10437] loss: 0.007771418429911137\n",
      "[step: 10438] loss: 0.00780292134732008\n",
      "[step: 10439] loss: 0.00786889623850584\n",
      "[step: 10440] loss: 0.007999168708920479\n",
      "[step: 10441] loss: 0.008173514157533646\n",
      "[step: 10442] loss: 0.008179829455912113\n",
      "[step: 10443] loss: 0.008012070320546627\n",
      "[step: 10444] loss: 0.007809730712324381\n",
      "[step: 10445] loss: 0.007763256318867207\n",
      "[step: 10446] loss: 0.007784136570990086\n",
      "[step: 10447] loss: 0.007935118861496449\n",
      "[step: 10448] loss: 0.008093937300145626\n",
      "[step: 10449] loss: 0.008107592351734638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 10450] loss: 0.007935328409075737\n",
      "[step: 10451] loss: 0.00775953521952033\n",
      "[step: 10452] loss: 0.007768553216010332\n",
      "[step: 10453] loss: 0.007842040620744228\n",
      "[step: 10454] loss: 0.007977059110999107\n",
      "[step: 10455] loss: 0.007853582501411438\n",
      "[step: 10456] loss: 0.007791978307068348\n",
      "[step: 10457] loss: 0.00775197334587574\n",
      "[step: 10458] loss: 0.007714834529906511\n",
      "[step: 10459] loss: 0.00775292469188571\n",
      "[step: 10460] loss: 0.007792996242642403\n",
      "[step: 10461] loss: 0.00791158713400364\n",
      "[step: 10462] loss: 0.00793276634067297\n",
      "[step: 10463] loss: 0.007882796227931976\n",
      "[step: 10464] loss: 0.007803059183061123\n",
      "[step: 10465] loss: 0.007721444591879845\n",
      "[step: 10466] loss: 0.007708828430622816\n",
      "[step: 10467] loss: 0.007694229483604431\n",
      "[step: 10468] loss: 0.007676344830542803\n",
      "[step: 10469] loss: 0.0076864175498485565\n",
      "[step: 10470] loss: 0.007674632128328085\n",
      "[step: 10471] loss: 0.007678060792386532\n",
      "[step: 10472] loss: 0.007700530346482992\n",
      "[step: 10473] loss: 0.007792654912918806\n",
      "[step: 10474] loss: 0.008086657151579857\n",
      "[step: 10475] loss: 0.00845708604902029\n",
      "[step: 10476] loss: 0.00839624647051096\n",
      "[step: 10477] loss: 0.007912456057965755\n",
      "[step: 10478] loss: 0.007729270029813051\n",
      "[step: 10479] loss: 0.00784134492278099\n",
      "[step: 10480] loss: 0.008006040006875992\n",
      "[step: 10481] loss: 0.008004823699593544\n",
      "[step: 10482] loss: 0.007757601328194141\n",
      "[step: 10483] loss: 0.0077773346565663815\n",
      "[step: 10484] loss: 0.007826597429811954\n",
      "[step: 10485] loss: 0.007751422934234142\n",
      "[step: 10486] loss: 0.007737481035292149\n",
      "[step: 10487] loss: 0.007662520743906498\n",
      "[step: 10488] loss: 0.00776010612025857\n",
      "[step: 10489] loss: 0.007800534833222628\n",
      "[step: 10490] loss: 0.007853429764509201\n",
      "[step: 10491] loss: 0.00772812869399786\n",
      "[step: 10492] loss: 0.007681111805140972\n",
      "[step: 10493] loss: 0.00764260720461607\n",
      "[step: 10494] loss: 0.007698403671383858\n",
      "[step: 10495] loss: 0.007745389360934496\n",
      "[step: 10496] loss: 0.00785764865577221\n",
      "[step: 10497] loss: 0.007986092008650303\n",
      "[step: 10498] loss: 0.008072071708738804\n",
      "[step: 10499] loss: 0.008045142516493797\n",
      "[step: 10500] loss: 0.007633299101144075\n",
      "[step: 10501] loss: 0.00807861052453518\n",
      "[step: 10502] loss: 0.008661776781082153\n",
      "[step: 10503] loss: 0.008440032601356506\n",
      "[step: 10504] loss: 0.008087174966931343\n",
      "[step: 10505] loss: 0.008243073709309101\n",
      "[step: 10506] loss: 0.008675815537571907\n",
      "[step: 10507] loss: 0.008612050674855709\n",
      "[step: 10508] loss: 0.007948932237923145\n",
      "[step: 10509] loss: 0.00844502355903387\n",
      "[step: 10510] loss: 0.008034592494368553\n",
      "[step: 10511] loss: 0.008047221228480339\n",
      "[step: 10512] loss: 0.008125863038003445\n",
      "[step: 10513] loss: 0.007982739247381687\n",
      "[step: 10514] loss: 0.008190677501261234\n",
      "[step: 10515] loss: 0.007857720367610455\n",
      "[step: 10516] loss: 0.007941065356135368\n",
      "[step: 10517] loss: 0.007938041351735592\n",
      "[step: 10518] loss: 0.007833780720829964\n",
      "[step: 10519] loss: 0.007839513942599297\n",
      "[step: 10520] loss: 0.007789852563291788\n",
      "[step: 10521] loss: 0.007801907602697611\n",
      "[step: 10522] loss: 0.007877720519900322\n",
      "[step: 10523] loss: 0.007680164184421301\n",
      "[step: 10524] loss: 0.007742002140730619\n",
      "[step: 10525] loss: 0.007652719505131245\n",
      "[step: 10526] loss: 0.007678317371755838\n",
      "[step: 10527] loss: 0.007709431927651167\n",
      "[step: 10528] loss: 0.007648127619177103\n",
      "[step: 10529] loss: 0.00761759327724576\n",
      "[step: 10530] loss: 0.00763110164552927\n",
      "[step: 10531] loss: 0.007588665001094341\n",
      "[step: 10532] loss: 0.007636288180947304\n",
      "[step: 10533] loss: 0.007610819302499294\n",
      "[step: 10534] loss: 0.007668163161724806\n",
      "[step: 10535] loss: 0.007741666864603758\n",
      "[step: 10536] loss: 0.00791194848716259\n",
      "[step: 10537] loss: 0.007959295995533466\n",
      "[step: 10538] loss: 0.007788688875734806\n",
      "[step: 10539] loss: 0.007625671103596687\n",
      "[step: 10540] loss: 0.007565786596387625\n",
      "[step: 10541] loss: 0.007585155311971903\n",
      "[step: 10542] loss: 0.0076780724339187145\n",
      "[step: 10543] loss: 0.007762220222502947\n",
      "[step: 10544] loss: 0.007640852592885494\n",
      "[step: 10545] loss: 0.0075818682089447975\n",
      "[step: 10546] loss: 0.007572608068585396\n",
      "[step: 10547] loss: 0.007572822272777557\n",
      "[step: 10548] loss: 0.007642371114343405\n",
      "[step: 10549] loss: 0.007709179539233446\n",
      "[step: 10550] loss: 0.007687627337872982\n",
      "[step: 10551] loss: 0.007603449746966362\n",
      "[step: 10552] loss: 0.007552218157798052\n",
      "[step: 10553] loss: 0.007575307507067919\n",
      "[step: 10554] loss: 0.007620274089276791\n",
      "[step: 10555] loss: 0.00771459611132741\n",
      "[step: 10556] loss: 0.007786393631249666\n",
      "[step: 10557] loss: 0.007756975945085287\n",
      "[step: 10558] loss: 0.007634381297975779\n",
      "[step: 10559] loss: 0.007574696093797684\n",
      "[step: 10560] loss: 0.007564404048025608\n",
      "[step: 10561] loss: 0.0075029777362942696\n",
      "[step: 10562] loss: 0.007542228326201439\n",
      "[step: 10563] loss: 0.007521227467805147\n",
      "[step: 10564] loss: 0.007566050160676241\n",
      "[step: 10565] loss: 0.007594704162329435\n",
      "[step: 10566] loss: 0.007681096438318491\n",
      "[step: 10567] loss: 0.007874746806919575\n",
      "[step: 10568] loss: 0.007887406274676323\n",
      "[step: 10569] loss: 0.0075872233137488365\n",
      "[step: 10570] loss: 0.007662993855774403\n",
      "[step: 10571] loss: 0.008166488260030746\n",
      "[step: 10572] loss: 0.007766528986394405\n",
      "[step: 10573] loss: 0.007692174054682255\n",
      "[step: 10574] loss: 0.007799065671861172\n",
      "[step: 10575] loss: 0.007799213752150536\n",
      "[step: 10576] loss: 0.007656589616090059\n",
      "[step: 10577] loss: 0.007718222215771675\n",
      "[step: 10578] loss: 0.008012320846319199\n",
      "[step: 10579] loss: 0.007815377786755562\n",
      "[step: 10580] loss: 0.007632739841938019\n",
      "[step: 10581] loss: 0.008105549961328506\n",
      "[step: 10582] loss: 0.008009002543985844\n",
      "[step: 10583] loss: 0.007541801314800978\n",
      "[step: 10584] loss: 0.007824847474694252\n",
      "[step: 10585] loss: 0.008096312172710896\n",
      "[step: 10586] loss: 0.007961764000356197\n",
      "[step: 10587] loss: 0.007902408950030804\n",
      "[step: 10588] loss: 0.007683672942221165\n",
      "[step: 10589] loss: 0.007979982532560825\n",
      "[step: 10590] loss: 0.007740216329693794\n",
      "[step: 10591] loss: 0.007782562170177698\n",
      "[step: 10592] loss: 0.007722130510956049\n",
      "[step: 10593] loss: 0.007843570783734322\n",
      "[step: 10594] loss: 0.007540551479905844\n",
      "[step: 10595] loss: 0.007730433251708746\n",
      "[step: 10596] loss: 0.007679878268390894\n",
      "[step: 10597] loss: 0.007575069088488817\n",
      "[step: 10598] loss: 0.007635545916855335\n",
      "[step: 10599] loss: 0.007611027453094721\n",
      "[step: 10600] loss: 0.007550767157226801\n",
      "[step: 10601] loss: 0.00762517424300313\n",
      "[step: 10602] loss: 0.00757504440844059\n",
      "[step: 10603] loss: 0.0076151276007294655\n",
      "[step: 10604] loss: 0.007577760145068169\n",
      "[step: 10605] loss: 0.007544694002717733\n",
      "[step: 10606] loss: 0.007663174998015165\n",
      "[step: 10607] loss: 0.007599305361509323\n",
      "[step: 10608] loss: 0.007470135577023029\n",
      "[step: 10609] loss: 0.007505273912101984\n",
      "[step: 10610] loss: 0.0075203776359558105\n",
      "[step: 10611] loss: 0.007515445351600647\n",
      "[step: 10612] loss: 0.007590081542730331\n",
      "[step: 10613] loss: 0.0074867624789476395\n",
      "[step: 10614] loss: 0.007464032620191574\n",
      "[step: 10615] loss: 0.007454465143382549\n",
      "[step: 10616] loss: 0.007452665362507105\n",
      "[step: 10617] loss: 0.007435678970068693\n",
      "[step: 10618] loss: 0.007470599375665188\n",
      "[step: 10619] loss: 0.007515544071793556\n",
      "[step: 10620] loss: 0.007626465987414122\n",
      "[step: 10621] loss: 0.007646504789590836\n",
      "[step: 10622] loss: 0.007543467916548252\n",
      "[step: 10623] loss: 0.00742499390617013\n",
      "[step: 10624] loss: 0.00748256454244256\n",
      "[step: 10625] loss: 0.007415976841002703\n",
      "[step: 10626] loss: 0.00752249313518405\n",
      "[step: 10627] loss: 0.007692781277000904\n",
      "[step: 10628] loss: 0.007633647881448269\n",
      "[step: 10629] loss: 0.007554259616881609\n",
      "[step: 10630] loss: 0.007598483003675938\n",
      "[step: 10631] loss: 0.007779196370393038\n",
      "[step: 10632] loss: 0.007765540853142738\n",
      "[step: 10633] loss: 0.007785811088979244\n",
      "[step: 10634] loss: 0.007606434635818005\n",
      "[step: 10635] loss: 0.007441941648721695\n",
      "[step: 10636] loss: 0.0078059714287519455\n",
      "[step: 10637] loss: 0.007738734595477581\n",
      "[step: 10638] loss: 0.007704962510615587\n",
      "[step: 10639] loss: 0.007556372322142124\n",
      "[step: 10640] loss: 0.007729566656053066\n",
      "[step: 10641] loss: 0.007905089296400547\n",
      "[step: 10642] loss: 0.007459590677171946\n",
      "[step: 10643] loss: 0.0077125015668570995\n",
      "[step: 10644] loss: 0.007936227135360241\n",
      "[step: 10645] loss: 0.007768827490508556\n",
      "[step: 10646] loss: 0.007801169995218515\n",
      "[step: 10647] loss: 0.008194764144718647\n",
      "[step: 10648] loss: 0.007620430085808039\n",
      "[step: 10649] loss: 0.007853907532989979\n",
      "[step: 10650] loss: 0.007947554811835289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 10651] loss: 0.007565872278064489\n",
      "[step: 10652] loss: 0.008060884661972523\n",
      "[step: 10653] loss: 0.008242808282375336\n",
      "[step: 10654] loss: 0.0078003848902881145\n",
      "[step: 10655] loss: 0.008489415049552917\n",
      "[step: 10656] loss: 0.00787346437573433\n",
      "[step: 10657] loss: 0.008006498217582703\n",
      "[step: 10658] loss: 0.007901139557361603\n",
      "[step: 10659] loss: 0.007640036754310131\n",
      "[step: 10660] loss: 0.007781959604471922\n",
      "[step: 10661] loss: 0.007529249880462885\n",
      "[step: 10662] loss: 0.007684892974793911\n",
      "[step: 10663] loss: 0.007686912082135677\n",
      "[step: 10664] loss: 0.007455435581505299\n",
      "[step: 10665] loss: 0.00775108439847827\n",
      "[step: 10666] loss: 0.007532323244959116\n",
      "[step: 10667] loss: 0.007487330585718155\n",
      "[step: 10668] loss: 0.00770369591191411\n",
      "[step: 10669] loss: 0.007612621411681175\n",
      "[step: 10670] loss: 0.007490466348826885\n",
      "[step: 10671] loss: 0.007781489286571741\n",
      "[step: 10672] loss: 0.007683914620429277\n",
      "[step: 10673] loss: 0.007611243054270744\n",
      "[step: 10674] loss: 0.007763581816107035\n",
      "[step: 10675] loss: 0.007722849957644939\n",
      "[step: 10676] loss: 0.008015058934688568\n",
      "[step: 10677] loss: 0.007992977276444435\n",
      "[step: 10678] loss: 0.007817880250513554\n",
      "[step: 10679] loss: 0.00805554911494255\n",
      "[step: 10680] loss: 0.007749895565211773\n",
      "[step: 10681] loss: 0.007586611434817314\n",
      "[step: 10682] loss: 0.008027857169508934\n",
      "[step: 10683] loss: 0.0074766757898032665\n",
      "[step: 10684] loss: 0.007673530839383602\n",
      "[step: 10685] loss: 0.007786337751895189\n",
      "[step: 10686] loss: 0.007447039242833853\n",
      "[step: 10687] loss: 0.007630334235727787\n",
      "[step: 10688] loss: 0.007551993243396282\n",
      "[step: 10689] loss: 0.007460900116711855\n",
      "[step: 10690] loss: 0.007615907583385706\n",
      "[step: 10691] loss: 0.007532412186264992\n",
      "[step: 10692] loss: 0.007598781492561102\n",
      "[step: 10693] loss: 0.007509975228458643\n",
      "[step: 10694] loss: 0.007554903160780668\n",
      "[step: 10695] loss: 0.007629168685525656\n",
      "[step: 10696] loss: 0.0073764934204518795\n",
      "[step: 10697] loss: 0.007736487314105034\n",
      "[step: 10698] loss: 0.007733108475804329\n",
      "[step: 10699] loss: 0.007466021925210953\n",
      "[step: 10700] loss: 0.007843321189284325\n",
      "[step: 10701] loss: 0.007890056818723679\n",
      "[step: 10702] loss: 0.007800876162946224\n",
      "[step: 10703] loss: 0.008058455772697926\n",
      "[step: 10704] loss: 0.007497411221265793\n",
      "[step: 10705] loss: 0.007879459299147129\n",
      "[step: 10706] loss: 0.007738960906863213\n",
      "[step: 10707] loss: 0.007879027165472507\n",
      "[step: 10708] loss: 0.00758501049131155\n",
      "[step: 10709] loss: 0.007735580205917358\n",
      "[step: 10710] loss: 0.00751080084592104\n",
      "[step: 10711] loss: 0.007586600258946419\n",
      "[step: 10712] loss: 0.007501420099288225\n",
      "[step: 10713] loss: 0.007595656905323267\n",
      "[step: 10714] loss: 0.007471390534192324\n",
      "[step: 10715] loss: 0.0075392769649624825\n",
      "[step: 10716] loss: 0.0074098072946071625\n",
      "[step: 10717] loss: 0.007479953579604626\n",
      "[step: 10718] loss: 0.007433260791003704\n",
      "[step: 10719] loss: 0.007450675591826439\n",
      "[step: 10720] loss: 0.007405238226056099\n",
      "[step: 10721] loss: 0.007408299949020147\n",
      "[step: 10722] loss: 0.007368324790149927\n",
      "[step: 10723] loss: 0.007395617198199034\n",
      "[step: 10724] loss: 0.007359987590461969\n",
      "[step: 10725] loss: 0.007385279051959515\n",
      "[step: 10726] loss: 0.007352488115429878\n",
      "[step: 10727] loss: 0.007332629524171352\n",
      "[step: 10728] loss: 0.007370289880782366\n",
      "[step: 10729] loss: 0.007307891268283129\n",
      "[step: 10730] loss: 0.007328323554247618\n",
      "[step: 10731] loss: 0.0073294914327561855\n",
      "[step: 10732] loss: 0.007300105877220631\n",
      "[step: 10733] loss: 0.007334661670029163\n",
      "[step: 10734] loss: 0.0072869504801929\n",
      "[step: 10735] loss: 0.007312995381653309\n",
      "[step: 10736] loss: 0.007299750577658415\n",
      "[step: 10737] loss: 0.007311330176889896\n",
      "[step: 10738] loss: 0.00730166956782341\n",
      "[step: 10739] loss: 0.007274549920111895\n",
      "[step: 10740] loss: 0.007293106056749821\n",
      "[step: 10741] loss: 0.0072716944850981236\n",
      "[step: 10742] loss: 0.007263911888003349\n",
      "[step: 10743] loss: 0.007277676369994879\n",
      "[step: 10744] loss: 0.007255026139318943\n",
      "[step: 10745] loss: 0.0072706169448792934\n",
      "[step: 10746] loss: 0.00726988073438406\n",
      "[step: 10747] loss: 0.007271734066307545\n",
      "[step: 10748] loss: 0.0072941952385008335\n",
      "[step: 10749] loss: 0.007353348657488823\n",
      "[step: 10750] loss: 0.00735804159194231\n",
      "[step: 10751] loss: 0.00740939611569047\n",
      "[step: 10752] loss: 0.007432295475155115\n",
      "[step: 10753] loss: 0.007467744406312704\n",
      "[step: 10754] loss: 0.007281483616679907\n",
      "[step: 10755] loss: 0.007272364571690559\n",
      "[step: 10756] loss: 0.007390652317553759\n",
      "[step: 10757] loss: 0.00746074877679348\n",
      "[step: 10758] loss: 0.007416658569127321\n",
      "[step: 10759] loss: 0.0072952560149133205\n",
      "[step: 10760] loss: 0.0072723631747066975\n",
      "[step: 10761] loss: 0.007253591902554035\n",
      "[step: 10762] loss: 0.0073133958503603935\n",
      "[step: 10763] loss: 0.007360497955232859\n",
      "[step: 10764] loss: 0.007252592593431473\n",
      "[step: 10765] loss: 0.007283186074346304\n",
      "[step: 10766] loss: 0.0073158410377800465\n",
      "[step: 10767] loss: 0.007350597996264696\n",
      "[step: 10768] loss: 0.007304568309336901\n",
      "[step: 10769] loss: 0.007221407722681761\n",
      "[step: 10770] loss: 0.007273662835359573\n",
      "[step: 10771] loss: 0.007258920464664698\n",
      "[step: 10772] loss: 0.007294351700693369\n",
      "[step: 10773] loss: 0.007356095593422651\n",
      "[step: 10774] loss: 0.007238802034407854\n",
      "[step: 10775] loss: 0.007257942110300064\n",
      "[step: 10776] loss: 0.007239330094307661\n",
      "[step: 10777] loss: 0.007242964580655098\n",
      "[step: 10778] loss: 0.007314298767596483\n",
      "[step: 10779] loss: 0.007319241762161255\n",
      "[step: 10780] loss: 0.007398654706776142\n",
      "[step: 10781] loss: 0.007343354634940624\n",
      "[step: 10782] loss: 0.00732918968424201\n",
      "[step: 10783] loss: 0.007285699713975191\n",
      "[step: 10784] loss: 0.0072289714589715\n",
      "[step: 10785] loss: 0.007183943409472704\n",
      "[step: 10786] loss: 0.007214550860226154\n",
      "[step: 10787] loss: 0.0072248224169015884\n",
      "[step: 10788] loss: 0.007269647438079119\n",
      "[step: 10789] loss: 0.007429412100464106\n",
      "[step: 10790] loss: 0.007329314947128296\n",
      "[step: 10791] loss: 0.007308646570891142\n",
      "[step: 10792] loss: 0.007258967496454716\n",
      "[step: 10793] loss: 0.007182282395660877\n",
      "[step: 10794] loss: 0.007193946745246649\n",
      "[step: 10795] loss: 0.007208573166280985\n",
      "[step: 10796] loss: 0.007210622075945139\n",
      "[step: 10797] loss: 0.007262654136866331\n",
      "[step: 10798] loss: 0.007369280327111483\n",
      "[step: 10799] loss: 0.007301287725567818\n",
      "[step: 10800] loss: 0.007298551499843597\n",
      "[step: 10801] loss: 0.007224582135677338\n",
      "[step: 10802] loss: 0.007191785611212254\n",
      "[step: 10803] loss: 0.00719826715067029\n",
      "[step: 10804] loss: 0.007163787726312876\n",
      "[step: 10805] loss: 0.007158203981816769\n",
      "[step: 10806] loss: 0.007173028774559498\n",
      "[step: 10807] loss: 0.007161601446568966\n",
      "[step: 10808] loss: 0.007175536826252937\n",
      "[step: 10809] loss: 0.00722205126658082\n",
      "[step: 10810] loss: 0.007291947491466999\n",
      "[step: 10811] loss: 0.00749522540718317\n",
      "[step: 10812] loss: 0.007389424368739128\n",
      "[step: 10813] loss: 0.007355099078267813\n",
      "[step: 10814] loss: 0.0073874820955097675\n",
      "[step: 10815] loss: 0.007239290047436953\n",
      "[step: 10816] loss: 0.007160474080592394\n",
      "[step: 10817] loss: 0.0071473149582743645\n",
      "[step: 10818] loss: 0.007188152521848679\n",
      "[step: 10819] loss: 0.00724282069131732\n",
      "[step: 10820] loss: 0.007204793859273195\n",
      "[step: 10821] loss: 0.0071623544208705425\n",
      "[step: 10822] loss: 0.007136342581361532\n",
      "[step: 10823] loss: 0.007150706369429827\n",
      "[step: 10824] loss: 0.007203013636171818\n",
      "[step: 10825] loss: 0.00725888554006815\n",
      "[step: 10826] loss: 0.007487492635846138\n",
      "[step: 10827] loss: 0.0072499350644648075\n",
      "[step: 10828] loss: 0.007236638106405735\n",
      "[step: 10829] loss: 0.007271307520568371\n",
      "[step: 10830] loss: 0.007288061082363129\n",
      "[step: 10831] loss: 0.007439169567078352\n",
      "[step: 10832] loss: 0.007350641302764416\n",
      "[step: 10833] loss: 0.00732781644910574\n",
      "[step: 10834] loss: 0.007452971767634153\n",
      "[step: 10835] loss: 0.007351558189839125\n",
      "[step: 10836] loss: 0.007237126585096121\n",
      "[step: 10837] loss: 0.007159252651035786\n",
      "[step: 10838] loss: 0.007177271880209446\n",
      "[step: 10839] loss: 0.007152621168643236\n",
      "[step: 10840] loss: 0.007197990082204342\n",
      "[step: 10841] loss: 0.007389653008431196\n",
      "[step: 10842] loss: 0.007152972277253866\n",
      "[step: 10843] loss: 0.007153264246881008\n",
      "[step: 10844] loss: 0.007236948702484369\n",
      "[step: 10845] loss: 0.007200653664767742\n",
      "[step: 10846] loss: 0.007237281184643507\n",
      "[step: 10847] loss: 0.007163272704929113\n",
      "[step: 10848] loss: 0.00710857892408967\n",
      "[step: 10849] loss: 0.007147649303078651\n",
      "[step: 10850] loss: 0.007122276350855827\n",
      "[step: 10851] loss: 0.007102043367922306\n",
      "[step: 10852] loss: 0.0071206712163984776\n",
      "[step: 10853] loss: 0.007149740122258663\n",
      "[step: 10854] loss: 0.007117141503840685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 10855] loss: 0.0071531543508172035\n",
      "[step: 10856] loss: 0.00724037503823638\n",
      "[step: 10857] loss: 0.007413260638713837\n",
      "[step: 10858] loss: 0.007271431852132082\n",
      "[step: 10859] loss: 0.007293671369552612\n",
      "[step: 10860] loss: 0.007149892393499613\n",
      "[step: 10861] loss: 0.007119662128388882\n",
      "[step: 10862] loss: 0.007173591759055853\n",
      "[step: 10863] loss: 0.0072641861625015736\n",
      "[step: 10864] loss: 0.007525446824729443\n",
      "[step: 10865] loss: 0.00742586562409997\n",
      "[step: 10866] loss: 0.007483314722776413\n",
      "[step: 10867] loss: 0.007275192067027092\n",
      "[step: 10868] loss: 0.007202140986919403\n",
      "[step: 10869] loss: 0.007095241919159889\n",
      "[step: 10870] loss: 0.007114109117537737\n",
      "[step: 10871] loss: 0.007227755151689053\n",
      "[step: 10872] loss: 0.0071893357671797276\n",
      "[step: 10873] loss: 0.00714606698602438\n",
      "[step: 10874] loss: 0.007089285179972649\n",
      "[step: 10875] loss: 0.00707357469946146\n",
      "[step: 10876] loss: 0.007091154810041189\n",
      "[step: 10877] loss: 0.007123867515474558\n",
      "[step: 10878] loss: 0.007137156557291746\n",
      "[step: 10879] loss: 0.00711269723251462\n",
      "[step: 10880] loss: 0.007093383464962244\n",
      "[step: 10881] loss: 0.00706489710137248\n",
      "[step: 10882] loss: 0.00705817062407732\n",
      "[step: 10883] loss: 0.007040796801447868\n",
      "[step: 10884] loss: 0.0070513454265892506\n",
      "[step: 10885] loss: 0.00707259401679039\n",
      "[step: 10886] loss: 0.007121064700186253\n",
      "[step: 10887] loss: 0.007308750879019499\n",
      "[step: 10888] loss: 0.007231198251247406\n",
      "[step: 10889] loss: 0.007383339107036591\n",
      "[step: 10890] loss: 0.007407388649880886\n",
      "[step: 10891] loss: 0.007431012112647295\n",
      "[step: 10892] loss: 0.007686047349125147\n",
      "[step: 10893] loss: 0.007309822831302881\n",
      "[step: 10894] loss: 0.007174388505518436\n",
      "[step: 10895] loss: 0.007059984840452671\n",
      "[step: 10896] loss: 0.007181709166616201\n",
      "[step: 10897] loss: 0.0073710111901164055\n",
      "[step: 10898] loss: 0.007229647599160671\n",
      "[step: 10899] loss: 0.007114474195986986\n",
      "[step: 10900] loss: 0.007107738871127367\n",
      "[step: 10901] loss: 0.007139494176954031\n",
      "[step: 10902] loss: 0.007183532230556011\n",
      "[step: 10903] loss: 0.007187836337834597\n",
      "[step: 10904] loss: 0.007075679954141378\n",
      "[step: 10905] loss: 0.007039315067231655\n",
      "[step: 10906] loss: 0.00705765513703227\n",
      "[step: 10907] loss: 0.007106810808181763\n",
      "[step: 10908] loss: 0.0070852115750312805\n",
      "[step: 10909] loss: 0.007114147301763296\n",
      "[step: 10910] loss: 0.00710042379796505\n",
      "[step: 10911] loss: 0.007150471210479736\n",
      "[step: 10912] loss: 0.007361873984336853\n",
      "[step: 10913] loss: 0.0074211228638887405\n",
      "[step: 10914] loss: 0.007850988768041134\n",
      "[step: 10915] loss: 0.007371806539595127\n",
      "[step: 10916] loss: 0.008695975877344608\n",
      "[step: 10917] loss: 0.009449592791497707\n",
      "[step: 10918] loss: 0.013887724839150906\n",
      "[step: 10919] loss: 0.015666265040636063\n",
      "[step: 10920] loss: 0.02364092506468296\n",
      "[step: 10921] loss: 0.04750795662403107\n",
      "[step: 10922] loss: 0.01658024452626705\n",
      "[step: 10923] loss: 0.04220012202858925\n",
      "[step: 10924] loss: 0.020091122016310692\n",
      "[step: 10925] loss: 0.02499416284263134\n",
      "[step: 10926] loss: 0.02560676448047161\n",
      "[step: 10927] loss: 0.021022478118538857\n",
      "[step: 10928] loss: 0.018908878788352013\n",
      "[step: 10929] loss: 0.015576806850731373\n",
      "[step: 10930] loss: 0.016862209886312485\n",
      "[step: 10931] loss: 0.01590127870440483\n",
      "[step: 10932] loss: 0.018208090215921402\n",
      "[step: 10933] loss: 0.013830665498971939\n",
      "[step: 10934] loss: 0.014344429597258568\n",
      "[step: 10935] loss: 0.014959033578634262\n",
      "[step: 10936] loss: 0.013807566836476326\n",
      "[step: 10937] loss: 0.014918174594640732\n",
      "[step: 10938] loss: 0.01325654424726963\n",
      "[step: 10939] loss: 0.012949426658451557\n",
      "[step: 10940] loss: 0.013383518904447556\n",
      "[step: 10941] loss: 0.01242227666079998\n",
      "[step: 10942] loss: 0.012816634960472584\n",
      "[step: 10943] loss: 0.013186970725655556\n",
      "[step: 10944] loss: 0.01249393168836832\n",
      "[step: 10945] loss: 0.012554806657135487\n",
      "[step: 10946] loss: 0.012552033178508282\n",
      "[step: 10947] loss: 0.011956601403653622\n",
      "[step: 10948] loss: 0.011968967504799366\n",
      "[step: 10949] loss: 0.012146856635808945\n",
      "[step: 10950] loss: 0.011868512257933617\n",
      "[step: 10951] loss: 0.011847402900457382\n",
      "[step: 10952] loss: 0.012003693729639053\n",
      "[step: 10953] loss: 0.011801152490079403\n",
      "[step: 10954] loss: 0.01161259040236473\n",
      "[step: 10955] loss: 0.011657767929136753\n",
      "[step: 10956] loss: 0.011563370935618877\n",
      "[step: 10957] loss: 0.011402263306081295\n",
      "[step: 10958] loss: 0.011445630341768265\n",
      "[step: 10959] loss: 0.011486758477985859\n",
      "[step: 10960] loss: 0.011380820535123348\n",
      "[step: 10961] loss: 0.011325189843773842\n",
      "[step: 10962] loss: 0.011332867667078972\n",
      "[step: 10963] loss: 0.011247154325246811\n",
      "[step: 10964] loss: 0.011143903248012066\n",
      "[step: 10965] loss: 0.01113802194595337\n",
      "[step: 10966] loss: 0.011137141846120358\n",
      "[step: 10967] loss: 0.011079994961619377\n",
      "[step: 10968] loss: 0.011044485494494438\n",
      "[step: 10969] loss: 0.011037354357540607\n",
      "[step: 10970] loss: 0.010990971699357033\n",
      "[step: 10971] loss: 0.010932516306638718\n",
      "[step: 10972] loss: 0.010920431464910507\n",
      "[step: 10973] loss: 0.010919593274593353\n",
      "[step: 10974] loss: 0.010890113189816475\n",
      "[step: 10975] loss: 0.01086394488811493\n",
      "[step: 10976] loss: 0.010855983942747116\n",
      "[step: 10977] loss: 0.010834231972694397\n",
      "[step: 10978] loss: 0.01080130785703659\n",
      "[step: 10979] loss: 0.010788582265377045\n",
      "[step: 10980] loss: 0.010787433944642544\n",
      "[step: 10981] loss: 0.010773124173283577\n",
      "[step: 10982] loss: 0.010754650458693504\n",
      "[step: 10983] loss: 0.01074534747749567\n",
      "[step: 10984] loss: 0.010734022594988346\n",
      "[step: 10985] loss: 0.010716193355619907\n",
      "[step: 10986] loss: 0.01070514228194952\n",
      "[step: 10987] loss: 0.010700879618525505\n",
      "[step: 10988] loss: 0.010690445080399513\n",
      "[step: 10989] loss: 0.010675101540982723\n",
      "[step: 10990] loss: 0.010664084926247597\n",
      "[step: 10991] loss: 0.01065483596175909\n",
      "[step: 10992] loss: 0.01064245868474245\n",
      "[step: 10993] loss: 0.010631379671394825\n",
      "[step: 10994] loss: 0.010623790323734283\n",
      "[step: 10995] loss: 0.010614277794957161\n",
      "[step: 10996] loss: 0.010601688176393509\n",
      "[step: 10997] loss: 0.010590927675366402\n",
      "[step: 10998] loss: 0.010582558810710907\n",
      "[step: 10999] loss: 0.01057332195341587\n",
      "[step: 11000] loss: 0.010563756339251995\n",
      "[step: 11001] loss: 0.010555843822658062\n",
      "[step: 11002] loss: 0.010548042133450508\n",
      "[step: 11003] loss: 0.01053904090076685\n",
      "[step: 11004] loss: 0.010530797764658928\n",
      "[step: 11005] loss: 0.010524291545152664\n",
      "[step: 11006] loss: 0.010517807677388191\n",
      "[step: 11007] loss: 0.010510729625821114\n",
      "[step: 11008] loss: 0.010504179634153843\n",
      "[step: 11009] loss: 0.010498108342289925\n",
      "[step: 11010] loss: 0.010491658002138138\n",
      "[step: 11011] loss: 0.010485293343663216\n",
      "[step: 11012] loss: 0.010479692369699478\n",
      "[step: 11013] loss: 0.010474231094121933\n",
      "[step: 11014] loss: 0.010468307882547379\n",
      "[step: 11015] loss: 0.010462423786520958\n",
      "[step: 11016] loss: 0.010456924326717854\n",
      "[step: 11017] loss: 0.01045142486691475\n",
      "[step: 11018] loss: 0.010445854626595974\n",
      "[step: 11019] loss: 0.010440545156598091\n",
      "[step: 11020] loss: 0.010435390286147594\n",
      "[step: 11021] loss: 0.010430045425891876\n",
      "[step: 11022] loss: 0.010424668900668621\n",
      "[step: 11023] loss: 0.010419543832540512\n",
      "[step: 11024] loss: 0.01041453704237938\n",
      "[step: 11025] loss: 0.0104094622656703\n",
      "[step: 11026] loss: 0.010404454544186592\n",
      "[step: 11027] loss: 0.010399560444056988\n",
      "[step: 11028] loss: 0.01039463933557272\n",
      "[step: 11029] loss: 0.010389705188572407\n",
      "[step: 11030] loss: 0.010384897701442242\n",
      "[step: 11031] loss: 0.010380186140537262\n",
      "[step: 11032] loss: 0.010375449433922768\n",
      "[step: 11033] loss: 0.010370711795985699\n",
      "[step: 11034] loss: 0.0103660449385643\n",
      "[step: 11035] loss: 0.01036140974611044\n",
      "[step: 11036] loss: 0.010356777347624302\n",
      "[step: 11037] loss: 0.010352197103202343\n",
      "[step: 11038] loss: 0.01034766435623169\n",
      "[step: 11039] loss: 0.01034313440322876\n",
      "[step: 11040] loss: 0.010338600724935532\n",
      "[step: 11041] loss: 0.010334109887480736\n",
      "[step: 11042] loss: 0.010329661890864372\n",
      "[step: 11043] loss: 0.010325218550860882\n",
      "[step: 11044] loss: 0.010320796631276608\n",
      "[step: 11045] loss: 0.010316404514014721\n",
      "[step: 11046] loss: 0.010312030091881752\n",
      "[step: 11047] loss: 0.010307656601071358\n",
      "[step: 11048] loss: 0.010303318500518799\n",
      "[step: 11049] loss: 0.010299012996256351\n",
      "[step: 11050] loss: 0.010294712148606777\n",
      "[step: 11051] loss: 0.010290421545505524\n",
      "[step: 11052] loss: 0.01028615701943636\n",
      "[step: 11053] loss: 0.010281902737915516\n",
      "[step: 11054] loss: 0.01027766428887844\n",
      "[step: 11055] loss: 0.010273443534970284\n",
      "[step: 11056] loss: 0.01026923581957817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 11057] loss: 0.010265039280056953\n",
      "[step: 11058] loss: 0.010260850191116333\n",
      "[step: 11059] loss: 0.010256673209369183\n",
      "[step: 11060] loss: 0.010252507403492928\n",
      "[step: 11061] loss: 0.01024834904819727\n",
      "[step: 11062] loss: 0.010244196280837059\n",
      "[step: 11063] loss: 0.010240056551992893\n",
      "[step: 11064] loss: 0.010235919617116451\n",
      "[step: 11065] loss: 0.01023178081959486\n",
      "[step: 11066] loss: 0.010227656923234463\n",
      "[step: 11067] loss: 0.010223539546132088\n",
      "[step: 11068] loss: 0.010219426825642586\n",
      "[step: 11069] loss: 0.010215314105153084\n",
      "[step: 11070] loss: 0.010211207903921604\n",
      "[step: 11071] loss: 0.010207106359302998\n",
      "[step: 11072] loss: 0.010203010402619839\n",
      "[step: 11073] loss: 0.010198917239904404\n",
      "[step: 11074] loss: 0.010194828733801842\n",
      "[step: 11075] loss: 0.01019074022769928\n",
      "[step: 11076] loss: 0.010186659172177315\n",
      "[step: 11077] loss: 0.010182582773268223\n",
      "[step: 11078] loss: 0.010178507305681705\n",
      "[step: 11079] loss: 0.010174436494708061\n",
      "[step: 11080] loss: 0.010170371271669865\n",
      "[step: 11081] loss: 0.010166309773921967\n",
      "[step: 11082] loss: 0.010162253864109516\n",
      "[step: 11083] loss: 0.010158196091651917\n",
      "[step: 11084] loss: 0.010154143907129765\n",
      "[step: 11085] loss: 0.01015009731054306\n",
      "[step: 11086] loss: 0.01014605350792408\n",
      "[step: 11087] loss: 0.010142015293240547\n",
      "[step: 11088] loss: 0.010137977078557014\n",
      "[step: 11089] loss: 0.010133943520486355\n",
      "[step: 11090] loss: 0.010129914619028568\n",
      "[step: 11091] loss: 0.01012589130550623\n",
      "[step: 11092] loss: 0.010121867060661316\n",
      "[step: 11093] loss: 0.010117850266397\n",
      "[step: 11094] loss: 0.010113833472132683\n",
      "[step: 11095] loss: 0.01010982133448124\n",
      "[step: 11096] loss: 0.010105807334184647\n",
      "[step: 11097] loss: 0.0101018026471138\n",
      "[step: 11098] loss: 0.010097796097397804\n",
      "[step: 11099] loss: 0.010093794204294682\n",
      "[step: 11100] loss: 0.010089794173836708\n",
      "[step: 11101] loss: 0.010085796937346458\n",
      "[step: 11102] loss: 0.010081800632178783\n",
      "[step: 11103] loss: 0.010077807120978832\n",
      "[step: 11104] loss: 0.010073813609778881\n",
      "[step: 11105] loss: 0.010069824755191803\n",
      "[step: 11106] loss: 0.01006583496928215\n",
      "[step: 11107] loss: 0.010061844252049923\n",
      "[step: 11108] loss: 0.010057858191430569\n",
      "[step: 11109] loss: 0.010053872130811214\n",
      "[step: 11110] loss: 0.010049885138869286\n",
      "[step: 11111] loss: 0.010045900009572506\n",
      "[step: 11112] loss: 0.010041913017630577\n",
      "[step: 11113] loss: 0.010037926025688648\n",
      "[step: 11114] loss: 0.01003393717110157\n",
      "[step: 11115] loss: 0.010029947385191917\n",
      "[step: 11116] loss: 0.010025955736637115\n",
      "[step: 11117] loss: 0.010021962225437164\n",
      "[step: 11118] loss: 0.010017964988946915\n",
      "[step: 11119] loss: 0.010013964958488941\n",
      "[step: 11120] loss: 0.010009960271418095\n",
      "[step: 11121] loss: 0.010005948133766651\n",
      "[step: 11122] loss: 0.010001935064792633\n",
      "[step: 11123] loss: 0.009997912682592869\n",
      "[step: 11124] loss: 0.009993885643780231\n",
      "[step: 11125] loss: 0.009989848360419273\n",
      "[step: 11126] loss: 0.009985807351768017\n",
      "[step: 11127] loss: 0.009981760755181313\n",
      "[step: 11128] loss: 0.00997769832611084\n",
      "[step: 11129] loss: 0.009973629377782345\n",
      "[step: 11130] loss: 0.009969550184905529\n",
      "[step: 11131] loss: 0.009965462610125542\n",
      "[step: 11132] loss: 0.00996136013418436\n",
      "[step: 11133] loss: 0.009957246482372284\n",
      "[step: 11134] loss: 0.009953120723366737\n",
      "[step: 11135] loss: 0.009948979131877422\n",
      "[step: 11136] loss: 0.009944823570549488\n",
      "[step: 11137] loss: 0.00994065497070551\n",
      "[step: 11138] loss: 0.009936467744410038\n",
      "[step: 11139] loss: 0.009932264685630798\n",
      "[step: 11140] loss: 0.009928042069077492\n",
      "[step: 11141] loss: 0.009923801757395267\n",
      "[step: 11142] loss: 0.009919545613229275\n",
      "[step: 11143] loss: 0.009915264323353767\n",
      "[step: 11144] loss: 0.009910967200994492\n",
      "[step: 11145] loss: 0.00990664679557085\n",
      "[step: 11146] loss: 0.009902304969727993\n",
      "[step: 11147] loss: 0.009897936135530472\n",
      "[step: 11148] loss: 0.009893552400171757\n",
      "[step: 11149] loss: 0.009889138862490654\n",
      "[step: 11150] loss: 0.009884701110422611\n",
      "[step: 11151] loss: 0.009880240075290203\n",
      "[step: 11152] loss: 0.009875751100480556\n",
      "[step: 11153] loss: 0.009871234185993671\n",
      "[step: 11154] loss: 0.00986669585108757\n",
      "[step: 11155] loss: 0.00986212957650423\n",
      "[step: 11156] loss: 0.00985753070563078\n",
      "[step: 11157] loss: 0.009852910414338112\n",
      "[step: 11158] loss: 0.009848261252045631\n",
      "[step: 11159] loss: 0.009843585081398487\n",
      "[step: 11160] loss: 0.009838880971074104\n",
      "[step: 11161] loss: 0.009834149852395058\n",
      "[step: 11162] loss: 0.009829390794038773\n",
      "[step: 11163] loss: 0.009824606589972973\n",
      "[step: 11164] loss: 0.009819802828133106\n",
      "[step: 11165] loss: 0.009814971126616001\n",
      "[step: 11166] loss: 0.009810117073357105\n",
      "[step: 11167] loss: 0.009805240668356419\n",
      "[step: 11168] loss: 0.00980034563690424\n",
      "[step: 11169] loss: 0.009795431047677994\n",
      "[step: 11170] loss: 0.009790508076548576\n",
      "[step: 11171] loss: 0.009785565547645092\n",
      "[step: 11172] loss: 0.009780609048902988\n",
      "[step: 11173] loss: 0.009775649756193161\n",
      "[step: 11174] loss: 0.009770688600838184\n",
      "[step: 11175] loss: 0.009765713475644588\n",
      "[step: 11176] loss: 0.009760750457644463\n",
      "[step: 11177] loss: 0.009755788370966911\n",
      "[step: 11178] loss: 0.009750829078257084\n",
      "[step: 11179] loss: 0.009745883755385876\n",
      "[step: 11180] loss: 0.00974095519632101\n",
      "[step: 11181] loss: 0.009736035950481892\n",
      "[step: 11182] loss: 0.009731135331094265\n",
      "[step: 11183] loss: 0.009726256132125854\n",
      "[step: 11184] loss: 0.009721399284899235\n",
      "[step: 11185] loss: 0.00971656572073698\n",
      "[step: 11186] loss: 0.009711753576993942\n",
      "[step: 11187] loss: 0.009706966578960419\n",
      "[step: 11188] loss: 0.00970220286399126\n",
      "[step: 11189] loss: 0.009697468020021915\n",
      "[step: 11190] loss: 0.00969274714589119\n",
      "[step: 11191] loss: 0.009688051417469978\n",
      "[step: 11192] loss: 0.009683369658887386\n",
      "[step: 11193] loss: 0.00967870932072401\n",
      "[step: 11194] loss: 0.009674064815044403\n",
      "[step: 11195] loss: 0.009669436141848564\n",
      "[step: 11196] loss: 0.009664821438491344\n",
      "[step: 11197] loss: 0.00966021791100502\n",
      "[step: 11198] loss: 0.009655635803937912\n",
      "[step: 11199] loss: 0.00965106301009655\n",
      "[step: 11200] loss: 0.009646506048738956\n",
      "[step: 11201] loss: 0.009641964919865131\n",
      "[step: 11202] loss: 0.0096374386921525\n",
      "[step: 11203] loss: 0.009632930159568787\n",
      "[step: 11204] loss: 0.009628443978726864\n",
      "[step: 11205] loss: 0.009623970836400986\n",
      "[step: 11206] loss: 0.009619520977139473\n",
      "[step: 11207] loss: 0.009615090675652027\n",
      "[step: 11208] loss: 0.009610681794583797\n",
      "[step: 11209] loss: 0.00960628967732191\n",
      "[step: 11210] loss: 0.009601921774446964\n",
      "[step: 11211] loss: 0.009597571566700935\n",
      "[step: 11212] loss: 0.009593244642019272\n",
      "[step: 11213] loss: 0.009588935412466526\n",
      "[step: 11214] loss: 0.009584644809365273\n",
      "[step: 11215] loss: 0.00958037469536066\n",
      "[step: 11216] loss: 0.009576121345162392\n",
      "[step: 11217] loss: 0.009571889415383339\n",
      "[step: 11218] loss: 0.009567677974700928\n",
      "[step: 11219] loss: 0.00956348143517971\n",
      "[step: 11220] loss: 0.009559305384755135\n",
      "[step: 11221] loss: 0.009555143304169178\n",
      "[step: 11222] loss: 0.009551005437970161\n",
      "[step: 11223] loss: 0.009546883404254913\n",
      "[step: 11224] loss: 0.009542782790958881\n",
      "[step: 11225] loss: 0.00953869428485632\n",
      "[step: 11226] loss: 0.009534625336527824\n",
      "[step: 11227] loss: 0.009530572220683098\n",
      "[step: 11228] loss: 0.009526534005999565\n",
      "[step: 11229] loss: 0.009522516280412674\n",
      "[step: 11230] loss: 0.009518508799374104\n",
      "[step: 11231] loss: 0.009514519944787025\n",
      "[step: 11232] loss: 0.009510546922683716\n",
      "[step: 11233] loss: 0.0095065888017416\n",
      "[step: 11234] loss: 0.009502642787992954\n",
      "[step: 11235] loss: 0.009498714469373226\n",
      "[step: 11236] loss: 0.009494798257946968\n",
      "[step: 11237] loss: 0.009490896947681904\n",
      "[step: 11238] loss: 0.009487006813287735\n",
      "[step: 11239] loss: 0.009483137167990208\n",
      "[step: 11240] loss: 0.009479275904595852\n",
      "[step: 11241] loss: 0.009475426748394966\n",
      "[step: 11242] loss: 0.009471592493355274\n",
      "[step: 11243] loss: 0.009467768482863903\n",
      "[step: 11244] loss: 0.009463950991630554\n",
      "[step: 11245] loss: 0.009460150264203548\n",
      "[step: 11246] loss: 0.009456358850002289\n",
      "[step: 11247] loss: 0.009452576749026775\n",
      "[step: 11248] loss: 0.009448808617889881\n",
      "[step: 11249] loss: 0.009445044212043285\n",
      "[step: 11250] loss: 0.009441292844712734\n",
      "[step: 11251] loss: 0.00943754706531763\n",
      "[step: 11252] loss: 0.009433811530470848\n",
      "[step: 11253] loss: 0.009430084377527237\n",
      "[step: 11254] loss: 0.009426362812519073\n",
      "[step: 11255] loss: 0.009422648698091507\n",
      "[step: 11256] loss: 0.009418942034244537\n",
      "[step: 11257] loss: 0.00941524002701044\n",
      "[step: 11258] loss: 0.009411546401679516\n",
      "[step: 11259] loss: 0.009407857432961464\n",
      "[step: 11260] loss: 0.00940417405217886\n",
      "[step: 11261] loss: 0.009400490671396255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 11262] loss: 0.009396816603839397\n",
      "[step: 11263] loss: 0.00939314253628254\n",
      "[step: 11264] loss: 0.009389473125338554\n",
      "[step: 11265] loss: 0.009385805577039719\n",
      "[step: 11266] loss: 0.009382139891386032\n",
      "[step: 11267] loss: 0.009378479793667793\n",
      "[step: 11268] loss: 0.009374821558594704\n",
      "[step: 11269] loss: 0.009371163323521614\n",
      "[step: 11270] loss: 0.009367506019771099\n",
      "[step: 11271] loss: 0.00936384592205286\n",
      "[step: 11272] loss: 0.009360183961689472\n",
      "[step: 11273] loss: 0.009356524795293808\n",
      "[step: 11274] loss: 0.009352865628898144\n",
      "[step: 11275] loss: 0.009349205531179905\n",
      "[step: 11276] loss: 0.009345542639493942\n",
      "[step: 11277] loss: 0.009341876022517681\n",
      "[step: 11278] loss: 0.009338213130831718\n",
      "[step: 11279] loss: 0.00933453906327486\n",
      "[step: 11280] loss: 0.009330865927040577\n",
      "[step: 11281] loss: 0.009327186271548271\n",
      "[step: 11282] loss: 0.009323504753410816\n",
      "[step: 11283] loss: 0.009319816716015339\n",
      "[step: 11284] loss: 0.009316126815974712\n",
      "[step: 11285] loss: 0.00931242574006319\n",
      "[step: 11286] loss: 0.00930872280150652\n",
      "[step: 11287] loss: 0.009305012412369251\n",
      "[step: 11288] loss: 0.009301293641328812\n",
      "[step: 11289] loss: 0.009297569282352924\n",
      "[step: 11290] loss: 0.00929383933544159\n",
      "[step: 11291] loss: 0.00929009635001421\n",
      "[step: 11292] loss: 0.009286346845328808\n",
      "[step: 11293] loss: 0.009282587096095085\n",
      "[step: 11294] loss: 0.009278821758925915\n",
      "[step: 11295] loss: 0.009275042451918125\n",
      "[step: 11296] loss: 0.009271255694329739\n",
      "[step: 11297] loss: 0.00926746055483818\n",
      "[step: 11298] loss: 0.00926364865154028\n",
      "[step: 11299] loss: 0.009259828366339207\n",
      "[step: 11300] loss: 0.009255998767912388\n",
      "[step: 11301] loss: 0.009252152405679226\n",
      "[step: 11302] loss: 0.009248295798897743\n",
      "[step: 11303] loss: 0.009244424290955067\n",
      "[step: 11304] loss: 0.009240545332431793\n",
      "[step: 11305] loss: 0.009236644953489304\n",
      "[step: 11306] loss: 0.009232733398675919\n",
      "[step: 11307] loss: 0.009228808805346489\n",
      "[step: 11308] loss: 0.009224866516888142\n",
      "[step: 11309] loss: 0.00922091118991375\n",
      "[step: 11310] loss: 0.009216941893100739\n",
      "[step: 11311] loss: 0.009212953969836235\n",
      "[step: 11312] loss: 0.009208953939378262\n",
      "[step: 11313] loss: 0.009204928763210773\n",
      "[step: 11314] loss: 0.009200895205140114\n",
      "[step: 11315] loss: 0.009196839295327663\n",
      "[step: 11316] loss: 0.00919276662170887\n",
      "[step: 11317] loss: 0.009188677184283733\n",
      "[step: 11318] loss: 0.009184573777019978\n",
      "[step: 11319] loss: 0.009180446155369282\n",
      "[step: 11320] loss: 0.00917629711329937\n",
      "[step: 11321] loss: 0.009172133170068264\n",
      "[step: 11322] loss: 0.009167950600385666\n",
      "[step: 11323] loss: 0.009163749404251575\n",
      "[step: 11324] loss: 0.009159522131085396\n",
      "[step: 11325] loss: 0.009155280888080597\n",
      "[step: 11326] loss: 0.009151016362011433\n",
      "[step: 11327] loss: 0.009146733209490776\n",
      "[step: 11328] loss: 0.009142424911260605\n",
      "[step: 11329] loss: 0.009138098917901516\n",
      "[step: 11330] loss: 0.009133749641478062\n",
      "[step: 11331] loss: 0.00912938266992569\n",
      "[step: 11332] loss: 0.009124989621341228\n",
      "[step: 11333] loss: 0.009120576083660126\n",
      "[step: 11334] loss: 0.009116138331592083\n",
      "[step: 11335] loss: 0.009111681021749973\n",
      "[step: 11336] loss: 0.009107201360166073\n",
      "[step: 11337] loss: 0.009102699346840382\n",
      "[step: 11338] loss: 0.009098172187805176\n",
      "[step: 11339] loss: 0.009093621745705605\n",
      "[step: 11340] loss: 0.009089053608477116\n",
      "[step: 11341] loss: 0.009084459394216537\n",
      "[step: 11342] loss: 0.00907983910292387\n",
      "[step: 11343] loss: 0.009075199253857136\n",
      "[step: 11344] loss: 0.009070535190403461\n",
      "[step: 11345] loss: 0.009065848775207996\n",
      "[step: 11346] loss: 0.009061139076948166\n",
      "[step: 11347] loss: 0.009056402370333672\n",
      "[step: 11348] loss: 0.009051648899912834\n",
      "[step: 11349] loss: 0.009046866558492184\n",
      "[step: 11350] loss: 0.009042065590620041\n",
      "[step: 11351] loss: 0.009037237614393234\n",
      "[step: 11352] loss: 0.009032388217747211\n",
      "[step: 11353] loss: 0.009027513675391674\n",
      "[step: 11354] loss: 0.009022618643939495\n",
      "[step: 11355] loss: 0.009017697535455227\n",
      "[step: 11356] loss: 0.009012758731842041\n",
      "[step: 11357] loss: 0.00900779478251934\n",
      "[step: 11358] loss: 0.009002807550132275\n",
      "[step: 11359] loss: 0.008997797966003418\n",
      "[step: 11360] loss: 0.00899276789277792\n",
      "[step: 11361] loss: 0.00898771546781063\n",
      "[step: 11362] loss: 0.008982637897133827\n",
      "[step: 11363] loss: 0.008977539837360382\n",
      "[step: 11364] loss: 0.008972421288490295\n",
      "[step: 11365] loss: 0.008967283181846142\n",
      "[step: 11366] loss: 0.008962119929492474\n",
      "[step: 11367] loss: 0.008956938982009888\n",
      "[step: 11368] loss: 0.008951736614108086\n",
      "[step: 11369] loss: 0.008946510963141918\n",
      "[step: 11370] loss: 0.008941270411014557\n",
      "[step: 11371] loss: 0.00893600843846798\n",
      "[step: 11372] loss: 0.008930722251534462\n",
      "[step: 11373] loss: 0.008925420232117176\n",
      "[step: 11374] loss: 0.008920099586248398\n",
      "[step: 11375] loss: 0.008914757519960403\n",
      "[step: 11376] loss: 0.008909397758543491\n",
      "[step: 11377] loss: 0.008904017508029938\n",
      "[step: 11378] loss: 0.00889862421900034\n",
      "[step: 11379] loss: 0.008893205784261227\n",
      "[step: 11380] loss: 0.008887768723070621\n",
      "[step: 11381] loss: 0.008882318623363972\n",
      "[step: 11382] loss: 0.008876848965883255\n",
      "[step: 11383] loss: 0.008871358819305897\n",
      "[step: 11384] loss: 0.008865853771567345\n",
      "[step: 11385] loss: 0.0088603300973773\n",
      "[step: 11386] loss: 0.008854791522026062\n",
      "[step: 11387] loss: 0.008849229663610458\n",
      "[step: 11388] loss: 0.008843652904033661\n",
      "[step: 11389] loss: 0.00883805938065052\n",
      "[step: 11390] loss: 0.008832446299493313\n",
      "[step: 11391] loss: 0.008826813660562038\n",
      "[step: 11392] loss: 0.008821167051792145\n",
      "[step: 11393] loss: 0.008815497159957886\n",
      "[step: 11394] loss: 0.00880980771034956\n",
      "[step: 11395] loss: 0.00880410335958004\n",
      "[step: 11396] loss: 0.008798375725746155\n",
      "[step: 11397] loss: 0.008792629465460777\n",
      "[step: 11398] loss: 0.008786866441369057\n",
      "[step: 11399] loss: 0.008781078271567822\n",
      "[step: 11400] loss: 0.00877528265118599\n",
      "[step: 11401] loss: 0.008769509382545948\n",
      "[step: 11402] loss: 0.008763974532485008\n",
      "[step: 11403] loss: 0.008760028518736362\n",
      "[step: 11404] loss: 0.008766652084887028\n",
      "[step: 11405] loss: 0.008844430558383465\n",
      "[step: 11406] loss: 0.00917421467602253\n",
      "[step: 11407] loss: 0.008981265127658844\n",
      "[step: 11408] loss: 0.010272038169205189\n",
      "[step: 11409] loss: 0.009124331176280975\n",
      "[step: 11410] loss: 0.009419085457921028\n",
      "[step: 11411] loss: 0.009092696942389011\n",
      "[step: 11412] loss: 0.009499229490756989\n",
      "[step: 11413] loss: 0.00909772515296936\n",
      "[step: 11414] loss: 0.009325345046818256\n",
      "[step: 11415] loss: 0.009139840491116047\n",
      "[step: 11416] loss: 0.008931704796850681\n",
      "[step: 11417] loss: 0.008973601274192333\n",
      "[step: 11418] loss: 0.009084248915314674\n",
      "[step: 11419] loss: 0.009164146147668362\n",
      "[step: 11420] loss: 0.00897578801959753\n",
      "[step: 11421] loss: 0.009174555540084839\n",
      "[step: 11422] loss: 0.008913785219192505\n",
      "[step: 11423] loss: 0.00897281151264906\n",
      "[step: 11424] loss: 0.008814108557999134\n",
      "[step: 11425] loss: 0.008755984716117382\n",
      "[step: 11426] loss: 0.008879694156348705\n",
      "[step: 11427] loss: 0.008737088181078434\n",
      "[step: 11428] loss: 0.008785049431025982\n",
      "[step: 11429] loss: 0.008917783387005329\n",
      "[step: 11430] loss: 0.008788333274424076\n",
      "[step: 11431] loss: 0.009277653880417347\n",
      "[step: 11432] loss: 0.009207983501255512\n",
      "[step: 11433] loss: 0.009595193900167942\n",
      "[step: 11434] loss: 0.009403521195054054\n",
      "[step: 11435] loss: 0.009254416450858116\n",
      "[step: 11436] loss: 0.008827765472233295\n",
      "[step: 11437] loss: 0.00928308442234993\n",
      "[step: 11438] loss: 0.009087804704904556\n",
      "[step: 11439] loss: 0.009251688607037067\n",
      "[step: 11440] loss: 0.009139927104115486\n",
      "[step: 11441] loss: 0.008946103975176811\n",
      "[step: 11442] loss: 0.009531799703836441\n",
      "[step: 11443] loss: 0.009096863679587841\n",
      "[step: 11444] loss: 0.009425739757716656\n",
      "[step: 11445] loss: 0.00917331874370575\n",
      "[step: 11446] loss: 0.009000723250210285\n",
      "[step: 11447] loss: 0.009657571092247963\n",
      "[step: 11448] loss: 0.00890267826616764\n",
      "[step: 11449] loss: 0.009300501085817814\n",
      "[step: 11450] loss: 0.00907595269382\n",
      "[step: 11451] loss: 0.008841859176754951\n",
      "[step: 11452] loss: 0.00937320850789547\n",
      "[step: 11453] loss: 0.008856541477143764\n",
      "[step: 11454] loss: 0.009173805825412273\n",
      "[step: 11455] loss: 0.008992764167487621\n",
      "[step: 11456] loss: 0.008766772225499153\n",
      "[step: 11457] loss: 0.008996853604912758\n",
      "[step: 11458] loss: 0.00893609318882227\n",
      "[step: 11459] loss: 0.009154940024018288\n",
      "[step: 11460] loss: 0.008959421887993813\n",
      "[step: 11461] loss: 0.008745102211833\n",
      "[step: 11462] loss: 0.008819542825222015\n",
      "[step: 11463] loss: 0.008889526128768921\n",
      "[step: 11464] loss: 0.009033753536641598\n",
      "[step: 11465] loss: 0.008855954743921757\n",
      "[step: 11466] loss: 0.008770967833697796\n",
      "[step: 11467] loss: 0.008665882050991058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 11468] loss: 0.008754533715546131\n",
      "[step: 11469] loss: 0.008722765371203423\n",
      "[step: 11470] loss: 0.008714718744158745\n",
      "[step: 11471] loss: 0.008625632151961327\n",
      "[step: 11472] loss: 0.008640258572995663\n",
      "[step: 11473] loss: 0.008620933629572392\n",
      "[step: 11474] loss: 0.008590121753513813\n",
      "[step: 11475] loss: 0.008585442788898945\n",
      "[step: 11476] loss: 0.008591602556407452\n",
      "[step: 11477] loss: 0.008596383966505527\n",
      "[step: 11478] loss: 0.008554885163903236\n",
      "[step: 11479] loss: 0.008539516478776932\n",
      "[step: 11480] loss: 0.008546644821763039\n",
      "[step: 11481] loss: 0.00856388732790947\n",
      "[step: 11482] loss: 0.008636506274342537\n",
      "[step: 11483] loss: 0.008529221639037132\n",
      "[step: 11484] loss: 0.008496423251926899\n",
      "[step: 11485] loss: 0.008480928838253021\n",
      "[step: 11486] loss: 0.008473524823784828\n",
      "[step: 11487] loss: 0.008474561385810375\n",
      "[step: 11488] loss: 0.008522951044142246\n",
      "[step: 11489] loss: 0.008656049147248268\n",
      "[step: 11490] loss: 0.009045773185789585\n",
      "[step: 11491] loss: 0.009053701534867287\n",
      "[step: 11492] loss: 0.00845210999250412\n",
      "[step: 11493] loss: 0.009139543399214745\n",
      "[step: 11494] loss: 0.009413954801857471\n",
      "[step: 11495] loss: 0.010708125308156013\n",
      "[step: 11496] loss: 0.010563359595835209\n",
      "[step: 11497] loss: 0.01035538874566555\n",
      "[step: 11498] loss: 0.009959733113646507\n",
      "[step: 11499] loss: 0.00996358785778284\n",
      "[step: 11500] loss: 0.009691379964351654\n",
      "[step: 11501] loss: 0.00982337724417448\n",
      "[step: 11502] loss: 0.009727937169373035\n",
      "[step: 11503] loss: 0.009741718880832195\n",
      "[step: 11504] loss: 0.01000794768333435\n",
      "[step: 11505] loss: 0.009746525436639786\n",
      "[step: 11506] loss: 0.01005601417273283\n",
      "[step: 11507] loss: 0.010010534897446632\n",
      "[step: 11508] loss: 0.00959559716284275\n",
      "[step: 11509] loss: 0.009529978036880493\n",
      "[step: 11510] loss: 0.00979025661945343\n",
      "[step: 11511] loss: 0.009786844253540039\n",
      "[step: 11512] loss: 0.009571894071996212\n",
      "[step: 11513] loss: 0.00959144625812769\n",
      "[step: 11514] loss: 0.009670939296483994\n",
      "[step: 11515] loss: 0.0096859997138381\n",
      "[step: 11516] loss: 0.009597615338861942\n",
      "[step: 11517] loss: 0.009484430775046349\n",
      "[step: 11518] loss: 0.009508597664535046\n",
      "[step: 11519] loss: 0.009569182060658932\n",
      "[step: 11520] loss: 0.009535213932394981\n",
      "[step: 11521] loss: 0.009429837577044964\n",
      "[step: 11522] loss: 0.00942199770361185\n",
      "[step: 11523] loss: 0.009449880570173264\n",
      "[step: 11524] loss: 0.009442250244319439\n",
      "[step: 11525] loss: 0.009408900514245033\n",
      "[step: 11526] loss: 0.00937960296869278\n",
      "[step: 11527] loss: 0.0093825189396739\n",
      "[step: 11528] loss: 0.009407668374478817\n",
      "[step: 11529] loss: 0.009390147402882576\n",
      "[step: 11530] loss: 0.009349505417048931\n",
      "[step: 11531] loss: 0.009344596415758133\n",
      "[step: 11532] loss: 0.009353340603411198\n",
      "[step: 11533] loss: 0.009348819963634014\n",
      "[step: 11534] loss: 0.009333471767604351\n",
      "[step: 11535] loss: 0.009313434362411499\n",
      "[step: 11536] loss: 0.009314017370343208\n",
      "[step: 11537] loss: 0.009313548915088177\n",
      "[step: 11538] loss: 0.009289582259953022\n",
      "[step: 11539] loss: 0.009278957732021809\n",
      "[step: 11540] loss: 0.009280300699174404\n",
      "[step: 11541] loss: 0.009274957701563835\n",
      "[step: 11542] loss: 0.009260982275009155\n",
      "[step: 11543] loss: 0.009253883734345436\n",
      "[step: 11544] loss: 0.009252356365323067\n",
      "[step: 11545] loss: 0.00923765916377306\n",
      "[step: 11546] loss: 0.009227676317095757\n",
      "[step: 11547] loss: 0.009223716333508492\n",
      "[step: 11548] loss: 0.009213494136929512\n",
      "[step: 11549] loss: 0.009202765300869942\n",
      "[step: 11550] loss: 0.009195882827043533\n",
      "[step: 11551] loss: 0.00918593630194664\n",
      "[step: 11552] loss: 0.009173569269478321\n",
      "[step: 11553] loss: 0.009164904244244099\n",
      "[step: 11554] loss: 0.009155483916401863\n",
      "[step: 11555] loss: 0.00914345495402813\n",
      "[step: 11556] loss: 0.009135126136243343\n",
      "[step: 11557] loss: 0.009125647135078907\n",
      "[step: 11558] loss: 0.009113103151321411\n",
      "[step: 11559] loss: 0.00910405907779932\n",
      "[step: 11560] loss: 0.009094796143472195\n",
      "[step: 11561] loss: 0.00908415112644434\n",
      "[step: 11562] loss: 0.009075632318854332\n",
      "[step: 11563] loss: 0.009068108163774014\n",
      "[step: 11564] loss: 0.009058719500899315\n",
      "[step: 11565] loss: 0.009050795808434486\n",
      "[step: 11566] loss: 0.00904387328773737\n",
      "[step: 11567] loss: 0.00903580617159605\n",
      "[step: 11568] loss: 0.009028903208673\n",
      "[step: 11569] loss: 0.009022483602166176\n",
      "[step: 11570] loss: 0.009015501476824284\n",
      "[step: 11571] loss: 0.009008780121803284\n",
      "[step: 11572] loss: 0.009002646431326866\n",
      "[step: 11573] loss: 0.008996100164949894\n",
      "[step: 11574] loss: 0.008989810943603516\n",
      "[step: 11575] loss: 0.008983943611383438\n",
      "[step: 11576] loss: 0.008977770805358887\n",
      "[step: 11577] loss: 0.008971634320914745\n",
      "[step: 11578] loss: 0.008965685032308102\n",
      "[step: 11579] loss: 0.00895955041050911\n",
      "[step: 11580] loss: 0.00895338412374258\n",
      "[step: 11581] loss: 0.008947483263909817\n",
      "[step: 11582] loss: 0.008941390551626682\n",
      "[step: 11583] loss: 0.00893523171544075\n",
      "[step: 11584] loss: 0.008929194882512093\n",
      "[step: 11585] loss: 0.008923068642616272\n",
      "[step: 11586] loss: 0.008916814811527729\n",
      "[step: 11587] loss: 0.00891069695353508\n",
      "[step: 11588] loss: 0.008904499933123589\n",
      "[step: 11589] loss: 0.008898138999938965\n",
      "[step: 11590] loss: 0.008891824632883072\n",
      "[step: 11591] loss: 0.00888544786721468\n",
      "[step: 11592] loss: 0.008878934197127819\n",
      "[step: 11593] loss: 0.008872420527040958\n",
      "[step: 11594] loss: 0.00886589102447033\n",
      "[step: 11595] loss: 0.00885919202119112\n",
      "[step: 11596] loss: 0.008852478116750717\n",
      "[step: 11597] loss: 0.00884571298956871\n",
      "[step: 11598] loss: 0.008838825859129429\n",
      "[step: 11599] loss: 0.008831876330077648\n",
      "[step: 11600] loss: 0.008824874646961689\n",
      "[step: 11601] loss: 0.008817754685878754\n",
      "[step: 11602] loss: 0.008810541592538357\n",
      "[step: 11603] loss: 0.008803279139101505\n",
      "[step: 11604] loss: 0.008795891888439655\n",
      "[step: 11605] loss: 0.008788421750068665\n",
      "[step: 11606] loss: 0.00878087803721428\n",
      "[step: 11607] loss: 0.008773216977715492\n",
      "[step: 11608] loss: 0.008765466511249542\n",
      "[step: 11609] loss: 0.008757632225751877\n",
      "[step: 11610] loss: 0.008749684318900108\n",
      "[step: 11611] loss: 0.0087416497990489\n",
      "[step: 11612] loss: 0.008733515627682209\n",
      "[step: 11613] loss: 0.008725271560251713\n",
      "[step: 11614] loss: 0.008716940879821777\n",
      "[step: 11615] loss: 0.008708504028618336\n",
      "[step: 11616] loss: 0.008699964731931686\n",
      "[step: 11617] loss: 0.008691343478858471\n",
      "[step: 11618] loss: 0.008682621642947197\n",
      "[step: 11619] loss: 0.00867379829287529\n",
      "[step: 11620] loss: 0.008664890192449093\n",
      "[step: 11621] loss: 0.008655888959765434\n",
      "[step: 11622] loss: 0.008646802976727486\n",
      "[step: 11623] loss: 0.008637642487883568\n",
      "[step: 11624] loss: 0.00862838327884674\n",
      "[step: 11625] loss: 0.008619064465165138\n",
      "[step: 11626] loss: 0.008609674870967865\n",
      "[step: 11627] loss: 0.00860021822154522\n",
      "[step: 11628] loss: 0.008590705692768097\n",
      "[step: 11629] loss: 0.008581139147281647\n",
      "[step: 11630] loss: 0.008571527898311615\n",
      "[step: 11631] loss: 0.008561885915696621\n",
      "[step: 11632] loss: 0.008552205748856068\n",
      "[step: 11633] loss: 0.008542506024241447\n",
      "[step: 11634] loss: 0.008532795123755932\n",
      "[step: 11635] loss: 0.008523081429302692\n",
      "[step: 11636] loss: 0.008513367734849453\n",
      "[step: 11637] loss: 0.008503670804202557\n",
      "[step: 11638] loss: 0.008493995293974876\n",
      "[step: 11639] loss: 0.008484352380037308\n",
      "[step: 11640] loss: 0.008474744856357574\n",
      "[step: 11641] loss: 0.00846518948674202\n",
      "[step: 11642] loss: 0.008455689065158367\n",
      "[step: 11643] loss: 0.00844626221805811\n",
      "[step: 11644] loss: 0.008436904288828373\n",
      "[step: 11645] loss: 0.008427634835243225\n",
      "[step: 11646] loss: 0.008418449200689793\n",
      "[step: 11647] loss: 0.00840936042368412\n",
      "[step: 11648] loss: 0.008400374092161655\n",
      "[step: 11649] loss: 0.008391493000090122\n",
      "[step: 11650] loss: 0.008382733911275864\n",
      "[step: 11651] loss: 0.008374087512493134\n",
      "[step: 11652] loss: 0.008365561254322529\n",
      "[step: 11653] loss: 0.008357160724699497\n",
      "[step: 11654] loss: 0.008348893374204636\n",
      "[step: 11655] loss: 0.00834075640887022\n",
      "[step: 11656] loss: 0.008332750760018826\n",
      "[step: 11657] loss: 0.008324877358973026\n",
      "[step: 11658] loss: 0.00831714179366827\n",
      "[step: 11659] loss: 0.008309543132781982\n",
      "[step: 11660] loss: 0.008302081376314163\n",
      "[step: 11661] loss: 0.008294761180877686\n",
      "[step: 11662] loss: 0.008287576958537102\n",
      "[step: 11663] loss: 0.008280524052679539\n",
      "[step: 11664] loss: 0.008273608982563019\n",
      "[step: 11665] loss: 0.008266831748187542\n",
      "[step: 11666] loss: 0.008260187692940235\n",
      "[step: 11667] loss: 0.00825368333607912\n",
      "[step: 11668] loss: 0.008247306570410728\n",
      "[step: 11669] loss: 0.008241081610321999\n",
      "[step: 11670] loss: 0.008234996348619461\n",
      "[step: 11671] loss: 0.008229113183915615\n",
      "[step: 11672] loss: 0.008223422802984715\n",
      "[step: 11673] loss: 0.008218164555728436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 11674] loss: 0.008213303983211517\n",
      "[step: 11675] loss: 0.00821002945303917\n",
      "[step: 11676] loss: 0.00820787064731121\n",
      "[step: 11677] loss: 0.008214103989303112\n",
      "[step: 11678] loss: 0.008226258680224419\n",
      "[step: 11679] loss: 0.008316163904964924\n",
      "[step: 11680] loss: 0.00851333886384964\n",
      "[step: 11681] loss: 0.013404530473053455\n",
      "[step: 11682] loss: 0.014283861964941025\n",
      "[step: 11683] loss: 0.00933301541954279\n",
      "[step: 11684] loss: 0.013101696968078613\n",
      "[step: 11685] loss: 0.010093280114233494\n",
      "[step: 11686] loss: 0.01067597046494484\n",
      "[step: 11687] loss: 0.010645211674273014\n",
      "[step: 11688] loss: 0.009281669743359089\n",
      "[step: 11689] loss: 0.010427279397845268\n",
      "[step: 11690] loss: 0.009272357448935509\n",
      "[step: 11691] loss: 0.009725856594741344\n",
      "[step: 11692] loss: 0.08338215202093124\n",
      "[step: 11693] loss: 0.17685352265834808\n",
      "[step: 11694] loss: 0.023952629417181015\n",
      "[step: 11695] loss: 0.03976373001933098\n",
      "[step: 11696] loss: 0.03006700798869133\n",
      "[step: 11697] loss: 0.06075282767415047\n",
      "[step: 11698] loss: 0.07447899132966995\n",
      "[step: 11699] loss: 0.06699178367853165\n",
      "[step: 11700] loss: 0.09603232145309448\n",
      "[step: 11701] loss: 0.11944901943206787\n",
      "[step: 11702] loss: 0.12454845011234283\n",
      "[step: 11703] loss: 0.07024441659450531\n",
      "[step: 11704] loss: 0.08058575540781021\n",
      "[step: 11705] loss: 0.08472432941198349\n",
      "[step: 11706] loss: 0.07304650545120239\n",
      "[step: 11707] loss: 0.0637831762433052\n",
      "[step: 11708] loss: 0.05516798049211502\n",
      "[step: 11709] loss: 0.05301371589303017\n",
      "[step: 11710] loss: 0.07595203071832657\n",
      "[step: 11711] loss: 0.04444889351725578\n",
      "[step: 11712] loss: 0.02492012456059456\n",
      "[step: 11713] loss: 0.033721890300512314\n",
      "[step: 11714] loss: 0.036069199442863464\n",
      "[step: 11715] loss: 0.03254173323512077\n",
      "[step: 11716] loss: 0.0387127660214901\n",
      "[step: 11717] loss: 0.043207839131355286\n",
      "[step: 11718] loss: 0.023898443207144737\n",
      "[step: 11719] loss: 0.02521042339503765\n",
      "[step: 11720] loss: 0.028328359127044678\n",
      "[step: 11721] loss: 0.027067577466368675\n",
      "[step: 11722] loss: 0.04212062805891037\n",
      "[step: 11723] loss: 0.02470124140381813\n",
      "[step: 11724] loss: 0.019948497414588928\n",
      "[step: 11725] loss: 0.01656930334866047\n",
      "[step: 11726] loss: 0.01683708094060421\n",
      "[step: 11727] loss: 0.017992980778217316\n",
      "[step: 11728] loss: 0.017170289531350136\n",
      "[step: 11729] loss: 0.017466912046074867\n",
      "[step: 11730] loss: 0.0166005939245224\n",
      "[step: 11731] loss: 0.016867604106664658\n",
      "[step: 11732] loss: 0.01603173464536667\n",
      "[step: 11733] loss: 0.014578036032617092\n",
      "[step: 11734] loss: 0.01633281446993351\n",
      "[step: 11735] loss: 0.015046843327581882\n",
      "[step: 11736] loss: 0.015190597623586655\n",
      "[step: 11737] loss: 0.014843640848994255\n",
      "[step: 11738] loss: 0.014126553200185299\n",
      "[step: 11739] loss: 0.01365911029279232\n",
      "[step: 11740] loss: 0.012965959496796131\n",
      "[step: 11741] loss: 0.012509510852396488\n",
      "[step: 11742] loss: 0.012642685323953629\n",
      "[step: 11743] loss: 0.012515129521489143\n",
      "[step: 11744] loss: 0.012379138730466366\n",
      "[step: 11745] loss: 0.0123257115483284\n",
      "[step: 11746] loss: 0.012298965826630592\n",
      "[step: 11747] loss: 0.012255950830876827\n",
      "[step: 11748] loss: 0.01217949390411377\n",
      "[step: 11749] loss: 0.012075548060238361\n",
      "[step: 11750] loss: 0.011956986971199512\n",
      "[step: 11751] loss: 0.011832879856228828\n",
      "[step: 11752] loss: 0.011707012541592121\n",
      "[step: 11753] loss: 0.01158474013209343\n",
      "[step: 11754] loss: 0.011482448317110538\n",
      "[step: 11755] loss: 0.011414745822548866\n",
      "[step: 11756] loss: 0.011377756483852863\n",
      "[step: 11757] loss: 0.011360560543835163\n",
      "[step: 11758] loss: 0.011352562345564365\n",
      "[step: 11759] loss: 0.01134511362761259\n",
      "[step: 11760] loss: 0.011333167552947998\n",
      "[step: 11761] loss: 0.011315318755805492\n",
      "[step: 11762] loss: 0.011292257346212864\n",
      "[step: 11763] loss: 0.011265018954873085\n",
      "[step: 11764] loss: 0.01123411487787962\n",
      "[step: 11765] loss: 0.011199841275811195\n",
      "[step: 11766] loss: 0.011163090355694294\n",
      "[step: 11767] loss: 0.011125699616968632\n",
      "[step: 11768] loss: 0.01108989492058754\n",
      "[step: 11769] loss: 0.011057271622121334\n",
      "[step: 11770] loss: 0.011028093285858631\n",
      "[step: 11771] loss: 0.011001542210578918\n",
      "[step: 11772] loss: 0.010976511985063553\n",
      "[step: 11773] loss: 0.010952350683510303\n",
      "[step: 11774] loss: 0.010929008014500141\n",
      "[step: 11775] loss: 0.010906726121902466\n",
      "[step: 11776] loss: 0.010885692201554775\n",
      "[step: 11777] loss: 0.010866005904972553\n",
      "[step: 11778] loss: 0.010847926139831543\n",
      "[step: 11779] loss: 0.010831930674612522\n",
      "[step: 11780] loss: 0.010818511247634888\n",
      "[step: 11781] loss: 0.010807746089994907\n",
      "[step: 11782] loss: 0.010799123905599117\n",
      "[step: 11783] loss: 0.010791595093905926\n",
      "[step: 11784] loss: 0.010784054175019264\n",
      "[step: 11785] loss: 0.010775784030556679\n",
      "[step: 11786] loss: 0.010766657069325447\n",
      "[step: 11787] loss: 0.010756992734968662\n",
      "[step: 11788] loss: 0.01074717752635479\n",
      "[step: 11789] loss: 0.010737481527030468\n",
      "[step: 11790] loss: 0.01072796992957592\n",
      "[step: 11791] loss: 0.010718688368797302\n",
      "[step: 11792] loss: 0.010709675028920174\n",
      "[step: 11793] loss: 0.01070095133036375\n",
      "[step: 11794] loss: 0.010692453943192959\n",
      "[step: 11795] loss: 0.010684045031666756\n",
      "[step: 11796] loss: 0.010675627738237381\n",
      "[step: 11797] loss: 0.010667207650840282\n",
      "[step: 11798] loss: 0.01065891608595848\n",
      "[step: 11799] loss: 0.01065094769001007\n",
      "[step: 11800] loss: 0.010643402114510536\n",
      "[step: 11801] loss: 0.0106362858787179\n",
      "[step: 11802] loss: 0.010629530064761639\n",
      "[step: 11803] loss: 0.010623031295835972\n",
      "[step: 11804] loss: 0.010616707615554333\n",
      "[step: 11805] loss: 0.010610517114400864\n",
      "[step: 11806] loss: 0.010604393668472767\n",
      "[step: 11807] loss: 0.010598301887512207\n",
      "[step: 11808] loss: 0.010592215694487095\n",
      "[step: 11809] loss: 0.010586133226752281\n",
      "[step: 11810] loss: 0.010580094531178474\n",
      "[step: 11811] loss: 0.010574125684797764\n",
      "[step: 11812] loss: 0.01056822668761015\n",
      "[step: 11813] loss: 0.010562382638454437\n",
      "[step: 11814] loss: 0.01055654976516962\n",
      "[step: 11815] loss: 0.01055071223527193\n",
      "[step: 11816] loss: 0.010544869117438793\n",
      "[step: 11817] loss: 0.010539036244153976\n",
      "[step: 11818] loss: 0.010533221997320652\n",
      "[step: 11819] loss: 0.010527442209422588\n",
      "[step: 11820] loss: 0.01052170991897583\n",
      "[step: 11821] loss: 0.010516027919948101\n",
      "[step: 11822] loss: 0.010510403662919998\n",
      "[step: 11823] loss: 0.010504839941859245\n",
      "[step: 11824] loss: 0.010499324649572372\n",
      "[step: 11825] loss: 0.010493854992091656\n",
      "[step: 11826] loss: 0.010488422587513924\n",
      "[step: 11827] loss: 0.010483022779226303\n",
      "[step: 11828] loss: 0.01047765463590622\n",
      "[step: 11829] loss: 0.010472334921360016\n",
      "[step: 11830] loss: 0.010467047803103924\n",
      "[step: 11831] loss: 0.010461798869073391\n",
      "[step: 11832] loss: 0.010456579737365246\n",
      "[step: 11833] loss: 0.01045138482004404\n",
      "[step: 11834] loss: 0.010446215979754925\n",
      "[step: 11835] loss: 0.010441066697239876\n",
      "[step: 11836] loss: 0.010435938835144043\n",
      "[step: 11837] loss: 0.010430827736854553\n",
      "[step: 11838] loss: 0.010425739921629429\n",
      "[step: 11839] loss: 0.010420671664178371\n",
      "[step: 11840] loss: 0.010415625758469105\n",
      "[step: 11841] loss: 0.01041059847921133\n",
      "[step: 11842] loss: 0.010405595414340496\n",
      "[step: 11843] loss: 0.010400617495179176\n",
      "[step: 11844] loss: 0.010395647957921028\n",
      "[step: 11845] loss: 0.010390710085630417\n",
      "[step: 11846] loss: 0.010385786183178425\n",
      "[step: 11847] loss: 0.010380888357758522\n",
      "[step: 11848] loss: 0.010376008227467537\n",
      "[step: 11849] loss: 0.010371150448918343\n",
      "[step: 11850] loss: 0.010366310365498066\n",
      "[step: 11851] loss: 0.010361488908529282\n",
      "[step: 11852] loss: 0.010356692597270012\n",
      "[step: 11853] loss: 0.010351911187171936\n",
      "[step: 11854] loss: 0.010347147472202778\n",
      "[step: 11855] loss: 0.010342398658394814\n",
      "[step: 11856] loss: 0.010337664745748043\n",
      "[step: 11857] loss: 0.010332955047488213\n",
      "[step: 11858] loss: 0.010328260250389576\n",
      "[step: 11859] loss: 0.010323578491806984\n",
      "[step: 11860] loss: 0.010318918153643608\n",
      "[step: 11861] loss: 0.010314271785318851\n",
      "[step: 11862] loss: 0.010309637524187565\n",
      "[step: 11863] loss: 0.010305024683475494\n",
      "[step: 11864] loss: 0.010300423949956894\n",
      "[step: 11865] loss: 0.010295839980244637\n",
      "[step: 11866] loss: 0.010291272774338722\n",
      "[step: 11867] loss: 0.010286719538271427\n",
      "[step: 11868] loss: 0.0102821821346879\n",
      "[step: 11869] loss: 0.010277657769620419\n",
      "[step: 11870] loss: 0.010273149237036705\n",
      "[step: 11871] loss: 0.010268657468259335\n",
      "[step: 11872] loss: 0.010264181531965733\n",
      "[step: 11873] loss: 0.0102597177028656\n",
      "[step: 11874] loss: 0.010255268774926662\n",
      "[step: 11875] loss: 0.010250832885503769\n",
      "[step: 11876] loss: 0.010246419347822666\n",
      "[step: 11877] loss: 0.010242016986012459\n",
      "[step: 11878] loss: 0.01023762859404087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 11879] loss: 0.010233256034553051\n",
      "[step: 11880] loss: 0.010228898376226425\n",
      "[step: 11881] loss: 0.010224559344351292\n",
      "[step: 11882] loss: 0.01022022869437933\n",
      "[step: 11883] loss: 0.01021591853350401\n",
      "[step: 11884] loss: 0.010211619548499584\n",
      "[step: 11885] loss: 0.010207338258624077\n",
      "[step: 11886] loss: 0.010203072801232338\n",
      "[step: 11887] loss: 0.010198823176324368\n",
      "[step: 11888] loss: 0.010194581933319569\n",
      "[step: 11889] loss: 0.010190358385443687\n",
      "[step: 11890] loss: 0.010186155326664448\n",
      "[step: 11891] loss: 0.010181963443756104\n",
      "[step: 11892] loss: 0.010177790187299252\n",
      "[step: 11893] loss: 0.010173631832003593\n",
      "[step: 11894] loss: 0.01016948837786913\n",
      "[step: 11895] loss: 0.010165357030928135\n",
      "[step: 11896] loss: 0.010161248967051506\n",
      "[step: 11897] loss: 0.010157149285078049\n",
      "[step: 11898] loss: 0.010153071023523808\n",
      "[step: 11899] loss: 0.010149004869163036\n",
      "[step: 11900] loss: 0.010144959203898907\n",
      "[step: 11901] loss: 0.01014093030244112\n",
      "[step: 11902] loss: 0.010136912576854229\n",
      "[step: 11903] loss: 0.010132917203009129\n",
      "[step: 11904] loss: 0.010128937661647797\n",
      "[step: 11905] loss: 0.010124973021447659\n",
      "[step: 11906] loss: 0.010121023282408714\n",
      "[step: 11907] loss: 0.010117091238498688\n",
      "[step: 11908] loss: 0.010113176889717579\n",
      "[step: 11909] loss: 0.010109277442097664\n",
      "[step: 11910] loss: 0.010105401277542114\n",
      "[step: 11911] loss: 0.01010153815150261\n",
      "[step: 11912] loss: 0.01009769830852747\n",
      "[step: 11913] loss: 0.010093866847455502\n",
      "[step: 11914] loss: 0.010090057738125324\n",
      "[step: 11915] loss: 0.010086271911859512\n",
      "[step: 11916] loss: 0.010082497261464596\n",
      "[step: 11917] loss: 0.010078739374876022\n",
      "[step: 11918] loss: 0.010075002908706665\n",
      "[step: 11919] loss: 0.010071281343698502\n",
      "[step: 11920] loss: 0.010067583993077278\n",
      "[step: 11921] loss: 0.010063902474939823\n",
      "[step: 11922] loss: 0.010060238651931286\n",
      "[step: 11923] loss: 0.010056590661406517\n",
      "[step: 11924] loss: 0.010052965022623539\n",
      "[step: 11925] loss: 0.01004935521632433\n",
      "[step: 11926] loss: 0.010045768693089485\n",
      "[step: 11927] loss: 0.010042200796306133\n",
      "[step: 11928] loss: 0.010038651525974274\n",
      "[step: 11929] loss: 0.010035119950771332\n",
      "[step: 11930] loss: 0.01003161072731018\n",
      "[step: 11931] loss: 0.010028115473687649\n",
      "[step: 11932] loss: 0.010024642571806908\n",
      "[step: 11933] loss: 0.010021187365055084\n",
      "[step: 11934] loss: 0.0100177563726902\n",
      "[step: 11935] loss: 0.010014343075454235\n",
      "[step: 11936] loss: 0.010010945610702038\n",
      "[step: 11937] loss: 0.01000757236033678\n",
      "[step: 11938] loss: 0.010004223324358463\n",
      "[step: 11939] loss: 0.01000088732689619\n",
      "[step: 11940] loss: 0.00999757181853056\n",
      "[step: 11941] loss: 0.00999427493661642\n",
      "[step: 11942] loss: 0.009991006925702095\n",
      "[step: 11943] loss: 0.009987750090658665\n",
      "[step: 11944] loss: 0.009984518401324749\n",
      "[step: 11945] loss: 0.009981303475797176\n",
      "[step: 11946] loss: 0.00997810997068882\n",
      "[step: 11947] loss: 0.00997493788599968\n",
      "[step: 11948] loss: 0.009971780702471733\n",
      "[step: 11949] loss: 0.009968654252588749\n",
      "[step: 11950] loss: 0.00996553897857666\n",
      "[step: 11951] loss: 0.009962447918951511\n",
      "[step: 11952] loss: 0.009959375485777855\n",
      "[step: 11953] loss: 0.00995632354170084\n",
      "[step: 11954] loss: 0.009953290224075317\n",
      "[step: 11955] loss: 0.00995028205215931\n",
      "[step: 11956] loss: 0.00994728971272707\n",
      "[step: 11957] loss: 0.009944317862391472\n",
      "[step: 11958] loss: 0.009941364638507366\n",
      "[step: 11959] loss: 0.009938434697687626\n",
      "[step: 11960] loss: 0.009935525245964527\n",
      "[step: 11961] loss: 0.009932630695402622\n",
      "[step: 11962] loss: 0.009929758496582508\n",
      "[step: 11963] loss: 0.009926905855536461\n",
      "[step: 11964] loss: 0.009924071840941906\n",
      "[step: 11965] loss: 0.009921261109411716\n",
      "[step: 11966] loss: 0.009918468073010445\n",
      "[step: 11967] loss: 0.009915689937770367\n",
      "[step: 11968] loss: 0.009912935085594654\n",
      "[step: 11969] loss: 0.009910198859870434\n",
      "[step: 11970] loss: 0.009907476603984833\n",
      "[step: 11971] loss: 0.009904779493808746\n",
      "[step: 11972] loss: 0.009902098216116428\n",
      "[step: 11973] loss: 0.009899434633553028\n",
      "[step: 11974] loss: 0.00989679154008627\n",
      "[step: 11975] loss: 0.009894167073071003\n",
      "[step: 11976] loss: 0.009891558438539505\n",
      "[step: 11977] loss: 0.0098889684304595\n",
      "[step: 11978] loss: 0.009886392392218113\n",
      "[step: 11979] loss: 0.009883839637041092\n",
      "[step: 11980] loss: 0.009881299920380116\n",
      "[step: 11981] loss: 0.009878779761493206\n",
      "[step: 11982] loss: 0.009876277297735214\n",
      "[step: 11983] loss: 0.009873790666460991\n",
      "[step: 11984] loss: 0.009871318936347961\n",
      "[step: 11985] loss: 0.00986886490136385\n",
      "[step: 11986] loss: 0.009866427630186081\n",
      "[step: 11987] loss: 0.00986400805413723\n",
      "[step: 11988] loss: 0.009861600585281849\n",
      "[step: 11989] loss: 0.00985921360552311\n",
      "[step: 11990] loss: 0.00985684059560299\n",
      "[step: 11991] loss: 0.009854476898908615\n",
      "[step: 11992] loss: 0.009852137416601181\n",
      "[step: 11993] loss: 0.009849808178842068\n",
      "[step: 11994] loss: 0.009847493842244148\n",
      "[step: 11995] loss: 0.009845193475484848\n",
      "[step: 11996] loss: 0.00984291173517704\n",
      "[step: 11997] loss: 0.009840640239417553\n",
      "[step: 11998] loss: 0.00983838364481926\n",
      "[step: 11999] loss: 0.00983614195138216\n",
      "[step: 12000] loss: 0.00983391236513853\n",
      "[step: 12001] loss: 0.009831700474023819\n",
      "[step: 12002] loss: 0.009829496033489704\n",
      "[step: 12003] loss: 0.009827308356761932\n",
      "[step: 12004] loss: 0.009825133718550205\n",
      "[step: 12005] loss: 0.009822972118854523\n",
      "[step: 12006] loss: 0.009820819832384586\n",
      "[step: 12007] loss: 0.009818684309720993\n",
      "[step: 12008] loss: 0.009816554374992847\n",
      "[step: 12009] loss: 0.009814439341425896\n",
      "[step: 12010] loss: 0.009812340140342712\n",
      "[step: 12011] loss: 0.009810252115130424\n",
      "[step: 12012] loss: 0.009808170609176159\n",
      "[step: 12013] loss: 0.009806103073060513\n",
      "[step: 12014] loss: 0.009804047644138336\n",
      "[step: 12015] loss: 0.00980200432240963\n",
      "[step: 12016] loss: 0.009799972176551819\n",
      "[step: 12017] loss: 0.00979794841259718\n",
      "[step: 12018] loss: 0.009795935824513435\n",
      "[step: 12019] loss: 0.009793932549655437\n",
      "[step: 12020] loss: 0.009791940450668335\n",
      "[step: 12021] loss: 0.009789958596229553\n",
      "[step: 12022] loss: 0.009787986055016518\n",
      "[step: 12023] loss: 0.009786020964384079\n",
      "[step: 12024] loss: 0.009784072637557983\n",
      "[step: 12025] loss: 0.009782128036022186\n",
      "[step: 12026] loss: 0.009780194610357285\n",
      "[step: 12027] loss: 0.009778269566595554\n",
      "[step: 12028] loss: 0.009776352904736996\n",
      "[step: 12029] loss: 0.009774447418749332\n",
      "[step: 12030] loss: 0.009772551245987415\n",
      "[step: 12031] loss: 0.00977066159248352\n",
      "[step: 12032] loss: 0.009768781252205372\n",
      "[step: 12033] loss: 0.009766904637217522\n",
      "[step: 12034] loss: 0.009765042923390865\n",
      "[step: 12035] loss: 0.009763185866177082\n",
      "[step: 12036] loss: 0.009761340916156769\n",
      "[step: 12037] loss: 0.00975949689745903\n",
      "[step: 12038] loss: 0.009757667779922485\n",
      "[step: 12039] loss: 0.009755839593708515\n",
      "[step: 12040] loss: 0.009754023514688015\n",
      "[step: 12041] loss: 0.009752212092280388\n",
      "[step: 12042] loss: 0.009750408120453358\n",
      "[step: 12043] loss: 0.009748613461852074\n",
      "[step: 12044] loss: 0.009746827185153961\n",
      "[step: 12045] loss: 0.009745044633746147\n",
      "[step: 12046] loss: 0.009743272326886654\n",
      "[step: 12047] loss: 0.009741502813994884\n",
      "[step: 12048] loss: 0.009739743545651436\n",
      "[step: 12049] loss: 0.00973798893392086\n",
      "[step: 12050] loss: 0.009736240841448307\n",
      "[step: 12051] loss: 0.00973450019955635\n",
      "[step: 12052] loss: 0.009732766076922417\n",
      "[step: 12053] loss: 0.009731036610901356\n",
      "[step: 12054] loss: 0.009729311801493168\n",
      "[step: 12055] loss: 0.009727591648697853\n",
      "[step: 12056] loss: 0.009725878946483135\n",
      "[step: 12057] loss: 0.009724177420139313\n",
      "[step: 12058] loss: 0.009722474962472916\n",
      "[step: 12059] loss: 0.009720785543322563\n",
      "[step: 12060] loss: 0.009719092398881912\n",
      "[step: 12061] loss: 0.009717414155602455\n",
      "[step: 12062] loss: 0.009715733118355274\n",
      "[step: 12063] loss: 0.009714062325656414\n",
      "[step: 12064] loss: 0.009712396189570427\n",
      "[step: 12065] loss: 0.009710733778774738\n",
      "[step: 12066] loss: 0.009709072299301624\n",
      "[step: 12067] loss: 0.009707423858344555\n",
      "[step: 12068] loss: 0.00970577634871006\n",
      "[step: 12069] loss: 0.009704134427011013\n",
      "[step: 12070] loss: 0.009702496230602264\n",
      "[step: 12071] loss: 0.009700864553451538\n",
      "[step: 12072] loss: 0.009699235670268536\n",
      "[step: 12073] loss: 0.009697617031633854\n",
      "[step: 12074] loss: 0.009695998392999172\n",
      "[step: 12075] loss: 0.009694380685687065\n",
      "[step: 12076] loss: 0.009692774154245853\n",
      "[step: 12077] loss: 0.009691167622804642\n",
      "[step: 12078] loss: 0.009689566679298878\n",
      "[step: 12079] loss: 0.009687968529760838\n",
      "[step: 12080] loss: 0.009686378762125969\n",
      "[step: 12081] loss: 0.009684787131845951\n",
      "[step: 12082] loss: 0.009683203883469105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 12083] loss: 0.009681628085672855\n",
      "[step: 12084] loss: 0.009680046699941158\n",
      "[step: 12085] loss: 0.009678473696112633\n",
      "[step: 12086] loss: 0.00967690534889698\n",
      "[step: 12087] loss: 0.009675341658294201\n",
      "[step: 12088] loss: 0.00967377983033657\n",
      "[step: 12089] loss: 0.009672223590314388\n",
      "[step: 12090] loss: 0.009670666418969631\n",
      "[step: 12091] loss: 0.009669112972915173\n",
      "[step: 12092] loss: 0.009667569771409035\n",
      "[step: 12093] loss: 0.009666026569902897\n",
      "[step: 12094] loss: 0.009664485231041908\n",
      "[step: 12095] loss: 0.009662948548793793\n",
      "[step: 12096] loss: 0.009661414660513401\n",
      "[step: 12097] loss: 0.009659884497523308\n",
      "[step: 12098] loss: 0.009658361785113811\n",
      "[step: 12099] loss: 0.009656837210059166\n",
      "[step: 12100] loss: 0.009655313566327095\n",
      "[step: 12101] loss: 0.00965379923582077\n",
      "[step: 12102] loss: 0.009652282111346722\n",
      "[step: 12103] loss: 0.009650769643485546\n",
      "[step: 12104] loss: 0.009649262763559818\n",
      "[step: 12105] loss: 0.009647754020988941\n",
      "[step: 12106] loss: 0.009646253660321236\n",
      "[step: 12107] loss: 0.00964475516229868\n",
      "[step: 12108] loss: 0.009643256664276123\n",
      "[step: 12109] loss: 0.009641759097576141\n",
      "[step: 12110] loss: 0.009640268050134182\n",
      "[step: 12111] loss: 0.009638777934014797\n",
      "[step: 12112] loss: 0.009637292474508286\n",
      "[step: 12113] loss: 0.0096358060836792\n",
      "[step: 12114] loss: 0.009634326212108135\n",
      "[step: 12115] loss: 0.009632845409214497\n",
      "[step: 12116] loss: 0.009631370194256306\n",
      "[step: 12117] loss: 0.009629896841943264\n",
      "[step: 12118] loss: 0.009628420695662498\n",
      "[step: 12119] loss: 0.009626953862607479\n",
      "[step: 12120] loss: 0.00962548702955246\n",
      "[step: 12121] loss: 0.00962402205914259\n",
      "[step: 12122] loss: 0.009622560814023018\n",
      "[step: 12123] loss: 0.00962110236287117\n",
      "[step: 12124] loss: 0.009619638323783875\n",
      "[step: 12125] loss: 0.009618183597922325\n",
      "[step: 12126] loss: 0.009616727940738201\n",
      "[step: 12127] loss: 0.009615276008844376\n",
      "[step: 12128] loss: 0.009613826870918274\n",
      "[step: 12129] loss: 0.009612380526959896\n",
      "[step: 12130] loss: 0.00961093045771122\n",
      "[step: 12131] loss: 0.009609485976397991\n",
      "[step: 12132] loss: 0.00960804708302021\n",
      "[step: 12133] loss: 0.009606605395674706\n",
      "[step: 12134] loss: 0.009605166502296925\n",
      "[step: 12135] loss: 0.009603730402886868\n",
      "[step: 12136] loss: 0.009602292440831661\n",
      "[step: 12137] loss: 0.009600860998034477\n",
      "[step: 12138] loss: 0.009599428623914719\n",
      "[step: 12139] loss: 0.009597999043762684\n",
      "[step: 12140] loss: 0.009596571326255798\n",
      "[step: 12141] loss: 0.009595142677426338\n",
      "[step: 12142] loss: 0.009593719616532326\n",
      "[step: 12143] loss: 0.009592295624315739\n",
      "[step: 12144] loss: 0.009590874426066875\n",
      "[step: 12145] loss: 0.009589452296495438\n",
      "[step: 12146] loss: 0.009588031098246574\n",
      "[step: 12147] loss: 0.009586617350578308\n",
      "[step: 12148] loss: 0.009585201740264893\n",
      "[step: 12149] loss: 0.009583786129951477\n",
      "[step: 12150] loss: 0.009582375176250935\n",
      "[step: 12151] loss: 0.009580961428582668\n",
      "[step: 12152] loss: 0.0095795514062047\n",
      "[step: 12153] loss: 0.009578144177794456\n",
      "[step: 12154] loss: 0.009576736018061638\n",
      "[step: 12155] loss: 0.009575328789651394\n",
      "[step: 12156] loss: 0.009573928080499172\n",
      "[step: 12157] loss: 0.009572521783411503\n",
      "[step: 12158] loss: 0.009571120142936707\n",
      "[step: 12159] loss: 0.00956971850246191\n",
      "[step: 12160] loss: 0.009568317793309689\n",
      "[step: 12161] loss: 0.009566918946802616\n",
      "[step: 12162] loss: 0.009565519168972969\n",
      "[step: 12163] loss: 0.009564126841723919\n",
      "[step: 12164] loss: 0.009562731720507145\n",
      "[step: 12165] loss: 0.009561334736645222\n",
      "[step: 12166] loss: 0.009559942409396172\n",
      "[step: 12167] loss: 0.009558549150824547\n",
      "[step: 12168] loss: 0.009557158686220646\n",
      "[step: 12169] loss: 0.00955576915293932\n",
      "[step: 12170] loss: 0.009554379619657993\n",
      "[step: 12171] loss: 0.009552990086376667\n",
      "[step: 12172] loss: 0.00955160427838564\n",
      "[step: 12173] loss: 0.009550219401717186\n",
      "[step: 12174] loss: 0.009548836387693882\n",
      "[step: 12175] loss: 0.009547453373670578\n",
      "[step: 12176] loss: 0.009546066634356976\n",
      "[step: 12177] loss: 0.00954468548297882\n",
      "[step: 12178] loss: 0.009543306194245815\n",
      "[step: 12179] loss: 0.00954192504286766\n",
      "[step: 12180] loss: 0.00954054482281208\n",
      "[step: 12181] loss: 0.0095391646027565\n",
      "[step: 12182] loss: 0.009537788107991219\n",
      "[step: 12183] loss: 0.00953641440719366\n",
      "[step: 12184] loss: 0.00953503418713808\n",
      "[step: 12185] loss: 0.009533662348985672\n",
      "[step: 12186] loss: 0.00953228585422039\n",
      "[step: 12187] loss: 0.009530915878713131\n",
      "[step: 12188] loss: 0.009529540315270424\n",
      "[step: 12189] loss: 0.00952816940844059\n",
      "[step: 12190] loss: 0.009526796638965607\n",
      "[step: 12191] loss: 0.009525424800813198\n",
      "[step: 12192] loss: 0.009524058550596237\n",
      "[step: 12193] loss: 0.009522686712443829\n",
      "[step: 12194] loss: 0.009521321393549442\n",
      "[step: 12195] loss: 0.009519954212009907\n",
      "[step: 12196] loss: 0.009518587030470371\n",
      "[step: 12197] loss: 0.00951722078025341\n",
      "[step: 12198] loss: 0.009515859186649323\n",
      "[step: 12199] loss: 0.009514492936432362\n",
      "[step: 12200] loss: 0.00951312854886055\n",
      "[step: 12201] loss: 0.009511766023933887\n",
      "[step: 12202] loss: 0.009510406292974949\n",
      "[step: 12203] loss: 0.009509042836725712\n",
      "[step: 12204] loss: 0.0095076784491539\n",
      "[step: 12205] loss: 0.00950632058084011\n",
      "[step: 12206] loss: 0.00950495433062315\n",
      "[step: 12207] loss: 0.009503602050244808\n",
      "[step: 12208] loss: 0.009502243250608444\n",
      "[step: 12209] loss: 0.009500885382294655\n",
      "[step: 12210] loss: 0.00949952844530344\n",
      "[step: 12211] loss: 0.0094981724396348\n",
      "[step: 12212] loss: 0.00949681457132101\n",
      "[step: 12213] loss: 0.009495464153587818\n",
      "[step: 12214] loss: 0.009494107216596603\n",
      "[step: 12215] loss: 0.009492753073573112\n",
      "[step: 12216] loss: 0.00949140079319477\n",
      "[step: 12217] loss: 0.00949004665017128\n",
      "[step: 12218] loss: 0.009488694369792938\n",
      "[step: 12219] loss: 0.009487342089414597\n",
      "[step: 12220] loss: 0.009485994465649128\n",
      "[step: 12221] loss: 0.009484644047915936\n",
      "[step: 12222] loss: 0.00948328897356987\n",
      "[step: 12223] loss: 0.00948194321244955\n",
      "[step: 12224] loss: 0.009480593726038933\n",
      "[step: 12225] loss: 0.009479247033596039\n",
      "[step: 12226] loss: 0.00947789940983057\n",
      "[step: 12227] loss: 0.009476551786065102\n",
      "[step: 12228] loss: 0.009475208818912506\n",
      "[step: 12229] loss: 0.009473861195147038\n",
      "[step: 12230] loss: 0.009472520090639591\n",
      "[step: 12231] loss: 0.009471173398196697\n",
      "[step: 12232] loss: 0.009469831362366676\n",
      "[step: 12233] loss: 0.009468485601246357\n",
      "[step: 12234] loss: 0.00946714635938406\n",
      "[step: 12235] loss: 0.009465799666941166\n",
      "[step: 12236] loss: 0.009464461356401443\n",
      "[step: 12237] loss: 0.009463122114539146\n",
      "[step: 12238] loss: 0.0094617810100317\n",
      "[step: 12239] loss: 0.009460441768169403\n",
      "[step: 12240] loss: 0.009459104388952255\n",
      "[step: 12241] loss: 0.009457766078412533\n",
      "[step: 12242] loss: 0.009456423111259937\n",
      "[step: 12243] loss: 0.009455091319978237\n",
      "[step: 12244] loss: 0.00945375021547079\n",
      "[step: 12245] loss: 0.009452415630221367\n",
      "[step: 12246] loss: 0.009451080113649368\n",
      "[step: 12247] loss: 0.009449748322367668\n",
      "[step: 12248] loss: 0.009448413737118244\n",
      "[step: 12249] loss: 0.00944708101451397\n",
      "[step: 12250] loss: 0.009445746429264545\n",
      "[step: 12251] loss: 0.00944441370666027\n",
      "[step: 12252] loss: 0.00944308377802372\n",
      "[step: 12253] loss: 0.009441752918064594\n",
      "[step: 12254] loss: 0.00944042019546032\n",
      "[step: 12255] loss: 0.009439096786081791\n",
      "[step: 12256] loss: 0.009437765926122665\n",
      "[step: 12257] loss: 0.009436434134840965\n",
      "[step: 12258] loss: 0.009435108862817287\n",
      "[step: 12259] loss: 0.009433777071535587\n",
      "[step: 12260] loss: 0.009432454593479633\n",
      "[step: 12261] loss: 0.009431127458810806\n",
      "[step: 12262] loss: 0.009429804980754852\n",
      "[step: 12263] loss: 0.0094284787774086\n",
      "[step: 12264] loss: 0.009427152574062347\n",
      "[step: 12265] loss: 0.009425831027328968\n",
      "[step: 12266] loss: 0.009424506686627865\n",
      "[step: 12267] loss: 0.00942318607121706\n",
      "[step: 12268] loss: 0.009421862661838531\n",
      "[step: 12269] loss: 0.009420542977750301\n",
      "[step: 12270] loss: 0.009419224224984646\n",
      "[step: 12271] loss: 0.00941790547221899\n",
      "[step: 12272] loss: 0.009416584856808186\n",
      "[step: 12273] loss: 0.009415271691977978\n",
      "[step: 12274] loss: 0.009413952007889748\n",
      "[step: 12275] loss: 0.009412635117769241\n",
      "[step: 12276] loss: 0.009411321021616459\n",
      "[step: 12277] loss: 0.009410004131495953\n",
      "[step: 12278] loss: 0.009408689104020596\n",
      "[step: 12279] loss: 0.009407378733158112\n",
      "[step: 12280] loss: 0.009406064637005329\n",
      "[step: 12281] loss: 0.009404751472175121\n",
      "[step: 12282] loss: 0.009403438307344913\n",
      "[step: 12283] loss: 0.009402131661772728\n",
      "[step: 12284] loss: 0.009400821290910244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 12285] loss: 0.009399509988725185\n",
      "[step: 12286] loss: 0.00939820148050785\n",
      "[step: 12287] loss: 0.009396897628903389\n",
      "[step: 12288] loss: 0.009395587258040905\n",
      "[step: 12289] loss: 0.009394279681146145\n",
      "[step: 12290] loss: 0.00939297303557396\n",
      "[step: 12291] loss: 0.009391668252646923\n",
      "[step: 12292] loss: 0.009390367195010185\n",
      "[step: 12293] loss: 0.009389065206050873\n",
      "[step: 12294] loss: 0.00938776321709156\n",
      "[step: 12295] loss: 0.009386458434164524\n",
      "[step: 12296] loss: 0.009385157376527786\n",
      "[step: 12297] loss: 0.009383853524923325\n",
      "[step: 12298] loss: 0.009382558986544609\n",
      "[step: 12299] loss: 0.00938126165419817\n",
      "[step: 12300] loss: 0.009379963390529156\n",
      "[step: 12301] loss: 0.009378663264214993\n",
      "[step: 12302] loss: 0.009377371519804\n",
      "[step: 12303] loss: 0.00937607791274786\n",
      "[step: 12304] loss: 0.00937478058040142\n",
      "[step: 12305] loss: 0.009373487904667854\n",
      "[step: 12306] loss: 0.009372193366289139\n",
      "[step: 12307] loss: 0.009370907209813595\n",
      "[step: 12308] loss: 0.009369611740112305\n",
      "[step: 12309] loss: 0.009368322789669037\n",
      "[step: 12310] loss: 0.009367033839225769\n",
      "[step: 12311] loss: 0.009365743026137352\n",
      "[step: 12312] loss: 0.009364458732306957\n",
      "[step: 12313] loss: 0.009363171644508839\n",
      "[step: 12314] loss: 0.009361887350678444\n",
      "[step: 12315] loss: 0.009360600262880325\n",
      "[step: 12316] loss: 0.009359316900372505\n",
      "[step: 12317] loss: 0.00935803260654211\n",
      "[step: 12318] loss: 0.009356752038002014\n",
      "[step: 12319] loss: 0.009355468675494194\n",
      "[step: 12320] loss: 0.009354187175631523\n",
      "[step: 12321] loss: 0.00935290940105915\n",
      "[step: 12322] loss: 0.009351632557809353\n",
      "[step: 12323] loss: 0.009350351989269257\n",
      "[step: 12324] loss: 0.009349079802632332\n",
      "[step: 12325] loss: 0.009347805753350258\n",
      "[step: 12326] loss: 0.00934653077274561\n",
      "[step: 12327] loss: 0.009345251135528088\n",
      "[step: 12328] loss: 0.009343979880213737\n",
      "[step: 12329] loss: 0.009342707693576813\n",
      "[step: 12330] loss: 0.009341435506939888\n",
      "[step: 12331] loss: 0.009340168908238411\n",
      "[step: 12332] loss: 0.00933889951556921\n",
      "[step: 12333] loss: 0.009337629191577435\n",
      "[step: 12334] loss: 0.009336361661553383\n",
      "[step: 12335] loss: 0.009335098788142204\n",
      "[step: 12336] loss: 0.009333830326795578\n",
      "[step: 12337] loss: 0.00933256559073925\n",
      "[step: 12338] loss: 0.00933130644261837\n",
      "[step: 12339] loss: 0.009330042637884617\n",
      "[step: 12340] loss: 0.009328780695796013\n",
      "[step: 12341] loss: 0.009327521547675133\n",
      "[step: 12342] loss: 0.009326263330876827\n",
      "[step: 12343] loss: 0.009325001388788223\n",
      "[step: 12344] loss: 0.009323746897280216\n",
      "[step: 12345] loss: 0.00932248868048191\n",
      "[step: 12346] loss: 0.009321234188973904\n",
      "[step: 12347] loss: 0.009319976903498173\n",
      "[step: 12348] loss: 0.009318726137280464\n",
      "[step: 12349] loss: 0.009317473508417606\n",
      "[step: 12350] loss: 0.009316223673522472\n",
      "[step: 12351] loss: 0.009314971044659615\n",
      "[step: 12352] loss: 0.009313722141087055\n",
      "[step: 12353] loss: 0.009312475100159645\n",
      "[step: 12354] loss: 0.009311221539974213\n",
      "[step: 12355] loss: 0.0093099819496274\n",
      "[step: 12356] loss: 0.00930873304605484\n",
      "[step: 12357] loss: 0.009307491593062878\n",
      "[step: 12358] loss: 0.009306252002716064\n",
      "[step: 12359] loss: 0.009305005893111229\n",
      "[step: 12360] loss: 0.00930376723408699\n",
      "[step: 12361] loss: 0.009302524849772453\n",
      "[step: 12362] loss: 0.009301284328103065\n",
      "[step: 12363] loss: 0.00930005218833685\n",
      "[step: 12364] loss: 0.009298812597990036\n",
      "[step: 12365] loss: 0.009297576732933521\n",
      "[step: 12366] loss: 0.009296339936554432\n",
      "[step: 12367] loss: 0.009295109659433365\n",
      "[step: 12368] loss: 0.009293875657022\n",
      "[step: 12369] loss: 0.009292642585933208\n",
      "[step: 12370] loss: 0.00929141603410244\n",
      "[step: 12371] loss: 0.009290186688303947\n",
      "[step: 12372] loss: 0.009288957342505455\n",
      "[step: 12373] loss: 0.009287728928029537\n",
      "[step: 12374] loss: 0.009286503307521343\n",
      "[step: 12375] loss: 0.009285279549658298\n",
      "[step: 12376] loss: 0.009284055791795254\n",
      "[step: 12377] loss: 0.009282831102609634\n",
      "[step: 12378] loss: 0.009281613864004612\n",
      "[step: 12379] loss: 0.009280389174818993\n",
      "[step: 12380] loss: 0.009279167279601097\n",
      "[step: 12381] loss: 0.009277952834963799\n",
      "[step: 12382] loss: 0.009276732802391052\n",
      "[step: 12383] loss: 0.009275516495108604\n",
      "[step: 12384] loss: 0.009274298325181007\n",
      "[step: 12385] loss: 0.009273089468479156\n",
      "[step: 12386] loss: 0.009271876886487007\n",
      "[step: 12387] loss: 0.009270662441849709\n",
      "[step: 12388] loss: 0.009269450791180134\n",
      "[step: 12389] loss: 0.009268240071833134\n",
      "[step: 12390] loss: 0.009267032146453857\n",
      "[step: 12391] loss: 0.009265827015042305\n",
      "[step: 12392] loss: 0.009264620020985603\n",
      "[step: 12393] loss: 0.009263412095606327\n",
      "[step: 12394] loss: 0.009262210689485073\n",
      "[step: 12395] loss: 0.00926100742071867\n",
      "[step: 12396] loss: 0.009259802289307117\n",
      "[step: 12397] loss: 0.009258599951863289\n",
      "[step: 12398] loss: 0.009257402271032333\n",
      "[step: 12399] loss: 0.009256205521523952\n",
      "[step: 12400] loss: 0.009255005046725273\n",
      "[step: 12401] loss: 0.009253808297216892\n",
      "[step: 12402] loss: 0.009252612479031086\n",
      "[step: 12403] loss: 0.009251417592167854\n",
      "[step: 12404] loss: 0.009250226430594921\n",
      "[step: 12405] loss: 0.009249028749763966\n",
      "[step: 12406] loss: 0.00924784317612648\n",
      "[step: 12407] loss: 0.009246650151908398\n",
      "[step: 12408] loss: 0.009245460852980614\n",
      "[step: 12409] loss: 0.009244275279343128\n",
      "[step: 12410] loss: 0.009243085980415344\n",
      "[step: 12411] loss: 0.009241905063390732\n",
      "[step: 12412] loss: 0.009240720421075821\n",
      "[step: 12413] loss: 0.009239536710083485\n",
      "[step: 12414] loss: 0.009238352999091148\n",
      "[step: 12415] loss: 0.009237172082066536\n",
      "[step: 12416] loss: 0.009235994890332222\n",
      "[step: 12417] loss: 0.00923481211066246\n",
      "[step: 12418] loss: 0.009233632124960423\n",
      "[step: 12419] loss: 0.009232454933226109\n",
      "[step: 12420] loss: 0.009231277741491795\n",
      "[step: 12421] loss: 0.00923010241240263\n",
      "[step: 12422] loss: 0.009228929877281189\n",
      "[step: 12423] loss: 0.009227759204804897\n",
      "[step: 12424] loss: 0.009226586669683456\n",
      "[step: 12425] loss: 0.009225414134562016\n",
      "[step: 12426] loss: 0.009224246256053448\n",
      "[step: 12427] loss: 0.009223081171512604\n",
      "[step: 12428] loss: 0.009221911430358887\n",
      "[step: 12429] loss: 0.009220749139785767\n",
      "[step: 12430] loss: 0.009219580329954624\n",
      "[step: 12431] loss: 0.009218416176736355\n",
      "[step: 12432] loss: 0.009217255748808384\n",
      "[step: 12433] loss: 0.00921608880162239\n",
      "[step: 12434] loss: 0.009214933961629868\n",
      "[step: 12435] loss: 0.009213771671056747\n",
      "[step: 12436] loss: 0.009212611243128777\n",
      "[step: 12437] loss: 0.009211456403136253\n",
      "[step: 12438] loss: 0.00921030156314373\n",
      "[step: 12439] loss: 0.009209142066538334\n",
      "[step: 12440] loss: 0.00920799095183611\n",
      "[step: 12441] loss: 0.009206835180521011\n",
      "[step: 12442] loss: 0.009205684997141361\n",
      "[step: 12443] loss: 0.009204530157148838\n",
      "[step: 12444] loss: 0.009203379973769188\n",
      "[step: 12445] loss: 0.009202233515679836\n",
      "[step: 12446] loss: 0.00920108612626791\n",
      "[step: 12447] loss: 0.009199941530823708\n",
      "[step: 12448] loss: 0.009198790416121483\n",
      "[step: 12449] loss: 0.009197648614645004\n",
      "[step: 12450] loss: 0.009196504950523376\n",
      "[step: 12451] loss: 0.009195361286401749\n",
      "[step: 12452] loss: 0.00919421762228012\n",
      "[step: 12453] loss: 0.009193078614771366\n",
      "[step: 12454] loss: 0.009191939607262611\n",
      "[step: 12455] loss: 0.009190800599753857\n",
      "[step: 12456] loss: 0.009189662523567677\n",
      "[step: 12457] loss: 0.009188523516058922\n",
      "[step: 12458] loss: 0.009187385439872742\n",
      "[step: 12459] loss: 0.009186257608234882\n",
      "[step: 12460] loss: 0.009185123257339\n",
      "[step: 12461] loss: 0.009183989837765694\n",
      "[step: 12462] loss: 0.00918285921216011\n",
      "[step: 12463] loss: 0.009181723929941654\n",
      "[step: 12464] loss: 0.009180596098303795\n",
      "[step: 12465] loss: 0.009179470129311085\n",
      "[step: 12466] loss: 0.009178339503705502\n",
      "[step: 12467] loss: 0.009177212603390217\n",
      "[step: 12468] loss: 0.009176086634397507\n",
      "[step: 12469] loss: 0.00917496345937252\n",
      "[step: 12470] loss: 0.00917383935302496\n",
      "[step: 12471] loss: 0.009172718971967697\n",
      "[step: 12472] loss: 0.009171595796942711\n",
      "[step: 12473] loss: 0.009170472621917725\n",
      "[step: 12474] loss: 0.009169354103505611\n",
      "[step: 12475] loss: 0.009168235585093498\n",
      "[step: 12476] loss: 0.009167117066681385\n",
      "[step: 12477] loss: 0.009166000410914421\n",
      "[step: 12478] loss: 0.009164884686470032\n",
      "[step: 12479] loss: 0.009163768962025642\n",
      "[step: 12480] loss: 0.009162654168903828\n",
      "[step: 12481] loss: 0.009161543101072311\n",
      "[step: 12482] loss: 0.009160430170595646\n",
      "[step: 12483] loss: 0.009159320965409279\n",
      "[step: 12484] loss: 0.009158210828900337\n",
      "[step: 12485] loss: 0.009157099761068821\n",
      "[step: 12486] loss: 0.009155990555882454\n",
      "[step: 12487] loss: 0.009154886938631535\n",
      "[step: 12488] loss: 0.009153777733445168\n",
      "[step: 12489] loss: 0.00915267039090395\n",
      "[step: 12490] loss: 0.00915156863629818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 12491] loss: 0.009150463156402111\n",
      "[step: 12492] loss: 0.009149358607828617\n",
      "[step: 12493] loss: 0.00914825964719057\n",
      "[step: 12494] loss: 0.009147155098617077\n",
      "[step: 12495] loss: 0.009146055206656456\n",
      "[step: 12496] loss: 0.009144955314695835\n",
      "[step: 12497] loss: 0.009143856354057789\n",
      "[step: 12498] loss: 0.009142758324742317\n",
      "[step: 12499] loss: 0.009141664020717144\n",
      "[step: 12500] loss: 0.009140567854046822\n",
      "[step: 12501] loss: 0.009139472618699074\n",
      "[step: 12502] loss: 0.00913838017731905\n",
      "[step: 12503] loss: 0.009137285873293877\n",
      "[step: 12504] loss: 0.009136192500591278\n",
      "[step: 12505] loss: 0.009135101921856403\n",
      "[step: 12506] loss: 0.009134012274444103\n",
      "[step: 12507] loss: 0.009132925420999527\n",
      "[step: 12508] loss: 0.009131835773587227\n",
      "[step: 12509] loss: 0.009130746126174927\n",
      "[step: 12510] loss: 0.0091296611353755\n",
      "[step: 12511] loss: 0.009128572419285774\n",
      "[step: 12512] loss: 0.009127483703196049\n",
      "[step: 12513] loss: 0.009126406162977219\n",
      "[step: 12514] loss: 0.009125317446887493\n",
      "[step: 12515] loss: 0.00912423525005579\n",
      "[step: 12516] loss: 0.009123150259256363\n",
      "[step: 12517] loss: 0.009122068993747234\n",
      "[step: 12518] loss: 0.009120989590883255\n",
      "[step: 12519] loss: 0.00911991111934185\n",
      "[step: 12520] loss: 0.009118830785155296\n",
      "[step: 12521] loss: 0.009117753244936466\n",
      "[step: 12522] loss: 0.009116675704717636\n",
      "[step: 12523] loss: 0.00911560095846653\n",
      "[step: 12524] loss: 0.0091145234182477\n",
      "[step: 12525] loss: 0.009113449603319168\n",
      "[step: 12526] loss: 0.009112379513680935\n",
      "[step: 12527] loss: 0.009111298248171806\n",
      "[step: 12528] loss: 0.009110229089856148\n",
      "[step: 12529] loss: 0.00910915993154049\n",
      "[step: 12530] loss: 0.009108087979257107\n",
      "[step: 12531] loss: 0.009107013233006\n",
      "[step: 12532] loss: 0.009105946868658066\n",
      "[step: 12533] loss: 0.009104876779019833\n",
      "[step: 12534] loss: 0.009103808552026749\n",
      "[step: 12535] loss: 0.00910273753106594\n",
      "[step: 12536] loss: 0.00910167396068573\n",
      "[step: 12537] loss: 0.009100609458982944\n",
      "[step: 12538] loss: 0.00909954309463501\n",
      "[step: 12539] loss: 0.009098478592932224\n",
      "[step: 12540] loss: 0.009097415022552013\n",
      "[step: 12541] loss: 0.009096354246139526\n",
      "[step: 12542] loss: 0.009095292538404465\n",
      "[step: 12543] loss: 0.009094230830669403\n",
      "[step: 12544] loss: 0.00909317098557949\n",
      "[step: 12545] loss: 0.009092109277844429\n",
      "[step: 12546] loss: 0.009091053158044815\n",
      "[step: 12547] loss: 0.009089992381632328\n",
      "[step: 12548] loss: 0.00908893533051014\n",
      "[step: 12549] loss: 0.009087875485420227\n",
      "[step: 12550] loss: 0.009086819365620613\n",
      "[step: 12551] loss: 0.009085766039788723\n",
      "[step: 12552] loss: 0.009084708988666534\n",
      "[step: 12553] loss: 0.009083653800189495\n",
      "[step: 12554] loss: 0.009082604199647903\n",
      "[step: 12555] loss: 0.009081550873816013\n",
      "[step: 12556] loss: 0.009080500341951847\n",
      "[step: 12557] loss: 0.009079447947442532\n",
      "[step: 12558] loss: 0.009078395552933216\n",
      "[step: 12559] loss: 0.009077342227101326\n",
      "[step: 12560] loss: 0.009076293557882309\n",
      "[step: 12561] loss: 0.009075245819985867\n",
      "[step: 12562] loss: 0.009074200876057148\n",
      "[step: 12563] loss: 0.009073149412870407\n",
      "[step: 12564] loss: 0.009072106331586838\n",
      "[step: 12565] loss: 0.009071054868400097\n",
      "[step: 12566] loss: 0.009070013649761677\n",
      "[step: 12567] loss: 0.00906896498054266\n",
      "[step: 12568] loss: 0.00906792189925909\n",
      "[step: 12569] loss: 0.009066879749298096\n",
      "[step: 12570] loss: 0.009065836668014526\n",
      "[step: 12571] loss: 0.009064793586730957\n",
      "[step: 12572] loss: 0.009063754230737686\n",
      "[step: 12573] loss: 0.009062710218131542\n",
      "[step: 12574] loss: 0.009061673656105995\n",
      "[step: 12575] loss: 0.009060630574822426\n",
      "[step: 12576] loss: 0.009059591218829155\n",
      "[step: 12577] loss: 0.009058556519448757\n",
      "[step: 12578] loss: 0.009057516232132912\n",
      "[step: 12579] loss: 0.009056477807462215\n",
      "[step: 12580] loss: 0.009055440314114094\n",
      "[step: 12581] loss: 0.009054403752088547\n",
      "[step: 12582] loss: 0.009053369984030724\n",
      "[step: 12583] loss: 0.0090523362159729\n",
      "[step: 12584] loss: 0.009051299653947353\n",
      "[step: 12585] loss: 0.00905026588588953\n",
      "[step: 12586] loss: 0.009049235843122005\n",
      "[step: 12587] loss: 0.009048202075064182\n",
      "[step: 12588] loss: 0.009047170169651508\n",
      "[step: 12589] loss: 0.009046139195561409\n",
      "[step: 12590] loss: 0.009045109152793884\n",
      "[step: 12591] loss: 0.009044080041348934\n",
      "[step: 12592] loss: 0.009043049067258835\n",
      "[step: 12593] loss: 0.00904202088713646\n",
      "[step: 12594] loss: 0.00904099177569151\n",
      "[step: 12595] loss: 0.009039961732923985\n",
      "[step: 12596] loss: 0.009038934484124184\n",
      "[step: 12597] loss: 0.009037910029292107\n",
      "[step: 12598] loss: 0.009036881849169731\n",
      "[step: 12599] loss: 0.009035857394337654\n",
      "[step: 12600] loss: 0.009034831076860428\n",
      "[step: 12601] loss: 0.0090338084846735\n",
      "[step: 12602] loss: 0.0090327812358737\n",
      "[step: 12603] loss: 0.009031761437654495\n",
      "[step: 12604] loss: 0.009030738845467567\n",
      "[step: 12605] loss: 0.009029719047248363\n",
      "[step: 12606] loss: 0.009028696455061436\n",
      "[step: 12607] loss: 0.009027675725519657\n",
      "[step: 12608] loss: 0.009026649408042431\n",
      "[step: 12609] loss: 0.009025635197758675\n",
      "[step: 12610] loss: 0.009024613536894321\n",
      "[step: 12611] loss: 0.009023593738675117\n",
      "[step: 12612] loss: 0.009022574871778488\n",
      "[step: 12613] loss: 0.009021560661494732\n",
      "[step: 12614] loss: 0.009020540863275528\n",
      "[step: 12615] loss: 0.009019524790346622\n",
      "[step: 12616] loss: 0.009018508717417717\n",
      "[step: 12617] loss: 0.009017491713166237\n",
      "[step: 12618] loss: 0.009016478434205055\n",
      "[step: 12619] loss: 0.009015461429953575\n",
      "[step: 12620] loss: 0.009014450013637543\n",
      "[step: 12621] loss: 0.009013435803353786\n",
      "[step: 12622] loss: 0.00901242159307003\n",
      "[step: 12623] loss: 0.009011405520141125\n",
      "[step: 12624] loss: 0.00901039782911539\n",
      "[step: 12625] loss: 0.009009388275444508\n",
      "[step: 12626] loss: 0.00900837779045105\n",
      "[step: 12627] loss: 0.009007368236780167\n",
      "[step: 12628] loss: 0.00900635588914156\n",
      "[step: 12629] loss: 0.009005345404148102\n",
      "[step: 12630] loss: 0.009004335850477219\n",
      "[step: 12631] loss: 0.009003326296806335\n",
      "[step: 12632] loss: 0.009002321399748325\n",
      "[step: 12633] loss: 0.009001314640045166\n",
      "[step: 12634] loss: 0.009000306017696857\n",
      "[step: 12635] loss: 0.008999299257993698\n",
      "[step: 12636] loss: 0.008998292498290539\n",
      "[step: 12637] loss: 0.008997290395200253\n",
      "[step: 12638] loss: 0.008996284566819668\n",
      "[step: 12639] loss: 0.008995279669761658\n",
      "[step: 12640] loss: 0.008994274772703648\n",
      "[step: 12641] loss: 0.008993270806968212\n",
      "[step: 12642] loss: 0.008992267772555351\n",
      "[step: 12643] loss: 0.008991268463432789\n",
      "[step: 12644] loss: 0.008990263566374779\n",
      "[step: 12645] loss: 0.008989262394607067\n",
      "[step: 12646] loss: 0.008988259360194206\n",
      "[step: 12647] loss: 0.008987260051071644\n",
      "[step: 12648] loss: 0.008986261673271656\n",
      "[step: 12649] loss: 0.008985260501503944\n",
      "[step: 12650] loss: 0.008984259329736233\n",
      "[step: 12651] loss: 0.008983262814581394\n",
      "[step: 12652] loss: 0.008982270024716854\n",
      "[step: 12653] loss: 0.008981271646916866\n",
      "[step: 12654] loss: 0.008980273269116879\n",
      "[step: 12655] loss: 0.008979273959994316\n",
      "[step: 12656] loss: 0.008978276513516903\n",
      "[step: 12657] loss: 0.008977280929684639\n",
      "[step: 12658] loss: 0.0089762844145298\n",
      "[step: 12659] loss: 0.00897529348731041\n",
      "[step: 12660] loss: 0.008974297903478146\n",
      "[step: 12661] loss: 0.008973304182291031\n",
      "[step: 12662] loss: 0.008972309529781342\n",
      "[step: 12663] loss: 0.008971315808594227\n",
      "[step: 12664] loss: 0.008970326744019985\n",
      "[step: 12665] loss: 0.008969332091510296\n",
      "[step: 12666] loss: 0.008968341164290905\n",
      "[step: 12667] loss: 0.008967351168394089\n",
      "[step: 12668] loss: 0.008966358378529549\n",
      "[step: 12669] loss: 0.008965369313955307\n",
      "[step: 12670] loss: 0.008964377455413342\n",
      "[step: 12671] loss: 0.008963390253484249\n",
      "[step: 12672] loss: 0.008962400257587433\n",
      "[step: 12673] loss: 0.008961410261690617\n",
      "[step: 12674] loss: 0.008960425853729248\n",
      "[step: 12675] loss: 0.008959436789155006\n",
      "[step: 12676] loss: 0.00895844865590334\n",
      "[step: 12677] loss: 0.008957462385296822\n",
      "[step: 12678] loss: 0.008956476114690304\n",
      "[step: 12679] loss: 0.00895549077540636\n",
      "[step: 12680] loss: 0.008954505436122417\n",
      "[step: 12681] loss: 0.008953520096838474\n",
      "[step: 12682] loss: 0.00895253662019968\n",
      "[step: 12683] loss: 0.008951553143560886\n",
      "[step: 12684] loss: 0.008950568735599518\n",
      "[step: 12685] loss: 0.008949586190283298\n",
      "[step: 12686] loss: 0.00894860364496708\n",
      "[step: 12687] loss: 0.008947620168328285\n",
      "[step: 12688] loss: 0.008946635760366917\n",
      "[step: 12689] loss: 0.00894565787166357\n",
      "[step: 12690] loss: 0.0089446771889925\n",
      "[step: 12691] loss: 0.008943699300289154\n",
      "[step: 12692] loss: 0.008942718617618084\n",
      "[step: 12693] loss: 0.008941736072301865\n",
      "[step: 12694] loss: 0.008940761908888817\n",
      "[step: 12695] loss: 0.008939779363572598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 12696] loss: 0.0089388033375144\n",
      "[step: 12697] loss: 0.008937827311456203\n",
      "[step: 12698] loss: 0.008936847560107708\n",
      "[step: 12699] loss: 0.008935867808759212\n",
      "[step: 12700] loss: 0.00893489271402359\n",
      "[step: 12701] loss: 0.008933915756642818\n",
      "[step: 12702] loss: 0.00893294345587492\n",
      "[step: 12703] loss: 0.008931967429816723\n",
      "[step: 12704] loss: 0.008930990472435951\n",
      "[step: 12705] loss: 0.008930016309022903\n",
      "[step: 12706] loss: 0.008929044008255005\n",
      "[step: 12707] loss: 0.008928067982196808\n",
      "[step: 12708] loss: 0.008927097544074059\n",
      "[step: 12709] loss: 0.00892612338066101\n",
      "[step: 12710] loss: 0.008925151079893112\n",
      "[step: 12711] loss: 0.008924182504415512\n",
      "[step: 12712] loss: 0.008923210203647614\n",
      "[step: 12713] loss: 0.008922239765524864\n",
      "[step: 12714] loss: 0.008921265602111816\n",
      "[step: 12715] loss: 0.008920298889279366\n",
      "[step: 12716] loss: 0.008919326588511467\n",
      "[step: 12717] loss: 0.008918356150388718\n",
      "[step: 12718] loss: 0.008917388506233692\n",
      "[step: 12719] loss: 0.008916421793401241\n",
      "[step: 12720] loss: 0.008915453217923641\n",
      "[step: 12721] loss: 0.008914485573768616\n",
      "[step: 12722] loss: 0.008913516066968441\n",
      "[step: 12723] loss: 0.008912550285458565\n",
      "[step: 12724] loss: 0.008911585435271263\n",
      "[step: 12725] loss: 0.008910618722438812\n",
      "[step: 12726] loss: 0.008909651078283787\n",
      "[step: 12727] loss: 0.00890868715941906\n",
      "[step: 12728] loss: 0.008907726965844631\n",
      "[step: 12729] loss: 0.008906763046979904\n",
      "[step: 12730] loss: 0.008905798196792603\n",
      "[step: 12731] loss: 0.008904834277927876\n",
      "[step: 12732] loss: 0.008903873153030872\n",
      "[step: 12733] loss: 0.00890290830284357\n",
      "[step: 12734] loss: 0.008901947177946568\n",
      "[step: 12735] loss: 0.00890098512172699\n",
      "[step: 12736] loss: 0.008900025859475136\n",
      "[step: 12737] loss: 0.008899061009287834\n",
      "[step: 12738] loss: 0.008898099884390831\n",
      "[step: 12739] loss: 0.008897143416106701\n",
      "[step: 12740] loss: 0.0088961785659194\n",
      "[step: 12741] loss: 0.008895223960280418\n",
      "[step: 12742] loss: 0.00889426190406084\n",
      "[step: 12743] loss: 0.008893304504454136\n",
      "[step: 12744] loss: 0.00889235083013773\n",
      "[step: 12745] loss: 0.008891389705240726\n",
      "[step: 12746] loss: 0.008890435099601746\n",
      "[step: 12747] loss: 0.008889477699995041\n",
      "[step: 12748] loss: 0.00888852309435606\n",
      "[step: 12749] loss: 0.008887561969459057\n",
      "[step: 12750] loss: 0.0088866101577878\n",
      "[step: 12751] loss: 0.008885656483471394\n",
      "[step: 12752] loss: 0.008884701877832413\n",
      "[step: 12753] loss: 0.008883744478225708\n",
      "[step: 12754] loss: 0.008882792666554451\n",
      "[step: 12755] loss: 0.008881837129592896\n",
      "[step: 12756] loss: 0.008880885317921638\n",
      "[step: 12757] loss: 0.008879934437572956\n",
      "[step: 12758] loss: 0.008878976106643677\n",
      "[step: 12759] loss: 0.008878026157617569\n",
      "[step: 12760] loss: 0.008877075277268887\n",
      "[step: 12761] loss: 0.008876124396920204\n",
      "[step: 12762] loss: 0.008875174447894096\n",
      "[step: 12763] loss: 0.008874228224158287\n",
      "[step: 12764] loss: 0.00887327641248703\n",
      "[step: 12765] loss: 0.008872325532138348\n",
      "[step: 12766] loss: 0.008871376514434814\n",
      "[step: 12767] loss: 0.00887043122202158\n",
      "[step: 12768] loss: 0.008869477547705173\n",
      "[step: 12769] loss: 0.008868533186614513\n",
      "[step: 12770] loss: 0.008867583237588406\n",
      "[step: 12771] loss: 0.008866636082530022\n",
      "[step: 12772] loss: 0.008865687064826488\n",
      "[step: 12773] loss: 0.008864744566380978\n",
      "[step: 12774] loss: 0.008863797411322594\n",
      "[step: 12775] loss: 0.008862853050231934\n",
      "[step: 12776] loss: 0.008861906826496124\n",
      "[step: 12777] loss: 0.00886095967143774\n",
      "[step: 12778] loss: 0.008860020898282528\n",
      "[step: 12779] loss: 0.008859074674546719\n",
      "[step: 12780] loss: 0.008858133107423782\n",
      "[step: 12781] loss: 0.008857189677655697\n",
      "[step: 12782] loss: 0.008856246247887611\n",
      "[step: 12783] loss: 0.0088553037494421\n",
      "[step: 12784] loss: 0.00885436125099659\n",
      "[step: 12785] loss: 0.008853419683873653\n",
      "[step: 12786] loss: 0.008852478116750717\n",
      "[step: 12787] loss: 0.008851534686982632\n",
      "[step: 12788] loss: 0.008850598707795143\n",
      "[step: 12789] loss: 0.008849658071994781\n",
      "[step: 12790] loss: 0.00884871743619442\n",
      "[step: 12791] loss: 0.008847781457006931\n",
      "[step: 12792] loss: 0.008846843615174294\n",
      "[step: 12793] loss: 0.008845903910696507\n",
      "[step: 12794] loss: 0.008844963274896145\n",
      "[step: 12795] loss: 0.008844024501740932\n",
      "[step: 12796] loss: 0.008843089453876019\n",
      "[step: 12797] loss: 0.00884215347468853\n",
      "[step: 12798] loss: 0.008841217495501041\n",
      "[step: 12799] loss: 0.00884027685970068\n",
      "[step: 12800] loss: 0.008839347399771214\n",
      "[step: 12801] loss: 0.008838406763970852\n",
      "[step: 12802] loss: 0.008837471716105938\n",
      "[step: 12803] loss: 0.00883653573691845\n",
      "[step: 12804] loss: 0.00883560162037611\n",
      "[step: 12805] loss: 0.008834665641188622\n",
      "[step: 12806] loss: 0.00883373524993658\n",
      "[step: 12807] loss: 0.008832802064716816\n",
      "[step: 12808] loss: 0.008831869810819626\n",
      "[step: 12809] loss: 0.008830937556922436\n",
      "[step: 12810] loss: 0.008830005303025246\n",
      "[step: 12811] loss: 0.008829071186482906\n",
      "[step: 12812] loss: 0.008828144520521164\n",
      "[step: 12813] loss: 0.008827212266623974\n",
      "[step: 12814] loss: 0.008826281875371933\n",
      "[step: 12815] loss: 0.008825352415442467\n",
      "[step: 12816] loss: 0.008824421092867851\n",
      "[step: 12817] loss: 0.008823493495583534\n",
      "[step: 12818] loss: 0.00882256031036377\n",
      "[step: 12819] loss: 0.008821633644402027\n",
      "[step: 12820] loss: 0.008820705115795135\n",
      "[step: 12821] loss: 0.008819778449833393\n",
      "[step: 12822] loss: 0.008818854577839375\n",
      "[step: 12823] loss: 0.008817922323942184\n",
      "[step: 12824] loss: 0.008816995657980442\n",
      "[step: 12825] loss: 0.008816069923341274\n",
      "[step: 12826] loss: 0.00881514698266983\n",
      "[step: 12827] loss: 0.008814217522740364\n",
      "[step: 12828] loss: 0.008813292719423771\n",
      "[step: 12829] loss: 0.008812366053462029\n",
      "[step: 12830] loss: 0.008811441250145435\n",
      "[step: 12831] loss: 0.00881052203476429\n",
      "[step: 12832] loss: 0.008809594437479973\n",
      "[step: 12833] loss: 0.00880866963416338\n",
      "[step: 12834] loss: 0.00880774762481451\n",
      "[step: 12835] loss: 0.008806823752820492\n",
      "[step: 12836] loss: 0.008805900812149048\n",
      "[step: 12837] loss: 0.008804982528090477\n",
      "[step: 12838] loss: 0.00880405679345131\n",
      "[step: 12839] loss: 0.008803138509392738\n",
      "[step: 12840] loss: 0.008802216500043869\n",
      "[step: 12841] loss: 0.008801297284662724\n",
      "[step: 12842] loss: 0.008800377137959003\n",
      "[step: 12843] loss: 0.00879945419728756\n",
      "[step: 12844] loss: 0.00879853405058384\n",
      "[step: 12845] loss: 0.00879761390388012\n",
      "[step: 12846] loss: 0.008796697482466698\n",
      "[step: 12847] loss: 0.008795778267085552\n",
      "[step: 12848] loss: 0.008794857189059258\n",
      "[step: 12849] loss: 0.00879394356161356\n",
      "[step: 12850] loss: 0.00879302341490984\n",
      "[step: 12851] loss: 0.008792105130851269\n",
      "[step: 12852] loss: 0.008791184984147549\n",
      "[step: 12853] loss: 0.008790271356701851\n",
      "[step: 12854] loss: 0.008789356797933578\n",
      "[step: 12855] loss: 0.008788441307842731\n",
      "[step: 12856] loss: 0.008787526749074459\n",
      "[step: 12857] loss: 0.008786610327661037\n",
      "[step: 12858] loss: 0.008785692043602467\n",
      "[step: 12859] loss: 0.008784781210124493\n",
      "[step: 12860] loss: 0.008783865720033646\n",
      "[step: 12861] loss: 0.008782951161265373\n",
      "[step: 12862] loss: 0.0087820366024971\n",
      "[step: 12863] loss: 0.008781124837696552\n",
      "[step: 12864] loss: 0.008780206553637981\n",
      "[step: 12865] loss: 0.008779298514127731\n",
      "[step: 12866] loss: 0.008778386749327183\n",
      "[step: 12867] loss: 0.008777473121881485\n",
      "[step: 12868] loss: 0.008776558563113213\n",
      "[step: 12869] loss: 0.008775649592280388\n",
      "[step: 12870] loss: 0.008774735033512115\n",
      "[step: 12871] loss: 0.008773825131356716\n",
      "[step: 12872] loss: 0.008772918954491615\n",
      "[step: 12873] loss: 0.008772004395723343\n",
      "[step: 12874] loss: 0.008771098218858242\n",
      "[step: 12875] loss: 0.008770186454057693\n",
      "[step: 12876] loss: 0.008769278414547443\n",
      "[step: 12877] loss: 0.00876836758106947\n",
      "[step: 12878] loss: 0.008767456747591496\n",
      "[step: 12879] loss: 0.00876654777675867\n",
      "[step: 12880] loss: 0.008765642531216145\n",
      "[step: 12881] loss: 0.008764732629060745\n",
      "[step: 12882] loss: 0.008763822726905346\n",
      "[step: 12883] loss: 0.008762916550040245\n",
      "[step: 12884] loss: 0.008762011304497719\n",
      "[step: 12885] loss: 0.008761105127632618\n",
      "[step: 12886] loss: 0.008760196156799793\n",
      "[step: 12887] loss: 0.008759292773902416\n",
      "[step: 12888] loss: 0.008758386597037315\n",
      "[step: 12889] loss: 0.008757481351494789\n",
      "[step: 12890] loss: 0.008756575174629688\n",
      "[step: 12891] loss: 0.00875567365437746\n",
      "[step: 12892] loss: 0.00875476747751236\n",
      "[step: 12893] loss: 0.008753863163292408\n",
      "[step: 12894] loss: 0.008752955123782158\n",
      "[step: 12895] loss: 0.008752052672207355\n",
      "[step: 12896] loss: 0.008751149289309978\n",
      "[step: 12897] loss: 0.00875024776905775\n",
      "[step: 12898] loss: 0.008749343454837799\n",
      "[step: 12899] loss: 0.008748441003262997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 12900] loss: 0.008747536689043045\n",
      "[step: 12901] loss: 0.008746632374823093\n",
      "[step: 12902] loss: 0.008745734579861164\n",
      "[step: 12903] loss: 0.008744831196963787\n",
      "[step: 12904] loss: 0.008743933402001858\n",
      "[step: 12905] loss: 0.008743030950427055\n",
      "[step: 12906] loss: 0.008742130361497402\n",
      "[step: 12907] loss: 0.0087412279099226\n",
      "[step: 12908] loss: 0.008740326389670372\n",
      "[step: 12909] loss: 0.008739425800740719\n",
      "[step: 12910] loss: 0.008738529868423939\n",
      "[step: 12911] loss: 0.008737629279494286\n",
      "[step: 12912] loss: 0.008736729621887207\n",
      "[step: 12913] loss: 0.008735829032957554\n",
      "[step: 12914] loss: 0.008734934031963348\n",
      "[step: 12915] loss: 0.008734027855098248\n",
      "[step: 12916] loss: 0.008733133785426617\n",
      "[step: 12917] loss: 0.008732234127819538\n",
      "[step: 12918] loss: 0.008731335401535034\n",
      "[step: 12919] loss: 0.008730435743927956\n",
      "[step: 12920] loss: 0.0087295426055789\n",
      "[step: 12921] loss: 0.008728643879294395\n",
      "[step: 12922] loss: 0.00872774887830019\n",
      "[step: 12923] loss: 0.008726854808628559\n",
      "[step: 12924] loss: 0.008725952357053757\n",
      "[step: 12925] loss: 0.008725056424736977\n",
      "[step: 12926] loss: 0.008724161423742771\n",
      "[step: 12927] loss: 0.00872326921671629\n",
      "[step: 12928] loss: 0.008722377009689808\n",
      "[step: 12929] loss: 0.008721478283405304\n",
      "[step: 12930] loss: 0.008720584213733673\n",
      "[step: 12931] loss: 0.008719686418771744\n",
      "[step: 12932] loss: 0.008718794211745262\n",
      "[step: 12933] loss: 0.008717899210751057\n",
      "[step: 12934] loss: 0.008717004209756851\n",
      "[step: 12935] loss: 0.008716107346117496\n",
      "[step: 12936] loss: 0.008715215139091015\n",
      "[step: 12937] loss: 0.008714324794709682\n",
      "[step: 12938] loss: 0.008713431656360626\n",
      "[step: 12939] loss: 0.00871253665536642\n",
      "[step: 12940] loss: 0.008711644448339939\n",
      "[step: 12941] loss: 0.008710754103958607\n",
      "[step: 12942] loss: 0.00870986096560955\n",
      "[step: 12943] loss: 0.008708964101970196\n",
      "[step: 12944] loss: 0.008708073757588863\n",
      "[step: 12945] loss: 0.008707182481884956\n",
      "[step: 12946] loss: 0.008706286549568176\n",
      "[step: 12947] loss: 0.008705400861799717\n",
      "[step: 12948] loss: 0.00870450772345066\n",
      "[step: 12949] loss: 0.008703616447746754\n",
      "[step: 12950] loss: 0.008702727034687996\n",
      "[step: 12951] loss: 0.008701837621629238\n",
      "[step: 12952] loss: 0.008700945414602757\n",
      "[step: 12953] loss: 0.008700056001543999\n",
      "[step: 12954] loss: 0.008699163794517517\n",
      "[step: 12955] loss: 0.00869827438145876\n",
      "[step: 12956] loss: 0.008697385899722576\n",
      "[step: 12957] loss: 0.008696491830050945\n",
      "[step: 12958] loss: 0.008695598691701889\n",
      "[step: 12959] loss: 0.008694716729223728\n",
      "[step: 12960] loss: 0.008693826384842396\n",
      "[step: 12961] loss: 0.008692934177815914\n",
      "[step: 12962] loss: 0.00869204942137003\n",
      "[step: 12963] loss: 0.008691160008311272\n",
      "[step: 12964] loss: 0.008690268732607365\n",
      "[step: 12965] loss: 0.008689381182193756\n",
      "[step: 12966] loss: 0.008688495494425297\n",
      "[step: 12967] loss: 0.008687606081366539\n",
      "[step: 12968] loss: 0.00868671853095293\n",
      "[step: 12969] loss: 0.008685830980539322\n",
      "[step: 12970] loss: 0.008684939704835415\n",
      "[step: 12971] loss: 0.008684049360454082\n",
      "[step: 12972] loss: 0.008683166466653347\n",
      "[step: 12973] loss: 0.008682282641530037\n",
      "[step: 12974] loss: 0.008681394159793854\n",
      "[step: 12975] loss: 0.00868050567805767\n",
      "[step: 12976] loss: 0.008679616264998913\n",
      "[step: 12977] loss: 0.008678731508553028\n",
      "[step: 12978] loss: 0.00867784395813942\n",
      "[step: 12979] loss: 0.00867695827037096\n",
      "[step: 12980] loss: 0.008676071651279926\n",
      "[step: 12981] loss: 0.008675187826156616\n",
      "[step: 12982] loss: 0.008674303069710732\n",
      "[step: 12983] loss: 0.008673412725329399\n",
      "[step: 12984] loss: 0.008672527968883514\n",
      "[step: 12985] loss: 0.008671638555824757\n",
      "[step: 12986] loss: 0.008670756593346596\n",
      "[step: 12987] loss: 0.008669871836900711\n",
      "[step: 12988] loss: 0.008668984286487103\n",
      "[step: 12989] loss: 0.00866809580475092\n",
      "[step: 12990] loss: 0.008667209185659885\n",
      "[step: 12991] loss: 0.008666325360536575\n",
      "[step: 12992] loss: 0.008665435947477818\n",
      "[step: 12993] loss: 0.008664555847644806\n",
      "[step: 12994] loss: 0.008663669228553772\n",
      "[step: 12995] loss: 0.008662786334753036\n",
      "[step: 12996] loss: 0.008661898784339428\n",
      "[step: 12997] loss: 0.008661012165248394\n",
      "[step: 12998] loss: 0.008660128340125084\n",
      "[step: 12999] loss: 0.008659247308969498\n",
      "[step: 13000] loss: 0.008658359758555889\n",
      "[step: 13001] loss: 0.008657478727400303\n",
      "[step: 13002] loss: 0.008656593970954418\n",
      "[step: 13003] loss: 0.008655708283185959\n",
      "[step: 13004] loss: 0.0086548225954175\n",
      "[step: 13005] loss: 0.008653937838971615\n",
      "[step: 13006] loss: 0.008653052151203156\n",
      "[step: 13007] loss: 0.008652173914015293\n",
      "[step: 13008] loss: 0.00865128543227911\n",
      "[step: 13009] loss: 0.0086504016071558\n",
      "[step: 13010] loss: 0.00864951778203249\n",
      "[step: 13011] loss: 0.008648634888231754\n",
      "[step: 13012] loss: 0.008647746406495571\n",
      "[step: 13013] loss: 0.008646863512694836\n",
      "[step: 13014] loss: 0.008645985275506973\n",
      "[step: 13015] loss: 0.008645097725093365\n",
      "[step: 13016] loss: 0.008644216693937778\n",
      "[step: 13017] loss: 0.00864332914352417\n",
      "[step: 13018] loss: 0.008642448112368584\n",
      "[step: 13019] loss: 0.008641562424600124\n",
      "[step: 13020] loss: 0.00864067766815424\n",
      "[step: 13021] loss: 0.008639798499643803\n",
      "[step: 13022] loss: 0.008638909086585045\n",
      "[step: 13023] loss: 0.008638022467494011\n",
      "[step: 13024] loss: 0.008637143298983574\n",
      "[step: 13025] loss: 0.008636257611215115\n",
      "[step: 13026] loss: 0.008635376580059528\n",
      "[step: 13027] loss: 0.008634493686258793\n",
      "[step: 13028] loss: 0.008633608929812908\n",
      "[step: 13029] loss: 0.008632725104689598\n",
      "[step: 13030] loss: 0.008631840348243713\n",
      "[step: 13031] loss: 0.008630958385765553\n",
      "[step: 13032] loss: 0.008630078285932541\n",
      "[step: 13033] loss: 0.008629193529486656\n",
      "[step: 13034] loss: 0.008628306910395622\n",
      "[step: 13035] loss: 0.00862742867320776\n",
      "[step: 13036] loss: 0.008626541122794151\n",
      "[step: 13037] loss: 0.008625658228993416\n",
      "[step: 13038] loss: 0.008624772541224957\n",
      "[step: 13039] loss: 0.008623892441391945\n",
      "[step: 13040] loss: 0.008623004890978336\n",
      "[step: 13041] loss: 0.008622122928500175\n",
      "[step: 13042] loss: 0.008621237240731716\n",
      "[step: 13043] loss: 0.00862035620957613\n",
      "[step: 13044] loss: 0.008619476109743118\n",
      "[step: 13045] loss: 0.00861858855932951\n",
      "[step: 13046] loss: 0.008617709390819073\n",
      "[step: 13047] loss: 0.00861681904643774\n",
      "[step: 13048] loss: 0.008615936152637005\n",
      "[step: 13049] loss: 0.008615062572062016\n",
      "[step: 13050] loss: 0.008614170365035534\n",
      "[step: 13051] loss: 0.008613290265202522\n",
      "[step: 13052] loss: 0.008612403646111488\n",
      "[step: 13053] loss: 0.008611522614955902\n",
      "[step: 13054] loss: 0.008610639721155167\n",
      "[step: 13055] loss: 0.008609753102064133\n",
      "[step: 13056] loss: 0.008608872070908546\n",
      "[step: 13057] loss: 0.008607985451817513\n",
      "[step: 13058] loss: 0.008607101626694202\n",
      "[step: 13059] loss: 0.00860622152686119\n",
      "[step: 13060] loss: 0.008605336770415306\n",
      "[step: 13061] loss: 0.008604449220001698\n",
      "[step: 13062] loss: 0.008603564463555813\n",
      "[step: 13063] loss: 0.008602680638432503\n",
      "[step: 13064] loss: 0.008601798675954342\n",
      "[step: 13065] loss: 0.00860091857612133\n",
      "[step: 13066] loss: 0.008600032888352871\n",
      "[step: 13067] loss: 0.008599143475294113\n",
      "[step: 13068] loss: 0.008598266169428825\n",
      "[step: 13069] loss: 0.00859738141298294\n",
      "[step: 13070] loss: 0.008596492931246758\n",
      "[step: 13071] loss: 0.008595612831413746\n",
      "[step: 13072] loss: 0.008594725281000137\n",
      "[step: 13073] loss: 0.008593843318521976\n",
      "[step: 13074] loss: 0.008592958562076092\n",
      "[step: 13075] loss: 0.008592074736952782\n",
      "[step: 13076] loss: 0.008591189980506897\n",
      "[step: 13077] loss: 0.00859030056744814\n",
      "[step: 13078] loss: 0.008589419536292553\n",
      "[step: 13079] loss: 0.008588532917201519\n",
      "[step: 13080] loss: 0.008587642572820187\n",
      "[step: 13081] loss: 0.00858676340430975\n",
      "[step: 13082] loss: 0.00858587771654129\n",
      "[step: 13083] loss: 0.00858499389141798\n",
      "[step: 13084] loss: 0.008584108203649521\n",
      "[step: 13085] loss: 0.008583225309848785\n",
      "[step: 13086] loss: 0.008582336828112602\n",
      "[step: 13087] loss: 0.008581454865634441\n",
      "[step: 13088] loss: 0.008580567315220833\n",
      "[step: 13089] loss: 0.00857967883348465\n",
      "[step: 13090] loss: 0.008578795939683914\n",
      "[step: 13091] loss: 0.008577905595302582\n",
      "[step: 13092] loss: 0.008577019907534122\n",
      "[step: 13093] loss: 0.008576132357120514\n",
      "[step: 13094] loss: 0.008575253188610077\n",
      "[step: 13095] loss: 0.008574367500841618\n",
      "[step: 13096] loss: 0.008573481813073158\n",
      "[step: 13097] loss: 0.008572593331336975\n",
      "[step: 13098] loss: 0.008571705780923367\n",
      "[step: 13099] loss: 0.008570822887122631\n",
      "[step: 13100] loss: 0.00856993068009615\n",
      "[step: 13101] loss: 0.008569052442908287\n",
      "[step: 13102] loss: 0.008568157441914082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 13103] loss: 0.00856727547943592\n",
      "[step: 13104] loss: 0.008566387929022312\n",
      "[step: 13105] loss: 0.008565498515963554\n",
      "[step: 13106] loss: 0.008564612828195095\n",
      "[step: 13107] loss: 0.008563725277781487\n",
      "[step: 13108] loss: 0.008562839590013027\n",
      "[step: 13109] loss: 0.00856194831430912\n",
      "[step: 13110] loss: 0.00856106635183096\n",
      "[step: 13111] loss: 0.008560171350836754\n",
      "[step: 13112] loss: 0.008559288457036018\n",
      "[step: 13113] loss: 0.008558398112654686\n",
      "[step: 13114] loss: 0.008557513356208801\n",
      "[step: 13115] loss: 0.008556624874472618\n",
      "[step: 13116] loss: 0.00855573732405901\n",
      "[step: 13117] loss: 0.008554846979677677\n",
      "[step: 13118] loss: 0.008553963154554367\n",
      "[step: 13119] loss: 0.008553067222237587\n",
      "[step: 13120] loss: 0.008552182465791702\n",
      "[step: 13121] loss: 0.008551294915378094\n",
      "[step: 13122] loss: 0.00855040643364191\n",
      "[step: 13123] loss: 0.008549515157938004\n",
      "[step: 13124] loss: 0.00854862853884697\n",
      "[step: 13125] loss: 0.00854774285107851\n",
      "[step: 13126] loss: 0.008546851575374603\n",
      "[step: 13127] loss: 0.008545961230993271\n",
      "[step: 13128] loss: 0.00854507740586996\n",
      "[step: 13129] loss: 0.008544180542230606\n",
      "[step: 13130] loss: 0.008543292060494423\n",
      "[step: 13131] loss: 0.008542407304048538\n",
      "[step: 13132] loss: 0.008541516959667206\n",
      "[step: 13133] loss: 0.008540624752640724\n",
      "[step: 13134] loss: 0.008539735339581966\n",
      "[step: 13135] loss: 0.008538846857845783\n",
      "[step: 13136] loss: 0.008537954650819302\n",
      "[step: 13137] loss: 0.00853706430643797\n",
      "[step: 13138] loss: 0.008536177687346935\n",
      "[step: 13139] loss: 0.008535287342965603\n",
      "[step: 13140] loss: 0.00853439886122942\n",
      "[step: 13141] loss: 0.008533506654202938\n",
      "[step: 13142] loss: 0.008532612584531307\n",
      "[step: 13143] loss: 0.0085317213088274\n",
      "[step: 13144] loss: 0.008530831895768642\n",
      "[step: 13145] loss: 0.008529942482709885\n",
      "[step: 13146] loss: 0.008529056794941425\n",
      "[step: 13147] loss: 0.008528164587914944\n",
      "[step: 13148] loss: 0.008527270518243313\n",
      "[step: 13149] loss: 0.00852638203650713\n",
      "[step: 13150] loss: 0.008525483310222626\n",
      "[step: 13151] loss: 0.008524599485099316\n",
      "[step: 13152] loss: 0.008523702621459961\n",
      "[step: 13153] loss: 0.00852280855178833\n",
      "[step: 13154] loss: 0.00852192286401987\n",
      "[step: 13155] loss: 0.008521029725670815\n",
      "[step: 13156] loss: 0.008520133793354034\n",
      "[step: 13157] loss: 0.008519245311617851\n",
      "[step: 13158] loss: 0.008518354035913944\n",
      "[step: 13159] loss: 0.008517459966242313\n",
      "[step: 13160] loss: 0.008516569621860981\n",
      "[step: 13161] loss: 0.008515672758221626\n",
      "[step: 13162] loss: 0.008514784276485443\n",
      "[step: 13163] loss: 0.008513887412846088\n",
      "[step: 13164] loss: 0.008512995205819607\n",
      "[step: 13165] loss: 0.0085121039301157\n",
      "[step: 13166] loss: 0.008511209860444069\n",
      "[step: 13167] loss: 0.008510314859449863\n",
      "[step: 13168] loss: 0.008509425446391106\n",
      "[step: 13169] loss: 0.008508527651429176\n",
      "[step: 13170] loss: 0.00850763637572527\n",
      "[step: 13171] loss: 0.008506741374731064\n",
      "[step: 13172] loss: 0.00850584451109171\n",
      "[step: 13173] loss: 0.008504950441420078\n",
      "[step: 13174] loss: 0.008504053577780724\n",
      "[step: 13175] loss: 0.008503157645463943\n",
      "[step: 13176] loss: 0.008502262644469738\n",
      "[step: 13177] loss: 0.008501368574798107\n",
      "[step: 13178] loss: 0.008500472642481327\n",
      "[step: 13179] loss: 0.008499574847519398\n",
      "[step: 13180] loss: 0.008498680777847767\n",
      "[step: 13181] loss: 0.008497781120240688\n",
      "[step: 13182] loss: 0.008496888913214207\n",
      "[step: 13183] loss: 0.008495987392961979\n",
      "[step: 13184] loss: 0.008495095185935497\n",
      "[step: 13185] loss: 0.008494195528328419\n",
      "[step: 13186] loss: 0.008493302389979362\n",
      "[step: 13187] loss: 0.008492402732372284\n",
      "[step: 13188] loss: 0.00849150214344263\n",
      "[step: 13189] loss: 0.00849060621112585\n",
      "[step: 13190] loss: 0.008489705622196198\n",
      "[step: 13191] loss: 0.008488811552524567\n",
      "[step: 13192] loss: 0.008487909100949764\n",
      "[step: 13193] loss: 0.00848701223731041\n",
      "[step: 13194] loss: 0.008486111648380756\n",
      "[step: 13195] loss: 0.008485211990773678\n",
      "[step: 13196] loss: 0.008484315127134323\n",
      "[step: 13197] loss: 0.008483413606882095\n",
      "[step: 13198] loss: 0.008482511155307293\n",
      "[step: 13199] loss: 0.00848161056637764\n",
      "[step: 13200] loss: 0.008480709977447987\n",
      "[step: 13201] loss: 0.008479809388518333\n",
      "[step: 13202] loss: 0.008478911593556404\n",
      "[step: 13203] loss: 0.00847800076007843\n",
      "[step: 13204] loss: 0.0084771029651165\n",
      "[step: 13205] loss: 0.008476202376186848\n",
      "[step: 13206] loss: 0.008475293405354023\n",
      "[step: 13207] loss: 0.00847439095377922\n",
      "[step: 13208] loss: 0.008473490364849567\n",
      "[step: 13209] loss: 0.00847258884459734\n",
      "[step: 13210] loss: 0.008471685461699963\n",
      "[step: 13211] loss: 0.008470774628221989\n",
      "[step: 13212] loss: 0.00846987683326006\n",
      "[step: 13213] loss: 0.008468967862427235\n",
      "[step: 13214] loss: 0.008468067273497581\n",
      "[step: 13215] loss: 0.008467158302664757\n",
      "[step: 13216] loss: 0.008466248400509357\n",
      "[step: 13217] loss: 0.008465347811579704\n",
      "[step: 13218] loss: 0.008464439772069454\n",
      "[step: 13219] loss: 0.008463535457849503\n",
      "[step: 13220] loss: 0.008462624624371529\n",
      "[step: 13221] loss: 0.00846171472221613\n",
      "[step: 13222] loss: 0.008460809476673603\n",
      "[step: 13223] loss: 0.008459901437163353\n",
      "[step: 13224] loss: 0.008458991535007954\n",
      "[step: 13225] loss: 0.008458085358142853\n",
      "[step: 13226] loss: 0.008457174524664879\n",
      "[step: 13227] loss: 0.00845626462250948\n",
      "[step: 13228] loss: 0.008455351926386356\n",
      "[step: 13229] loss: 0.008454437367618084\n",
      "[step: 13230] loss: 0.008453533984720707\n",
      "[step: 13231] loss: 0.008452625013887882\n",
      "[step: 13232] loss: 0.00845171045511961\n",
      "[step: 13233] loss: 0.008450798690319061\n",
      "[step: 13234] loss: 0.008449885994195938\n",
      "[step: 13235] loss: 0.008448971435427666\n",
      "[step: 13236] loss: 0.008448063395917416\n",
      "[step: 13237] loss: 0.00844714418053627\n",
      "[step: 13238] loss: 0.008446229621767998\n",
      "[step: 13239] loss: 0.008445318788290024\n",
      "[step: 13240] loss: 0.008444404229521751\n",
      "[step: 13241] loss: 0.008443485014140606\n",
      "[step: 13242] loss: 0.008442566730082035\n",
      "[step: 13243] loss: 0.008441654033958912\n",
      "[step: 13244] loss: 0.008440738543868065\n",
      "[step: 13245] loss: 0.008439816534519196\n",
      "[step: 13246] loss: 0.008438901975750923\n",
      "[step: 13247] loss: 0.008437982760369778\n",
      "[step: 13248] loss: 0.008437066338956356\n",
      "[step: 13249] loss: 0.00843614898622036\n",
      "[step: 13250] loss: 0.008435224182903767\n",
      "[step: 13251] loss: 0.008434307761490345\n",
      "[step: 13252] loss: 0.008433391340076923\n",
      "[step: 13253] loss: 0.008432464674115181\n",
      "[step: 13254] loss: 0.008431550115346909\n",
      "[step: 13255] loss: 0.008430623449385166\n",
      "[step: 13256] loss: 0.008429702371358871\n",
      "[step: 13257] loss: 0.0084287840873003\n",
      "[step: 13258] loss: 0.008427862077951431\n",
      "[step: 13259] loss: 0.008426936343312263\n",
      "[step: 13260] loss: 0.008426009677350521\n",
      "[step: 13261] loss: 0.008425083942711353\n",
      "[step: 13262] loss: 0.008424164727330208\n",
      "[step: 13263] loss: 0.008423238061368465\n",
      "[step: 13264] loss: 0.008422310464084148\n",
      "[step: 13265] loss: 0.008421383798122406\n",
      "[step: 13266] loss: 0.008420458994805813\n",
      "[step: 13267] loss: 0.00841953419148922\n",
      "[step: 13268] loss: 0.008418606594204903\n",
      "[step: 13269] loss: 0.008417676202952862\n",
      "[step: 13270] loss: 0.008416753262281418\n",
      "[step: 13271] loss: 0.008415824733674526\n",
      "[step: 13272] loss: 0.008414899930357933\n",
      "[step: 13273] loss: 0.008413966745138168\n",
      "[step: 13274] loss: 0.00841303076595068\n",
      "[step: 13275] loss: 0.008412103168666363\n",
      "[step: 13276] loss: 0.008411174640059471\n",
      "[step: 13277] loss: 0.008410237729549408\n",
      "[step: 13278] loss: 0.008409307338297367\n",
      "[step: 13279] loss: 0.008408374153077602\n",
      "[step: 13280] loss: 0.008407444693148136\n",
      "[step: 13281] loss: 0.008406508713960648\n",
      "[step: 13282] loss: 0.008405577391386032\n",
      "[step: 13283] loss: 0.008404643274843693\n",
      "[step: 13284] loss: 0.00840370636433363\n",
      "[step: 13285] loss: 0.00840277411043644\n",
      "[step: 13286] loss: 0.008401835337281227\n",
      "[step: 13287] loss: 0.008400904014706612\n",
      "[step: 13288] loss: 0.008399964310228825\n",
      "[step: 13289] loss: 0.008399026468396187\n",
      "[step: 13290] loss: 0.0083980867639184\n",
      "[step: 13291] loss: 0.00839715264737606\n",
      "[step: 13292] loss: 0.00839621014893055\n",
      "[step: 13293] loss: 0.008395278826355934\n",
      "[step: 13294] loss: 0.008394339121878147\n",
      "[step: 13295] loss: 0.008393400348722935\n",
      "[step: 13296] loss: 0.00839245691895485\n",
      "[step: 13297] loss: 0.008391513489186764\n",
      "[step: 13298] loss: 0.008390570990741253\n",
      "[step: 13299] loss: 0.008389630354940891\n",
      "[step: 13300] loss: 0.008388688787817955\n",
      "[step: 13301] loss: 0.008387749083340168\n",
      "[step: 13302] loss: 0.008386805653572083\n",
      "[step: 13303] loss: 0.008385858498513699\n",
      "[step: 13304] loss: 0.008384914137423038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 13305] loss: 0.008383974432945251\n",
      "[step: 13306] loss: 0.00838303193449974\n",
      "[step: 13307] loss: 0.008382085710763931\n",
      "[step: 13308] loss: 0.008381140418350697\n",
      "[step: 13309] loss: 0.008380191400647163\n",
      "[step: 13310] loss: 0.008379246108233929\n",
      "[step: 13311] loss: 0.00837829988449812\n",
      "[step: 13312] loss: 0.008377352729439735\n",
      "[step: 13313] loss: 0.008376403711736202\n",
      "[step: 13314] loss: 0.008375454694032669\n",
      "[step: 13315] loss: 0.00837450847029686\n",
      "[step: 13316] loss: 0.008373556658625603\n",
      "[step: 13317] loss: 0.00837260764092207\n",
      "[step: 13318] loss: 0.00837166141718626\n",
      "[step: 13319] loss: 0.00837070494890213\n",
      "[step: 13320] loss: 0.008369759656488895\n",
      "[step: 13321] loss: 0.00836880411952734\n",
      "[step: 13322] loss: 0.008367852307856083\n",
      "[step: 13323] loss: 0.008366899564862251\n",
      "[step: 13324] loss: 0.008365945890545845\n",
      "[step: 13325] loss: 0.008364991284906864\n",
      "[step: 13326] loss: 0.008364042267203331\n",
      "[step: 13327] loss: 0.00836308766156435\n",
      "[step: 13328] loss: 0.008362127467989922\n",
      "[step: 13329] loss: 0.008361180312931538\n",
      "[step: 13330] loss: 0.008360220119357109\n",
      "[step: 13331] loss: 0.008359267376363277\n",
      "[step: 13332] loss: 0.008358310908079147\n",
      "[step: 13333] loss: 0.008357352577149868\n",
      "[step: 13334] loss: 0.008356396108865738\n",
      "[step: 13335] loss: 0.008355433121323586\n",
      "[step: 13336] loss: 0.008354478515684605\n",
      "[step: 13337] loss: 0.008353518322110176\n",
      "[step: 13338] loss: 0.008352561853826046\n",
      "[step: 13339] loss: 0.008351604454219341\n",
      "[step: 13340] loss: 0.008350638672709465\n",
      "[step: 13341] loss: 0.008349680341780186\n",
      "[step: 13342] loss: 0.008348722010850906\n",
      "[step: 13343] loss: 0.008347759954631329\n",
      "[step: 13344] loss: 0.008346796967089176\n",
      "[step: 13345] loss: 0.008345832116901875\n",
      "[step: 13346] loss: 0.008344870060682297\n",
      "[step: 13347] loss: 0.008343912661075592\n",
      "[step: 13348] loss: 0.008342948742210865\n",
      "[step: 13349] loss: 0.008341983892023563\n",
      "[step: 13350] loss: 0.008341025561094284\n",
      "[step: 13351] loss: 0.008340056985616684\n",
      "[step: 13352] loss: 0.008339089341461658\n",
      "[step: 13353] loss: 0.008338120765984058\n",
      "[step: 13354] loss: 0.008337162435054779\n",
      "[step: 13355] loss: 0.008336192928254604\n",
      "[step: 13356] loss: 0.008335220627486706\n",
      "[step: 13357] loss: 0.008334258571267128\n",
      "[step: 13358] loss: 0.008333289995789528\n",
      "[step: 13359] loss: 0.008332327008247375\n",
      "[step: 13360] loss: 0.008331353776156902\n",
      "[step: 13361] loss: 0.008330386132001877\n",
      "[step: 13362] loss: 0.008329420350492\n",
      "[step: 13363] loss: 0.00832844153046608\n",
      "[step: 13364] loss: 0.008327476680278778\n",
      "[step: 13365] loss: 0.008326510898768902\n",
      "[step: 13366] loss: 0.00832553580403328\n",
      "[step: 13367] loss: 0.00832456350326538\n",
      "[step: 13368] loss: 0.008323593996465206\n",
      "[step: 13369] loss: 0.008322625420987606\n",
      "[step: 13370] loss: 0.008321651257574558\n",
      "[step: 13371] loss: 0.008320676162838936\n",
      "[step: 13372] loss: 0.008319705724716187\n",
      "[step: 13373] loss: 0.008318730629980564\n",
      "[step: 13374] loss: 0.00831775926053524\n",
      "[step: 13375] loss: 0.008316787891089916\n",
      "[step: 13376] loss: 0.00831580813974142\n",
      "[step: 13377] loss: 0.008314837701618671\n",
      "[step: 13378] loss: 0.008313862606883049\n",
      "[step: 13379] loss: 0.008312883786857128\n",
      "[step: 13380] loss: 0.008311906829476357\n",
      "[step: 13381] loss: 0.008310931734740734\n",
      "[step: 13382] loss: 0.008309961296617985\n",
      "[step: 13383] loss: 0.00830898154526949\n",
      "[step: 13384] loss: 0.008308007381856441\n",
      "[step: 13385] loss: 0.008307022973895073\n",
      "[step: 13386] loss: 0.008306048810482025\n",
      "[step: 13387] loss: 0.008305065333843231\n",
      "[step: 13388] loss: 0.008304091170430183\n",
      "[step: 13389] loss: 0.008303111419081688\n",
      "[step: 13390] loss: 0.008302131667733192\n",
      "[step: 13391] loss: 0.008301151916384697\n",
      "[step: 13392] loss: 0.008300169371068478\n",
      "[step: 13393] loss: 0.008299191482365131\n",
      "[step: 13394] loss: 0.008298208005726337\n",
      "[step: 13395] loss: 0.008297225460410118\n",
      "[step: 13396] loss: 0.008296250365674496\n",
      "[step: 13397] loss: 0.008295266889035702\n",
      "[step: 13398] loss: 0.008294284343719482\n",
      "[step: 13399] loss: 0.008293304592370987\n",
      "[step: 13400] loss: 0.008292320184409618\n",
      "[step: 13401] loss: 0.008291334845125675\n",
      "[step: 13402] loss: 0.008290351368486881\n",
      "[step: 13403] loss: 0.008289370685815811\n",
      "[step: 13404] loss: 0.008288383483886719\n",
      "[step: 13405] loss: 0.0082874009385705\n",
      "[step: 13406] loss: 0.008286411873996258\n",
      "[step: 13407] loss: 0.00828542374074459\n",
      "[step: 13408] loss: 0.00828444305807352\n",
      "[step: 13409] loss: 0.008283453993499279\n",
      "[step: 13410] loss: 0.008282468654215336\n",
      "[step: 13411] loss: 0.008281481452286243\n",
      "[step: 13412] loss: 0.00828049797564745\n",
      "[step: 13413] loss: 0.00827950332313776\n",
      "[step: 13414] loss: 0.008278517052531242\n",
      "[step: 13415] loss: 0.008277532644569874\n",
      "[step: 13416] loss: 0.008276539854705334\n",
      "[step: 13417] loss: 0.008275549858808517\n",
      "[step: 13418] loss: 0.008274563588202\n",
      "[step: 13419] loss: 0.008273572660982609\n",
      "[step: 13420] loss: 0.008272582665085793\n",
      "[step: 13421] loss: 0.008271592669188976\n",
      "[step: 13422] loss: 0.008270593360066414\n",
      "[step: 13423] loss: 0.00826960988342762\n",
      "[step: 13424] loss: 0.008268613368272781\n",
      "[step: 13425] loss: 0.008267627097666264\n",
      "[step: 13426] loss: 0.008266628719866276\n",
      "[step: 13427] loss: 0.008265639655292034\n",
      "[step: 13428] loss: 0.00826464407145977\n",
      "[step: 13429] loss: 0.00826365128159523\n",
      "[step: 13430] loss: 0.008262650109827518\n",
      "[step: 13431] loss: 0.008261657319962978\n",
      "[step: 13432] loss: 0.008260659873485565\n",
      "[step: 13433] loss: 0.008259669877588749\n",
      "[step: 13434] loss: 0.008258670568466187\n",
      "[step: 13435] loss: 0.008257666602730751\n",
      "[step: 13436] loss: 0.008256676606833935\n",
      "[step: 13437] loss: 0.008255677297711372\n",
      "[step: 13438] loss: 0.00825467985123396\n",
      "[step: 13439] loss: 0.008253680542111397\n",
      "[step: 13440] loss: 0.008252677507698536\n",
      "[step: 13441] loss: 0.008251679129898548\n",
      "[step: 13442] loss: 0.008250679820775986\n",
      "[step: 13443] loss: 0.008249680511653423\n",
      "[step: 13444] loss: 0.008248685859143734\n",
      "[step: 13445] loss: 0.00824768003076315\n",
      "[step: 13446] loss: 0.008246670477092266\n",
      "[step: 13447] loss: 0.008245675824582577\n",
      "[step: 13448] loss: 0.008244669996201992\n",
      "[step: 13449] loss: 0.008243662305176258\n",
      "[step: 13450] loss: 0.008242659270763397\n",
      "[step: 13451] loss: 0.008241654373705387\n",
      "[step: 13452] loss: 0.008240658789873123\n",
      "[step: 13453] loss: 0.008239646442234516\n",
      "[step: 13454] loss: 0.008238639682531357\n",
      "[step: 13455] loss: 0.008237630128860474\n",
      "[step: 13456] loss: 0.008236625231802464\n",
      "[step: 13457] loss: 0.008235613815486431\n",
      "[step: 13458] loss: 0.00823461264371872\n",
      "[step: 13459] loss: 0.00823359563946724\n",
      "[step: 13460] loss: 0.008232592605054379\n",
      "[step: 13461] loss: 0.008231576532125473\n",
      "[step: 13462] loss: 0.008230569772422314\n",
      "[step: 13463] loss: 0.008229556493461132\n",
      "[step: 13464] loss: 0.008228541351854801\n",
      "[step: 13465] loss: 0.008227527141571045\n",
      "[step: 13466] loss: 0.008226516656577587\n",
      "[step: 13467] loss: 0.00822550430893898\n",
      "[step: 13468] loss: 0.0082244873046875\n",
      "[step: 13469] loss: 0.008223473094403744\n",
      "[step: 13470] loss: 0.008222454227507114\n",
      "[step: 13471] loss: 0.008221442811191082\n",
      "[step: 13472] loss: 0.008220422081649303\n",
      "[step: 13473] loss: 0.008219408802688122\n",
      "[step: 13474] loss: 0.008218384347856045\n",
      "[step: 13475] loss: 0.008217372000217438\n",
      "[step: 13476] loss: 0.008216348476707935\n",
      "[step: 13477] loss: 0.008215325884521008\n",
      "[step: 13478] loss: 0.008214306086301804\n",
      "[step: 13479] loss: 0.008213281631469727\n",
      "[step: 13480] loss: 0.008212264627218246\n",
      "[step: 13481] loss: 0.008211242035031319\n",
      "[step: 13482] loss: 0.008210216648876667\n",
      "[step: 13483] loss: 0.00820919405668974\n",
      "[step: 13484] loss: 0.008208164945244789\n",
      "[step: 13485] loss: 0.00820714607834816\n",
      "[step: 13486] loss: 0.00820611696690321\n",
      "[step: 13487] loss: 0.008205094374716282\n",
      "[step: 13488] loss: 0.008204065263271332\n",
      "[step: 13489] loss: 0.008203034289181232\n",
      "[step: 13490] loss: 0.008201999589800835\n",
      "[step: 13491] loss: 0.008200978860259056\n",
      "[step: 13492] loss: 0.008199946023523808\n",
      "[step: 13493] loss: 0.008198910392820835\n",
      "[step: 13494] loss: 0.008197878487408161\n",
      "[step: 13495] loss: 0.008196848444640636\n",
      "[step: 13496] loss: 0.00819582212716341\n",
      "[step: 13497] loss: 0.008194787427783012\n",
      "[step: 13498] loss: 0.00819374993443489\n",
      "[step: 13499] loss: 0.008192712441086769\n",
      "[step: 13500] loss: 0.008191674947738647\n",
      "[step: 13501] loss: 0.008190643973648548\n",
      "[step: 13502] loss: 0.008189598098397255\n",
      "[step: 13503] loss: 0.008188562467694283\n",
      "[step: 13504] loss: 0.008187524974346161\n",
      "[step: 13505] loss: 0.008186486549675465\n",
      "[step: 13506] loss: 0.00818544439971447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 13507] loss: 0.008184402249753475\n",
      "[step: 13508] loss: 0.00818336196243763\n",
      "[step: 13509] loss: 0.008182317018508911\n",
      "[step: 13510] loss: 0.008181274868547916\n",
      "[step: 13511] loss: 0.008180229924619198\n",
      "[step: 13512] loss: 0.008179177530109882\n",
      "[step: 13513] loss: 0.00817814003676176\n",
      "[step: 13514] loss: 0.008177087642252445\n",
      "[step: 13515] loss: 0.008176042698323727\n",
      "[step: 13516] loss: 0.008174994960427284\n",
      "[step: 13517] loss: 0.008173941634595394\n",
      "[step: 13518] loss: 0.008172892965376377\n",
      "[step: 13519] loss: 0.008171841502189636\n",
      "[step: 13520] loss: 0.008170786313712597\n",
      "[step: 13521] loss: 0.008169732987880707\n",
      "[step: 13522] loss: 0.008168677799403667\n",
      "[step: 13523] loss: 0.008167625404894352\n",
      "[step: 13524] loss: 0.008166567422449589\n",
      "[step: 13525] loss: 0.008165515027940273\n",
      "[step: 13526] loss: 0.008164455182850361\n",
      "[step: 13527] loss: 0.008163398131728172\n",
      "[step: 13528] loss: 0.008162343874573708\n",
      "[step: 13529] loss: 0.008161273784935474\n",
      "[step: 13530] loss: 0.008160221390426159\n",
      "[step: 13531] loss: 0.008159153163433075\n",
      "[step: 13532] loss: 0.008158092387020588\n",
      "[step: 13533] loss: 0.008157020434737206\n",
      "[step: 13534] loss: 0.008155962452292442\n",
      "[step: 13535] loss: 0.008154897019267082\n",
      "[step: 13536] loss: 0.008153831586241722\n",
      "[step: 13537] loss: 0.008152765221893787\n",
      "[step: 13538] loss: 0.008151690475642681\n",
      "[step: 13539] loss: 0.008150625973939896\n",
      "[step: 13540] loss: 0.008149555884301662\n",
      "[step: 13541] loss: 0.008148484863340855\n",
      "[step: 13542] loss: 0.008147419430315495\n",
      "[step: 13543] loss: 0.008146340027451515\n",
      "[step: 13544] loss: 0.008145264349877834\n",
      "[step: 13545] loss: 0.008144186809659004\n",
      "[step: 13546] loss: 0.008143112063407898\n",
      "[step: 13547] loss: 0.008142034523189068\n",
      "[step: 13548] loss: 0.008140956051647663\n",
      "[step: 13549] loss: 0.008139878511428833\n",
      "[step: 13550] loss: 0.00813879445195198\n",
      "[step: 13551] loss: 0.008137715049088001\n",
      "[step: 13552] loss: 0.008136631920933723\n",
      "[step: 13553] loss: 0.008135553449392319\n",
      "[step: 13554] loss: 0.008134470321238041\n",
      "[step: 13555] loss: 0.008133381605148315\n",
      "[step: 13556] loss: 0.00813229288905859\n",
      "[step: 13557] loss: 0.008131207898259163\n",
      "[step: 13558] loss: 0.008130120113492012\n",
      "[step: 13559] loss: 0.008129027672111988\n",
      "[step: 13560] loss: 0.00812793429940939\n",
      "[step: 13561] loss: 0.008126843720674515\n",
      "[step: 13562] loss: 0.00812575127929449\n",
      "[step: 13563] loss: 0.00812466349452734\n",
      "[step: 13564] loss: 0.00812355987727642\n",
      "[step: 13565] loss: 0.008122462779283524\n",
      "[step: 13566] loss: 0.008121367543935776\n",
      "[step: 13567] loss: 0.00812026858329773\n",
      "[step: 13568] loss: 0.008119169622659683\n",
      "[step: 13569] loss: 0.008118066936731339\n",
      "[step: 13570] loss: 0.008116966113448143\n",
      "[step: 13571] loss: 0.008115863427519798\n",
      "[step: 13572] loss: 0.00811475608497858\n",
      "[step: 13573] loss: 0.008113651536405087\n",
      "[step: 13574] loss: 0.008112546987831593\n",
      "[step: 13575] loss: 0.008111432194709778\n",
      "[step: 13576] loss: 0.00811032485216856\n",
      "[step: 13577] loss: 0.008109216578304768\n",
      "[step: 13578] loss: 0.00810810923576355\n",
      "[step: 13579] loss: 0.008106987923383713\n",
      "[step: 13580] loss: 0.008105875924229622\n",
      "[step: 13581] loss: 0.008104758337140083\n",
      "[step: 13582] loss: 0.008103645406663418\n",
      "[step: 13583] loss: 0.008102537132799625\n",
      "[step: 13584] loss: 0.00810141023248434\n",
      "[step: 13585] loss: 0.008100291714072227\n",
      "[step: 13586] loss: 0.008099174126982689\n",
      "[step: 13587] loss: 0.008098054677248001\n",
      "[step: 13588] loss: 0.008096961304545403\n",
      "[step: 13589] loss: 0.0080958791077137\n",
      "[step: 13590] loss: 0.00809487234801054\n",
      "[step: 13591] loss: 0.008094042539596558\n",
      "[step: 13592] loss: 0.008093703538179398\n",
      "[step: 13593] loss: 0.00809452310204506\n",
      "[step: 13594] loss: 0.008098953403532505\n",
      "[step: 13595] loss: 0.008110840804874897\n",
      "[step: 13596] loss: 0.008149377070367336\n",
      "[step: 13597] loss: 0.008207852020859718\n",
      "[step: 13598] loss: 0.008366554975509644\n",
      "[step: 13599] loss: 0.00827814545482397\n",
      "[step: 13600] loss: 0.008230792358517647\n",
      "[step: 13601] loss: 0.008103952743113041\n",
      "[step: 13602] loss: 0.00809570588171482\n",
      "[step: 13603] loss: 0.008180796168744564\n",
      "[step: 13604] loss: 0.008233587257564068\n",
      "[step: 13605] loss: 0.008260165341198444\n",
      "[step: 13606] loss: 0.00811813399195671\n",
      "[step: 13607] loss: 0.008111770264804363\n",
      "[step: 13608] loss: 0.008229535073041916\n",
      "[step: 13609] loss: 0.008203335106372833\n",
      "[step: 13610] loss: 0.008124480955302715\n",
      "[step: 13611] loss: 0.00808745063841343\n",
      "[step: 13612] loss: 0.008142631500959396\n",
      "[step: 13613] loss: 0.00818821880966425\n",
      "[step: 13614] loss: 0.008092244155704975\n",
      "[step: 13615] loss: 0.008104011416435242\n",
      "[step: 13616] loss: 0.008179089985787868\n",
      "[step: 13617] loss: 0.008141043595969677\n",
      "[step: 13618] loss: 0.008078099228441715\n",
      "[step: 13619] loss: 0.008120511658489704\n",
      "[step: 13620] loss: 0.008143522776663303\n",
      "[step: 13621] loss: 0.00814514234662056\n",
      "[step: 13622] loss: 0.00807069893926382\n",
      "[step: 13623] loss: 0.008113541640341282\n",
      "[step: 13624] loss: 0.008170021697878838\n",
      "[step: 13625] loss: 0.008101250976324081\n",
      "[step: 13626] loss: 0.00807473435997963\n",
      "[step: 13627] loss: 0.008143305778503418\n",
      "[step: 13628] loss: 0.00815928541123867\n",
      "[step: 13629] loss: 0.00810905359685421\n",
      "[step: 13630] loss: 0.008077647536993027\n",
      "[step: 13631] loss: 0.008115959353744984\n",
      "[step: 13632] loss: 0.008199227973818779\n",
      "[step: 13633] loss: 0.00808016024529934\n",
      "[step: 13634] loss: 0.00808010995388031\n",
      "[step: 13635] loss: 0.008130308240652084\n",
      "[step: 13636] loss: 0.008137175813317299\n",
      "[step: 13637] loss: 0.008065917529165745\n",
      "[step: 13638] loss: 0.008071421645581722\n",
      "[step: 13639] loss: 0.008103054948151112\n",
      "[step: 13640] loss: 0.008148666471242905\n",
      "[step: 13641] loss: 0.008069658651947975\n",
      "[step: 13642] loss: 0.008056462742388248\n",
      "[step: 13643] loss: 0.008090516552329063\n",
      "[step: 13644] loss: 0.008088515140116215\n",
      "[step: 13645] loss: 0.008073239587247372\n",
      "[step: 13646] loss: 0.008044734597206116\n",
      "[step: 13647] loss: 0.008046861737966537\n",
      "[step: 13648] loss: 0.00805314164608717\n",
      "[step: 13649] loss: 0.008060881868004799\n",
      "[step: 13650] loss: 0.008061456494033337\n",
      "[step: 13651] loss: 0.008045097813010216\n",
      "[step: 13652] loss: 0.008037260733544827\n",
      "[step: 13653] loss: 0.008034005761146545\n",
      "[step: 13654] loss: 0.008041376248002052\n",
      "[step: 13655] loss: 0.008045939728617668\n",
      "[step: 13656] loss: 0.008047736249864101\n",
      "[step: 13657] loss: 0.008050027303397655\n",
      "[step: 13658] loss: 0.00804193690419197\n",
      "[step: 13659] loss: 0.008036929182708263\n",
      "[step: 13660] loss: 0.008030173368752003\n",
      "[step: 13661] loss: 0.008026715368032455\n",
      "[step: 13662] loss: 0.008023574948310852\n",
      "[step: 13663] loss: 0.008021042682230473\n",
      "[step: 13664] loss: 0.008020139299333096\n",
      "[step: 13665] loss: 0.008018332533538342\n",
      "[step: 13666] loss: 0.008017688058316708\n",
      "[step: 13667] loss: 0.008017071522772312\n",
      "[step: 13668] loss: 0.008016740903258324\n",
      "[step: 13669] loss: 0.008018290624022484\n",
      "[step: 13670] loss: 0.008022742345929146\n",
      "[step: 13671] loss: 0.008032867684960365\n",
      "[step: 13672] loss: 0.008061510510742664\n",
      "[step: 13673] loss: 0.008104398846626282\n",
      "[step: 13674] loss: 0.00821930356323719\n",
      "[step: 13675] loss: 0.00819770060479641\n",
      "[step: 13676] loss: 0.008204304613173008\n",
      "[step: 13677] loss: 0.0080619091168046\n",
      "[step: 13678] loss: 0.008007614873349667\n",
      "[step: 13679] loss: 0.008022082038223743\n",
      "[step: 13680] loss: 0.00808008760213852\n",
      "[step: 13681] loss: 0.008171487599611282\n",
      "[step: 13682] loss: 0.00812300480902195\n",
      "[step: 13683] loss: 0.008048957213759422\n",
      "[step: 13684] loss: 0.008004470728337765\n",
      "[step: 13685] loss: 0.008065388537943363\n",
      "[step: 13686] loss: 0.008177963085472584\n",
      "[step: 13687] loss: 0.008122337982058525\n",
      "[step: 13688] loss: 0.008045164868235588\n",
      "[step: 13689] loss: 0.008013362996280193\n",
      "[step: 13690] loss: 0.008083038963377476\n",
      "[step: 13691] loss: 0.008168826811015606\n",
      "[step: 13692] loss: 0.008070231415331364\n",
      "[step: 13693] loss: 0.008005944080650806\n",
      "[step: 13694] loss: 0.008025865070521832\n",
      "[step: 13695] loss: 0.008056871592998505\n",
      "[step: 13696] loss: 0.0080464668571949\n",
      "[step: 13697] loss: 0.008003599010407925\n",
      "[step: 13698] loss: 0.007993940263986588\n",
      "[step: 13699] loss: 0.008019229397177696\n",
      "[step: 13700] loss: 0.008017980493605137\n",
      "[step: 13701] loss: 0.007998793385922909\n",
      "[step: 13702] loss: 0.007986264303326607\n",
      "[step: 13703] loss: 0.00798998773097992\n",
      "[step: 13704] loss: 0.008002049289643764\n",
      "[step: 13705] loss: 0.008010040037333965\n",
      "[step: 13706] loss: 0.00800766795873642\n",
      "[step: 13707] loss: 0.007988090626895428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 13708] loss: 0.007977680303156376\n",
      "[step: 13709] loss: 0.007976681925356388\n",
      "[step: 13710] loss: 0.007979939691722393\n",
      "[step: 13711] loss: 0.007990962825715542\n",
      "[step: 13712] loss: 0.008001905865967274\n",
      "[step: 13713] loss: 0.008016079664230347\n",
      "[step: 13714] loss: 0.008022068068385124\n",
      "[step: 13715] loss: 0.008050077594816685\n",
      "[step: 13716] loss: 0.008055822923779488\n",
      "[step: 13717] loss: 0.008086184971034527\n",
      "[step: 13718] loss: 0.008050793781876564\n",
      "[step: 13719] loss: 0.00802922435104847\n",
      "[step: 13720] loss: 0.007994266226887703\n",
      "[step: 13721] loss: 0.007980341091752052\n",
      "[step: 13722] loss: 0.007970591075718403\n",
      "[step: 13723] loss: 0.007966007106006145\n",
      "[step: 13724] loss: 0.007962513715028763\n",
      "[step: 13725] loss: 0.00795991625636816\n",
      "[step: 13726] loss: 0.007958408445119858\n",
      "[step: 13727] loss: 0.007957667112350464\n",
      "[step: 13728] loss: 0.007957816123962402\n",
      "[step: 13729] loss: 0.007960410788655281\n",
      "[step: 13730] loss: 0.007965408265590668\n",
      "[step: 13731] loss: 0.00798050407320261\n",
      "[step: 13732] loss: 0.008005624637007713\n",
      "[step: 13733] loss: 0.00807951670140028\n",
      "[step: 13734] loss: 0.008130175061523914\n",
      "[step: 13735] loss: 0.008211318403482437\n",
      "[step: 13736] loss: 0.008042936213314533\n",
      "[step: 13737] loss: 0.00795544870197773\n",
      "[step: 13738] loss: 0.007955910637974739\n",
      "[step: 13739] loss: 0.008027128875255585\n",
      "[step: 13740] loss: 0.008156556636095047\n",
      "[step: 13741] loss: 0.0081454673781991\n",
      "[step: 13742] loss: 0.00804536696523428\n",
      "[step: 13743] loss: 0.007964725606143475\n",
      "[step: 13744] loss: 0.008159471675753593\n",
      "[step: 13745] loss: 0.008438853546977043\n",
      "[step: 13746] loss: 0.008062568493187428\n",
      "[step: 13747] loss: 0.008137980476021767\n",
      "[step: 13748] loss: 0.008442232385277748\n",
      "[step: 13749] loss: 0.008136874064803123\n",
      "[step: 13750] loss: 0.008489202708005905\n",
      "[step: 13751] loss: 0.008548092097043991\n",
      "[step: 13752] loss: 0.008146954700350761\n",
      "[step: 13753] loss: 0.008324163034558296\n",
      "[step: 13754] loss: 0.00806982722133398\n",
      "[step: 13755] loss: 0.00836780946701765\n",
      "[step: 13756] loss: 0.00806070864200592\n",
      "[step: 13757] loss: 0.008249141275882721\n",
      "[step: 13758] loss: 0.008143855258822441\n",
      "[step: 13759] loss: 0.008084496483206749\n",
      "[step: 13760] loss: 0.008113331161439419\n",
      "[step: 13761] loss: 0.00807877629995346\n",
      "[step: 13762] loss: 0.008085518144071102\n",
      "[step: 13763] loss: 0.008071696385741234\n",
      "[step: 13764] loss: 0.008033414371311665\n",
      "[step: 13765] loss: 0.008089005015790462\n",
      "[step: 13766] loss: 0.007994569838047028\n",
      "[step: 13767] loss: 0.008017128333449364\n",
      "[step: 13768] loss: 0.007981154136359692\n",
      "[step: 13769] loss: 0.007974481210112572\n",
      "[step: 13770] loss: 0.008005580864846706\n",
      "[step: 13771] loss: 0.007933562621474266\n",
      "[step: 13772] loss: 0.007986952550709248\n",
      "[step: 13773] loss: 0.007973258383572102\n",
      "[step: 13774] loss: 0.007946092635393143\n",
      "[step: 13775] loss: 0.007977024652063847\n",
      "[step: 13776] loss: 0.007949521765112877\n",
      "[step: 13777] loss: 0.007978610694408417\n",
      "[step: 13778] loss: 0.00795760378241539\n",
      "[step: 13779] loss: 0.007923145778477192\n",
      "[step: 13780] loss: 0.007938382215797901\n",
      "[step: 13781] loss: 0.007930457592010498\n",
      "[step: 13782] loss: 0.00794996041804552\n",
      "[step: 13783] loss: 0.00799053069204092\n",
      "[step: 13784] loss: 0.007993102073669434\n",
      "[step: 13785] loss: 0.00796451885253191\n",
      "[step: 13786] loss: 0.007940959185361862\n",
      "[step: 13787] loss: 0.007922411896288395\n",
      "[step: 13788] loss: 0.007924438454210758\n",
      "[step: 13789] loss: 0.007914189249277115\n",
      "[step: 13790] loss: 0.007915003225207329\n",
      "[step: 13791] loss: 0.007919696159660816\n",
      "[step: 13792] loss: 0.007912753149867058\n",
      "[step: 13793] loss: 0.007917957380414009\n",
      "[step: 13794] loss: 0.007936414331197739\n",
      "[step: 13795] loss: 0.00795749481767416\n",
      "[step: 13796] loss: 0.00801689550280571\n",
      "[step: 13797] loss: 0.00797497108578682\n",
      "[step: 13798] loss: 0.007967662066221237\n",
      "[step: 13799] loss: 0.007938573136925697\n",
      "[step: 13800] loss: 0.007917888462543488\n",
      "[step: 13801] loss: 0.007902215234935284\n",
      "[step: 13802] loss: 0.007897615432739258\n",
      "[step: 13803] loss: 0.007887902669608593\n",
      "[step: 13804] loss: 0.007885290309786797\n",
      "[step: 13805] loss: 0.00788746029138565\n",
      "[step: 13806] loss: 0.007885828614234924\n",
      "[step: 13807] loss: 0.007888386957347393\n",
      "[step: 13808] loss: 0.00789294671267271\n",
      "[step: 13809] loss: 0.00789529737085104\n",
      "[step: 13810] loss: 0.007899902760982513\n",
      "[step: 13811] loss: 0.007917017675936222\n",
      "[step: 13812] loss: 0.007920649833977222\n",
      "[step: 13813] loss: 0.00794557947665453\n",
      "[step: 13814] loss: 0.007941481657326221\n",
      "[step: 13815] loss: 0.007952583953738213\n",
      "[step: 13816] loss: 0.00793042778968811\n",
      "[step: 13817] loss: 0.007936415262520313\n",
      "[step: 13818] loss: 0.007910736836493015\n",
      "[step: 13819] loss: 0.00790254957973957\n",
      "[step: 13820] loss: 0.007888756692409515\n",
      "[step: 13821] loss: 0.007880966179072857\n",
      "[step: 13822] loss: 0.007875110022723675\n",
      "[step: 13823] loss: 0.007873320952057838\n",
      "[step: 13824] loss: 0.007870457135140896\n",
      "[step: 13825] loss: 0.007871788926422596\n",
      "[step: 13826] loss: 0.007874805480241776\n",
      "[step: 13827] loss: 0.007883970625698566\n",
      "[step: 13828] loss: 0.007898692041635513\n",
      "[step: 13829] loss: 0.007947267033159733\n",
      "[step: 13830] loss: 0.007969427853822708\n",
      "[step: 13831] loss: 0.008047156035900116\n",
      "[step: 13832] loss: 0.007980003021657467\n",
      "[step: 13833] loss: 0.007932320237159729\n",
      "[step: 13834] loss: 0.007871358655393124\n",
      "[step: 13835] loss: 0.007853026501834393\n",
      "[step: 13836] loss: 0.007855120114982128\n",
      "[step: 13837] loss: 0.007873855531215668\n",
      "[step: 13838] loss: 0.007915112189948559\n",
      "[step: 13839] loss: 0.00792173482477665\n",
      "[step: 13840] loss: 0.007928913459181786\n",
      "[step: 13841] loss: 0.00790252722799778\n",
      "[step: 13842] loss: 0.007884079590439796\n",
      "[step: 13843] loss: 0.007864533923566341\n",
      "[step: 13844] loss: 0.007855580188333988\n",
      "[step: 13845] loss: 0.007846013642847538\n",
      "[step: 13846] loss: 0.007841570302844048\n",
      "[step: 13847] loss: 0.007839218713343143\n",
      "[step: 13848] loss: 0.007836508564651012\n",
      "[step: 13849] loss: 0.007834391668438911\n",
      "[step: 13850] loss: 0.007833044044673443\n",
      "[step: 13851] loss: 0.007831547409296036\n",
      "[step: 13852] loss: 0.007829582318663597\n",
      "[step: 13853] loss: 0.0078278174623847\n",
      "[step: 13854] loss: 0.007826575078070164\n",
      "[step: 13855] loss: 0.00782508309930563\n",
      "[step: 13856] loss: 0.007823494262993336\n",
      "[step: 13857] loss: 0.00782325305044651\n",
      "[step: 13858] loss: 0.007828067988157272\n",
      "[step: 13859] loss: 0.007854945957660675\n",
      "[step: 13860] loss: 0.008001729846000671\n",
      "[step: 13861] loss: 0.008308971300721169\n",
      "[step: 13862] loss: 0.008822426199913025\n",
      "[step: 13863] loss: 0.00815710611641407\n",
      "[step: 13864] loss: 0.009247844107449055\n",
      "[step: 13865] loss: 0.009426389820873737\n",
      "[step: 13866] loss: 0.00884053111076355\n",
      "[step: 13867] loss: 0.008595774881541729\n",
      "[step: 13868] loss: 0.008704932406544685\n",
      "[step: 13869] loss: 0.008552070707082748\n",
      "[step: 13870] loss: 0.008674533106386662\n",
      "[step: 13871] loss: 0.00834616832435131\n",
      "[step: 13872] loss: 0.008483017794787884\n",
      "[step: 13873] loss: 0.008261444978415966\n",
      "[step: 13874] loss: 0.008495638146996498\n",
      "[step: 13875] loss: 0.008082500658929348\n",
      "[step: 13876] loss: 0.008478274568915367\n",
      "[step: 13877] loss: 0.008088844828307629\n",
      "[step: 13878] loss: 0.008244549855589867\n",
      "[step: 13879] loss: 0.00797060877084732\n",
      "[step: 13880] loss: 0.008393063209950924\n",
      "[step: 13881] loss: 0.007970153354108334\n",
      "[step: 13882] loss: 0.00819800142198801\n",
      "[step: 13883] loss: 0.007982863113284111\n",
      "[step: 13884] loss: 0.008181385695934296\n",
      "[step: 13885] loss: 0.007949176244437695\n",
      "[step: 13886] loss: 0.008032706566154957\n",
      "[step: 13887] loss: 0.007937366142868996\n",
      "[step: 13888] loss: 0.00802519265562296\n",
      "[step: 13889] loss: 0.00791111122816801\n",
      "[step: 13890] loss: 0.007961232215166092\n",
      "[step: 13891] loss: 0.007945681922137737\n",
      "[step: 13892] loss: 0.007927552796900272\n",
      "[step: 13893] loss: 0.007893627509474754\n",
      "[step: 13894] loss: 0.007880155928432941\n",
      "[step: 13895] loss: 0.007937771268188953\n",
      "[step: 13896] loss: 0.00786526221781969\n",
      "[step: 13897] loss: 0.007872555404901505\n",
      "[step: 13898] loss: 0.007870304398238659\n",
      "[step: 13899] loss: 0.007862210273742676\n",
      "[step: 13900] loss: 0.00785654503852129\n",
      "[step: 13901] loss: 0.00782916322350502\n",
      "[step: 13902] loss: 0.007855888456106186\n",
      "[step: 13903] loss: 0.007839217782020569\n",
      "[step: 13904] loss: 0.007815487682819366\n",
      "[step: 13905] loss: 0.00784734170883894\n",
      "[step: 13906] loss: 0.007835320197045803\n",
      "[step: 13907] loss: 0.007804890628904104\n",
      "[step: 13908] loss: 0.007826333865523338\n",
      "[step: 13909] loss: 0.00782676413655281\n",
      "[step: 13910] loss: 0.007801731117069721\n",
      "[step: 13911] loss: 0.007815542630851269\n",
      "[step: 13912] loss: 0.007821290753781796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 13913] loss: 0.007799542974680662\n",
      "[step: 13914] loss: 0.007806506939232349\n",
      "[step: 13915] loss: 0.007812147494405508\n",
      "[step: 13916] loss: 0.007794513367116451\n",
      "[step: 13917] loss: 0.007799592334777117\n",
      "[step: 13918] loss: 0.007807450834661722\n",
      "[step: 13919] loss: 0.0077908216044306755\n",
      "[step: 13920] loss: 0.00779231172055006\n",
      "[step: 13921] loss: 0.00780259957537055\n",
      "[step: 13922] loss: 0.007795518264174461\n",
      "[step: 13923] loss: 0.0078039392828941345\n",
      "[step: 13924] loss: 0.007842708379030228\n",
      "[step: 13925] loss: 0.007906211540102959\n",
      "[step: 13926] loss: 0.007938035763800144\n",
      "[step: 13927] loss: 0.008046378381550312\n",
      "[step: 13928] loss: 0.007853072136640549\n",
      "[step: 13929] loss: 0.007785850204527378\n",
      "[step: 13930] loss: 0.007800176274031401\n",
      "[step: 13931] loss: 0.007863023318350315\n",
      "[step: 13932] loss: 0.007959953509271145\n",
      "[step: 13933] loss: 0.007913079112768173\n",
      "[step: 13934] loss: 0.0077901859767735004\n",
      "[step: 13935] loss: 0.007872100919485092\n",
      "[step: 13936] loss: 0.00802881270647049\n",
      "[step: 13937] loss: 0.007970435544848442\n",
      "[step: 13938] loss: 0.007909451611340046\n",
      "[step: 13939] loss: 0.00817994587123394\n",
      "[step: 13940] loss: 0.008207511156797409\n",
      "[step: 13941] loss: 0.008216018788516521\n",
      "[step: 13942] loss: 0.008206230588257313\n",
      "[step: 13943] loss: 0.007967008277773857\n",
      "[step: 13944] loss: 0.008093072101473808\n",
      "[step: 13945] loss: 0.007835222408175468\n",
      "[step: 13946] loss: 0.008100391365587711\n",
      "[step: 13947] loss: 0.007827364839613438\n",
      "[step: 13948] loss: 0.007987924851477146\n",
      "[step: 13949] loss: 0.007932239212095737\n",
      "[step: 13950] loss: 0.007934617809951305\n",
      "[step: 13951] loss: 0.00793047621846199\n",
      "[step: 13952] loss: 0.007983091287314892\n",
      "[step: 13953] loss: 0.007836708799004555\n",
      "[step: 13954] loss: 0.008026069030165672\n",
      "[step: 13955] loss: 0.007888476364314556\n",
      "[step: 13956] loss: 0.007909242995083332\n",
      "[step: 13957] loss: 0.007916165515780449\n",
      "[step: 13958] loss: 0.007814622484147549\n",
      "[step: 13959] loss: 0.00786261074244976\n",
      "[step: 13960] loss: 0.0078067178837955\n",
      "[step: 13961] loss: 0.007850086316466331\n",
      "[step: 13962] loss: 0.00789782963693142\n",
      "[step: 13963] loss: 0.007777164224535227\n",
      "[step: 13964] loss: 0.008013724349439144\n",
      "[step: 13965] loss: 0.008194399066269398\n",
      "[step: 13966] loss: 0.00786958821117878\n",
      "[step: 13967] loss: 0.008330704644322395\n",
      "[step: 13968] loss: 0.008430443704128265\n",
      "[step: 13969] loss: 0.008307447656989098\n",
      "[step: 13970] loss: 0.00812512170523405\n",
      "[step: 13971] loss: 0.008074882440268993\n",
      "[step: 13972] loss: 0.008116650395095348\n",
      "[step: 13973] loss: 0.008190100081264973\n",
      "[step: 13974] loss: 0.007996455766260624\n",
      "[step: 13975] loss: 0.008210446685552597\n",
      "[step: 13976] loss: 0.007913331501185894\n",
      "[step: 13977] loss: 0.00807761587202549\n",
      "[step: 13978] loss: 0.00789090059697628\n",
      "[step: 13979] loss: 0.007969818077981472\n",
      "[step: 13980] loss: 0.007861662656068802\n",
      "[step: 13981] loss: 0.007924564182758331\n",
      "[step: 13982] loss: 0.007819321937859058\n",
      "[step: 13983] loss: 0.007903915829956532\n",
      "[step: 13984] loss: 0.007831315509974957\n",
      "[step: 13985] loss: 0.007895302958786488\n",
      "[step: 13986] loss: 0.007832813076674938\n",
      "[step: 13987] loss: 0.007833097130060196\n",
      "[step: 13988] loss: 0.007893750444054604\n",
      "[step: 13989] loss: 0.007827597670257092\n",
      "[step: 13990] loss: 0.007845453917980194\n",
      "[step: 13991] loss: 0.007885179482400417\n",
      "[step: 13992] loss: 0.007851962931454182\n",
      "[step: 13993] loss: 0.007810804061591625\n",
      "[step: 13994] loss: 0.00792296975851059\n",
      "[step: 13995] loss: 0.007795796729624271\n",
      "[step: 13996] loss: 0.007797867059707642\n",
      "[step: 13997] loss: 0.007847463712096214\n",
      "[step: 13998] loss: 0.007745861541479826\n",
      "[step: 13999] loss: 0.007777889259159565\n",
      "[step: 14000] loss: 0.007779666688293219\n",
      "[step: 14001] loss: 0.007753791753202677\n",
      "[step: 14002] loss: 0.007771942764520645\n",
      "[step: 14003] loss: 0.007737378589808941\n",
      "[step: 14004] loss: 0.00776051077991724\n",
      "[step: 14005] loss: 0.0078102098777890205\n",
      "[step: 14006] loss: 0.0077290465123951435\n",
      "[step: 14007] loss: 0.007744128815829754\n",
      "[step: 14008] loss: 0.007782163564115763\n",
      "[step: 14009] loss: 0.007734054233878851\n",
      "[step: 14010] loss: 0.007737061474472284\n",
      "[step: 14011] loss: 0.007736141327768564\n",
      "[step: 14012] loss: 0.007732553873211145\n",
      "[step: 14013] loss: 0.007754433434456587\n",
      "[step: 14014] loss: 0.007738669868558645\n",
      "[step: 14015] loss: 0.007708692457526922\n",
      "[step: 14016] loss: 0.00773017480969429\n",
      "[step: 14017] loss: 0.007738398388028145\n",
      "[step: 14018] loss: 0.007741241715848446\n",
      "[step: 14019] loss: 0.00774615490809083\n",
      "[step: 14020] loss: 0.007726297248154879\n",
      "[step: 14021] loss: 0.007700447924435139\n",
      "[step: 14022] loss: 0.007721537724137306\n",
      "[step: 14023] loss: 0.007727819494903088\n",
      "[step: 14024] loss: 0.007719035260379314\n",
      "[step: 14025] loss: 0.007744744885712862\n",
      "[step: 14026] loss: 0.007739733904600143\n",
      "[step: 14027] loss: 0.007709756959229708\n",
      "[step: 14028] loss: 0.00771566666662693\n",
      "[step: 14029] loss: 0.007706111762672663\n",
      "[step: 14030] loss: 0.007686171215027571\n",
      "[step: 14031] loss: 0.007710704579949379\n",
      "[step: 14032] loss: 0.007724343799054623\n",
      "[step: 14033] loss: 0.007700341288000345\n",
      "[step: 14034] loss: 0.007728289347141981\n",
      "[step: 14035] loss: 0.0077539910562336445\n",
      "[step: 14036] loss: 0.007729557808488607\n",
      "[step: 14037] loss: 0.0077349222265183926\n",
      "[step: 14038] loss: 0.007741772569715977\n",
      "[step: 14039] loss: 0.007689685095101595\n",
      "[step: 14040] loss: 0.007713163737207651\n",
      "[step: 14041] loss: 0.00773279694840312\n",
      "[step: 14042] loss: 0.007680894341319799\n",
      "[step: 14043] loss: 0.00771663524210453\n",
      "[step: 14044] loss: 0.0077347466722130775\n",
      "[step: 14045] loss: 0.0076715294271707535\n",
      "[step: 14046] loss: 0.007721547037363052\n",
      "[step: 14047] loss: 0.007768809329718351\n",
      "[step: 14048] loss: 0.007690852507948875\n",
      "[step: 14049] loss: 0.007814466021955013\n",
      "[step: 14050] loss: 0.00784389115869999\n",
      "[step: 14051] loss: 0.00771982129663229\n",
      "[step: 14052] loss: 0.007980679161846638\n",
      "[step: 14053] loss: 0.007968109101057053\n",
      "[step: 14054] loss: 0.008224506862461567\n",
      "[step: 14055] loss: 0.007774507161229849\n",
      "[step: 14056] loss: 0.008235161192715168\n",
      "[step: 14057] loss: 0.008949385024607182\n",
      "[step: 14058] loss: 0.008376764133572578\n",
      "[step: 14059] loss: 0.008497949689626694\n",
      "[step: 14060] loss: 0.00809236615896225\n",
      "[step: 14061] loss: 0.008274836465716362\n",
      "[step: 14062] loss: 0.008228595368564129\n",
      "[step: 14063] loss: 0.008407908491790295\n",
      "[step: 14064] loss: 0.008032343350350857\n",
      "[step: 14065] loss: 0.008048192597925663\n",
      "[step: 14066] loss: 0.00806089024990797\n",
      "[step: 14067] loss: 0.00801326334476471\n",
      "[step: 14068] loss: 0.008071129210293293\n",
      "[step: 14069] loss: 0.008024339564144611\n",
      "[step: 14070] loss: 0.007969164289534092\n",
      "[step: 14071] loss: 0.00778070418164134\n",
      "[step: 14072] loss: 0.008029520511627197\n",
      "[step: 14073] loss: 0.007895692251622677\n",
      "[step: 14074] loss: 0.007908416911959648\n",
      "[step: 14075] loss: 0.007822503335773945\n",
      "[step: 14076] loss: 0.007894719950854778\n",
      "[step: 14077] loss: 0.00779758719727397\n",
      "[step: 14078] loss: 0.007855145260691643\n",
      "[step: 14079] loss: 0.00775507278740406\n",
      "[step: 14080] loss: 0.007767964154481888\n",
      "[step: 14081] loss: 0.007771173492074013\n",
      "[step: 14082] loss: 0.007779855281114578\n",
      "[step: 14083] loss: 0.0077387611381709576\n",
      "[step: 14084] loss: 0.007709053810685873\n",
      "[step: 14085] loss: 0.007748325355350971\n",
      "[step: 14086] loss: 0.007694136817008257\n",
      "[step: 14087] loss: 0.0077482047490775585\n",
      "[step: 14088] loss: 0.007685893215239048\n",
      "[step: 14089] loss: 0.007705379277467728\n",
      "[step: 14090] loss: 0.007714982144534588\n",
      "[step: 14091] loss: 0.00766501622274518\n",
      "[step: 14092] loss: 0.00768820196390152\n",
      "[step: 14093] loss: 0.007671407423913479\n",
      "[step: 14094] loss: 0.007665958721190691\n",
      "[step: 14095] loss: 0.007684009615331888\n",
      "[step: 14096] loss: 0.0076460326090455055\n",
      "[step: 14097] loss: 0.0076571376994252205\n",
      "[step: 14098] loss: 0.0076745920814573765\n",
      "[step: 14099] loss: 0.007642950862646103\n",
      "[step: 14100] loss: 0.007656276226043701\n",
      "[step: 14101] loss: 0.0076949638314545155\n",
      "[step: 14102] loss: 0.0076698544435203075\n",
      "[step: 14103] loss: 0.007709403056651354\n",
      "[step: 14104] loss: 0.007762236054986715\n",
      "[step: 14105] loss: 0.007705398369580507\n",
      "[step: 14106] loss: 0.007685819175094366\n",
      "[step: 14107] loss: 0.007661610841751099\n",
      "[step: 14108] loss: 0.007624086458235979\n",
      "[step: 14109] loss: 0.007682947441935539\n",
      "[step: 14110] loss: 0.0077308532781898975\n",
      "[step: 14111] loss: 0.007645194884389639\n",
      "[step: 14112] loss: 0.007697370834648609\n",
      "[step: 14113] loss: 0.0077115935273468494\n",
      "[step: 14114] loss: 0.007621660828590393\n",
      "[step: 14115] loss: 0.007768712006509304\n",
      "[step: 14116] loss: 0.0077873594127595425\n",
      "[step: 14117] loss: 0.007750947028398514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 14118] loss: 0.007858801633119583\n",
      "[step: 14119] loss: 0.0077577377669513226\n",
      "[step: 14120] loss: 0.007970151491463184\n",
      "[step: 14121] loss: 0.0077124182134866714\n",
      "[step: 14122] loss: 0.00797350611537695\n",
      "[step: 14123] loss: 0.008615242317318916\n",
      "[step: 14124] loss: 0.008170648477971554\n",
      "[step: 14125] loss: 0.008220462128520012\n",
      "[step: 14126] loss: 0.008079473860561848\n",
      "[step: 14127] loss: 0.008113051764667034\n",
      "[step: 14128] loss: 0.00798261072486639\n",
      "[step: 14129] loss: 0.00816239695996046\n",
      "[step: 14130] loss: 0.007983788847923279\n",
      "[step: 14131] loss: 0.007924633100628853\n",
      "[step: 14132] loss: 0.007841674610972404\n",
      "[step: 14133] loss: 0.00789950042963028\n",
      "[step: 14134] loss: 0.007910395972430706\n",
      "[step: 14135] loss: 0.007886476814746857\n",
      "[step: 14136] loss: 0.007780315354466438\n",
      "[step: 14137] loss: 0.007849182933568954\n",
      "[step: 14138] loss: 0.007844479754567146\n",
      "[step: 14139] loss: 0.007867456413805485\n",
      "[step: 14140] loss: 0.0077559552155435085\n",
      "[step: 14141] loss: 0.007917751558125019\n",
      "[step: 14142] loss: 0.007824906148016453\n",
      "[step: 14143] loss: 0.007838506251573563\n",
      "[step: 14144] loss: 0.007830617018043995\n",
      "[step: 14145] loss: 0.007782494183629751\n",
      "[step: 14146] loss: 0.007677133660763502\n",
      "[step: 14147] loss: 0.007778220809996128\n",
      "[step: 14148] loss: 0.00784366112202406\n",
      "[step: 14149] loss: 0.007626609411090612\n",
      "[step: 14150] loss: 0.007854553870856762\n",
      "[step: 14151] loss: 0.007903339341282845\n",
      "[step: 14152] loss: 0.007851460948586464\n",
      "[step: 14153] loss: 0.0077212657779455185\n",
      "[step: 14154] loss: 0.00785969290882349\n",
      "[step: 14155] loss: 0.00809715036302805\n",
      "[step: 14156] loss: 0.008276363834738731\n",
      "[step: 14157] loss: 0.007853747345507145\n",
      "[step: 14158] loss: 0.00831706915050745\n",
      "[step: 14159] loss: 0.007793887052685022\n",
      "[step: 14160] loss: 0.00830021221190691\n",
      "[step: 14161] loss: 0.007682825438678265\n",
      "[step: 14162] loss: 0.008428185246884823\n",
      "[step: 14163] loss: 0.007859133183956146\n",
      "[step: 14164] loss: 0.008289598859846592\n",
      "[step: 14165] loss: 0.008041429333388805\n",
      "[step: 14166] loss: 0.008061924017965794\n",
      "[step: 14167] loss: 0.007884060963988304\n",
      "[step: 14168] loss: 0.008033087477087975\n",
      "[step: 14169] loss: 0.008071760646998882\n",
      "[step: 14170] loss: 0.007914024405181408\n",
      "[step: 14171] loss: 0.007845090702176094\n",
      "[step: 14172] loss: 0.007844442501664162\n",
      "[step: 14173] loss: 0.007726428564637899\n",
      "[step: 14174] loss: 0.007784717250615358\n",
      "[step: 14175] loss: 0.007780001033097506\n",
      "[step: 14176] loss: 0.007714380044490099\n",
      "[step: 14177] loss: 0.007734762039035559\n",
      "[step: 14178] loss: 0.007767191156744957\n",
      "[step: 14179] loss: 0.007757408078759909\n",
      "[step: 14180] loss: 0.007644101046025753\n",
      "[step: 14181] loss: 0.007750299293547869\n",
      "[step: 14182] loss: 0.0076651498675346375\n",
      "[step: 14183] loss: 0.00767500139772892\n",
      "[step: 14184] loss: 0.007646034471690655\n",
      "[step: 14185] loss: 0.0076743257232010365\n",
      "[step: 14186] loss: 0.0076180254109203815\n",
      "[step: 14187] loss: 0.007657189387828112\n",
      "[step: 14188] loss: 0.0076196216978132725\n",
      "[step: 14189] loss: 0.00762543547898531\n",
      "[step: 14190] loss: 0.007630114443600178\n",
      "[step: 14191] loss: 0.007610677741467953\n",
      "[step: 14192] loss: 0.007612645160406828\n",
      "[step: 14193] loss: 0.007605954073369503\n",
      "[step: 14194] loss: 0.007629044353961945\n",
      "[step: 14195] loss: 0.0075865634717047215\n",
      "[step: 14196] loss: 0.007601879071444273\n",
      "[step: 14197] loss: 0.007623887155205011\n",
      "[step: 14198] loss: 0.007599474396556616\n",
      "[step: 14199] loss: 0.007595406845211983\n",
      "[step: 14200] loss: 0.0075802491046488285\n",
      "[step: 14201] loss: 0.0075699333101511\n",
      "[step: 14202] loss: 0.007574678864330053\n",
      "[step: 14203] loss: 0.007571862544864416\n",
      "[step: 14204] loss: 0.007597753778100014\n",
      "[step: 14205] loss: 0.007621700409799814\n",
      "[step: 14206] loss: 0.0077429357916116714\n",
      "[step: 14207] loss: 0.007658649235963821\n",
      "[step: 14208] loss: 0.007562933024019003\n",
      "[step: 14209] loss: 0.00784039031714201\n",
      "[step: 14210] loss: 0.008020888082683086\n",
      "[step: 14211] loss: 0.008341985754668713\n",
      "[step: 14212] loss: 0.008147142827510834\n",
      "[step: 14213] loss: 0.008098163641989231\n",
      "[step: 14214] loss: 0.007901342585682869\n",
      "[step: 14215] loss: 0.008006865158677101\n",
      "[step: 14216] loss: 0.007962888106703758\n",
      "[step: 14217] loss: 0.008043856360018253\n",
      "[step: 14218] loss: 0.00797546561807394\n",
      "[step: 14219] loss: 0.007800217252224684\n",
      "[step: 14220] loss: 0.007733708713203669\n",
      "[step: 14221] loss: 0.007880154065787792\n",
      "[step: 14222] loss: 0.007810750976204872\n",
      "[step: 14223] loss: 0.007973065599799156\n",
      "[step: 14224] loss: 0.007845833897590637\n",
      "[step: 14225] loss: 0.007809879723936319\n",
      "[step: 14226] loss: 0.007720876485109329\n",
      "[step: 14227] loss: 0.007794581353664398\n",
      "[step: 14228] loss: 0.007899667136371136\n",
      "[step: 14229] loss: 0.007674617692828178\n",
      "[step: 14230] loss: 0.007900631055235863\n",
      "[step: 14231] loss: 0.007743149530142546\n",
      "[step: 14232] loss: 0.007912274450063705\n",
      "[step: 14233] loss: 0.007799999322742224\n",
      "[step: 14234] loss: 0.007733997423201799\n",
      "[step: 14235] loss: 0.007811507675796747\n",
      "[step: 14236] loss: 0.007766677066683769\n",
      "[step: 14237] loss: 0.007796084508299828\n",
      "[step: 14238] loss: 0.007600076496601105\n",
      "[step: 14239] loss: 0.007823037914931774\n",
      "[step: 14240] loss: 0.0077162208035588264\n",
      "[step: 14241] loss: 0.007902959361672401\n",
      "[step: 14242] loss: 0.007661088369786739\n",
      "[step: 14243] loss: 0.0077917324379086494\n",
      "[step: 14244] loss: 0.007553256116807461\n",
      "[step: 14245] loss: 0.007660894654691219\n",
      "[step: 14246] loss: 0.00760883092880249\n",
      "[step: 14247] loss: 0.007821006700396538\n",
      "[step: 14248] loss: 0.007981753908097744\n",
      "[step: 14249] loss: 0.008802274242043495\n",
      "[step: 14250] loss: 0.00853082537651062\n",
      "[step: 14251] loss: 0.008279718458652496\n",
      "[step: 14252] loss: 0.008366292342543602\n",
      "[step: 14253] loss: 0.008250831626355648\n",
      "[step: 14254] loss: 0.008072738535702229\n",
      "[step: 14255] loss: 0.007907218299806118\n",
      "[step: 14256] loss: 0.008130906149744987\n",
      "[step: 14257] loss: 0.008233617059886456\n",
      "[step: 14258] loss: 0.008028749376535416\n",
      "[step: 14259] loss: 0.007868734188377857\n",
      "[step: 14260] loss: 0.007957381196320057\n",
      "[step: 14261] loss: 0.007877378724515438\n",
      "[step: 14262] loss: 0.007957491092383862\n",
      "[step: 14263] loss: 0.007993083447217941\n",
      "[step: 14264] loss: 0.007890824228525162\n",
      "[step: 14265] loss: 0.0078421114012599\n",
      "[step: 14266] loss: 0.007851594127714634\n",
      "[step: 14267] loss: 0.007761887740343809\n",
      "[step: 14268] loss: 0.007769329939037561\n",
      "[step: 14269] loss: 0.007855108939111233\n",
      "[step: 14270] loss: 0.007716441061347723\n",
      "[step: 14271] loss: 0.007719059940427542\n",
      "[step: 14272] loss: 0.00763352494686842\n",
      "[step: 14273] loss: 0.007718908134847879\n",
      "[step: 14274] loss: 0.0075964792631566525\n",
      "[step: 14275] loss: 0.007625511381775141\n",
      "[step: 14276] loss: 0.007616526447236538\n",
      "[step: 14277] loss: 0.007640570402145386\n",
      "[step: 14278] loss: 0.007621992379426956\n",
      "[step: 14279] loss: 0.007623341865837574\n",
      "[step: 14280] loss: 0.00762195186689496\n",
      "[step: 14281] loss: 0.007584534119814634\n",
      "[step: 14282] loss: 0.007575967349112034\n",
      "[step: 14283] loss: 0.007545786909759045\n",
      "[step: 14284] loss: 0.007591179106384516\n",
      "[step: 14285] loss: 0.007577306125313044\n",
      "[step: 14286] loss: 0.007543480955064297\n",
      "[step: 14287] loss: 0.007615102455019951\n",
      "[step: 14288] loss: 0.007787459995597601\n",
      "[step: 14289] loss: 0.007583783008158207\n",
      "[step: 14290] loss: 0.0076843975111842155\n",
      "[step: 14291] loss: 0.007704610470682383\n",
      "[step: 14292] loss: 0.00754122668877244\n",
      "[step: 14293] loss: 0.007872192189097404\n",
      "[step: 14294] loss: 0.007965734228491783\n",
      "[step: 14295] loss: 0.007784538436681032\n",
      "[step: 14296] loss: 0.007856933400034904\n",
      "[step: 14297] loss: 0.007646381855010986\n",
      "[step: 14298] loss: 0.007833718322217464\n",
      "[step: 14299] loss: 0.0077833691611886024\n",
      "[step: 14300] loss: 0.00777282053604722\n",
      "[step: 14301] loss: 0.007803476415574551\n",
      "[step: 14302] loss: 0.007678119465708733\n",
      "[step: 14303] loss: 0.007734859827905893\n",
      "[step: 14304] loss: 0.0076798368245363235\n",
      "[step: 14305] loss: 0.007792440243065357\n",
      "[step: 14306] loss: 0.0076305256225168705\n",
      "[step: 14307] loss: 0.007672805339097977\n",
      "[step: 14308] loss: 0.007618166506290436\n",
      "[step: 14309] loss: 0.007761363405734301\n",
      "[step: 14310] loss: 0.007590254303067923\n",
      "[step: 14311] loss: 0.007642979267984629\n",
      "[step: 14312] loss: 0.0075880372896790504\n",
      "[step: 14313] loss: 0.007616650778800249\n",
      "[step: 14314] loss: 0.007546907756477594\n",
      "[step: 14315] loss: 0.0075690350495278835\n",
      "[step: 14316] loss: 0.007536082062870264\n",
      "[step: 14317] loss: 0.007570640649646521\n",
      "[step: 14318] loss: 0.007539027836173773\n",
      "[step: 14319] loss: 0.007504053879529238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 14320] loss: 0.007534365635365248\n",
      "[step: 14321] loss: 0.007527440786361694\n",
      "[step: 14322] loss: 0.007483448833227158\n",
      "[step: 14323] loss: 0.007494437042623758\n",
      "[step: 14324] loss: 0.007497268263250589\n",
      "[step: 14325] loss: 0.007487903349101543\n",
      "[step: 14326] loss: 0.007474355865269899\n",
      "[step: 14327] loss: 0.007470478769391775\n",
      "[step: 14328] loss: 0.007479148451238871\n",
      "[step: 14329] loss: 0.0074764457531273365\n",
      "[step: 14330] loss: 0.007474999409168959\n",
      "[step: 14331] loss: 0.00747115770354867\n",
      "[step: 14332] loss: 0.007487607654184103\n",
      "[step: 14333] loss: 0.007500648032873869\n",
      "[step: 14334] loss: 0.007587004452943802\n",
      "[step: 14335] loss: 0.007484355941414833\n",
      "[step: 14336] loss: 0.0074654873460531235\n",
      "[step: 14337] loss: 0.0075635360553860664\n",
      "[step: 14338] loss: 0.007580794859677553\n",
      "[step: 14339] loss: 0.007626886945217848\n",
      "[step: 14340] loss: 0.007524217013269663\n",
      "[step: 14341] loss: 0.00759149668738246\n",
      "[step: 14342] loss: 0.007633434142917395\n",
      "[step: 14343] loss: 0.007494396064430475\n",
      "[step: 14344] loss: 0.007543697487562895\n",
      "[step: 14345] loss: 0.007583907805383205\n",
      "[step: 14346] loss: 0.007567668333649635\n",
      "[step: 14347] loss: 0.007556578144431114\n",
      "[step: 14348] loss: 0.007502859458327293\n",
      "[step: 14349] loss: 0.0074817826971411705\n",
      "[step: 14350] loss: 0.007519923150539398\n",
      "[step: 14351] loss: 0.007480616681277752\n",
      "[step: 14352] loss: 0.007508996874094009\n",
      "[step: 14353] loss: 0.007450668141245842\n",
      "[step: 14354] loss: 0.007471706252545118\n",
      "[step: 14355] loss: 0.0074525377713143826\n",
      "[step: 14356] loss: 0.007462023291736841\n",
      "[step: 14357] loss: 0.007433637976646423\n",
      "[step: 14358] loss: 0.0074437460862100124\n",
      "[step: 14359] loss: 0.007438955828547478\n",
      "[step: 14360] loss: 0.007456343621015549\n",
      "[step: 14361] loss: 0.007493649609386921\n",
      "[step: 14362] loss: 0.007493399549275637\n",
      "[step: 14363] loss: 0.007441006600856781\n",
      "[step: 14364] loss: 0.00747451139613986\n",
      "[step: 14365] loss: 0.007881609722971916\n",
      "[step: 14366] loss: 0.00762088131159544\n",
      "[step: 14367] loss: 0.0075672646053135395\n",
      "[step: 14368] loss: 0.007956787012517452\n",
      "[step: 14369] loss: 0.007587550673633814\n",
      "[step: 14370] loss: 0.007512371521443129\n",
      "[step: 14371] loss: 0.007578075863420963\n",
      "[step: 14372] loss: 0.0074418834410607815\n",
      "[step: 14373] loss: 0.007539412472397089\n",
      "[step: 14374] loss: 0.007493009325116873\n",
      "[step: 14375] loss: 0.0074377809651196\n",
      "[step: 14376] loss: 0.007495004218071699\n",
      "[step: 14377] loss: 0.007439508568495512\n",
      "[step: 14378] loss: 0.0074731893837451935\n",
      "[step: 14379] loss: 0.007462695706635714\n",
      "[step: 14380] loss: 0.007412985898554325\n",
      "[step: 14381] loss: 0.007448705844581127\n",
      "[step: 14382] loss: 0.007429790683090687\n",
      "[step: 14383] loss: 0.007406642194837332\n",
      "[step: 14384] loss: 0.007437592837959528\n",
      "[step: 14385] loss: 0.007441034074872732\n",
      "[step: 14386] loss: 0.0074071078561246395\n",
      "[step: 14387] loss: 0.007401203271001577\n",
      "[step: 14388] loss: 0.0074302623979747295\n",
      "[step: 14389] loss: 0.0075462376698851585\n",
      "[step: 14390] loss: 0.007472056429833174\n",
      "[step: 14391] loss: 0.007439495529979467\n",
      "[step: 14392] loss: 0.007391293998807669\n",
      "[step: 14393] loss: 0.0075501855462789536\n",
      "[step: 14394] loss: 0.008332357741892338\n",
      "[step: 14395] loss: 0.007903346791863441\n",
      "[step: 14396] loss: 0.008193762972950935\n",
      "[step: 14397] loss: 0.007918685674667358\n",
      "[step: 14398] loss: 0.007931309752166271\n",
      "[step: 14399] loss: 0.008038739673793316\n",
      "[step: 14400] loss: 0.007876498624682426\n",
      "[step: 14401] loss: 0.008026721887290478\n",
      "[step: 14402] loss: 0.007565251551568508\n",
      "[step: 14403] loss: 0.008113226853311062\n",
      "[step: 14404] loss: 0.0075650508515536785\n",
      "[step: 14405] loss: 0.008101392537355423\n",
      "[step: 14406] loss: 0.00763131445273757\n",
      "[step: 14407] loss: 0.007953894324600697\n",
      "[step: 14408] loss: 0.0075190081261098385\n",
      "[step: 14409] loss: 0.007799631915986538\n",
      "[step: 14410] loss: 0.007561023812741041\n",
      "[step: 14411] loss: 0.0077272881753742695\n",
      "[step: 14412] loss: 0.007524969056248665\n",
      "[step: 14413] loss: 0.007696052081882954\n",
      "[step: 14414] loss: 0.007509664632380009\n",
      "[step: 14415] loss: 0.007567208260297775\n",
      "[step: 14416] loss: 0.007498845458030701\n",
      "[step: 14417] loss: 0.0075845555402338505\n",
      "[step: 14418] loss: 0.007481327746063471\n",
      "[step: 14419] loss: 0.007477185223251581\n",
      "[step: 14420] loss: 0.007488928735256195\n",
      "[step: 14421] loss: 0.007539695128798485\n",
      "[step: 14422] loss: 0.007830665446817875\n",
      "[step: 14423] loss: 0.0077083520591259\n",
      "[step: 14424] loss: 0.007523985113948584\n",
      "[step: 14425] loss: 0.008303346112370491\n",
      "[step: 14426] loss: 0.007584249600768089\n",
      "[step: 14427] loss: 0.007816369645297527\n",
      "[step: 14428] loss: 0.0076707759872078896\n",
      "[step: 14429] loss: 0.007941843010485172\n",
      "[step: 14430] loss: 0.007652267348021269\n",
      "[step: 14431] loss: 0.007783969398587942\n",
      "[step: 14432] loss: 0.007661331444978714\n",
      "[step: 14433] loss: 0.0077171288430690765\n",
      "[step: 14434] loss: 0.0076512536033988\n",
      "[step: 14435] loss: 0.007550491020083427\n",
      "[step: 14436] loss: 0.007652582135051489\n",
      "[step: 14437] loss: 0.00749496603384614\n",
      "[step: 14438] loss: 0.007598411291837692\n",
      "[step: 14439] loss: 0.007464214228093624\n",
      "[step: 14440] loss: 0.007614596746861935\n",
      "[step: 14441] loss: 0.007454205770045519\n",
      "[step: 14442] loss: 0.007503551431000233\n",
      "[step: 14443] loss: 0.0074323141016066074\n",
      "[step: 14444] loss: 0.007487394381314516\n",
      "[step: 14445] loss: 0.007486427668482065\n",
      "[step: 14446] loss: 0.007451239507645369\n",
      "[step: 14447] loss: 0.007401944603770971\n",
      "[step: 14448] loss: 0.0074459463357925415\n",
      "[step: 14449] loss: 0.007518318481743336\n",
      "[step: 14450] loss: 0.008034221827983856\n",
      "[step: 14451] loss: 0.0077482713386416435\n",
      "[step: 14452] loss: 0.007890687324106693\n",
      "[step: 14453] loss: 0.00763738015666604\n",
      "[step: 14454] loss: 0.007612857036292553\n",
      "[step: 14455] loss: 0.0077179595828056335\n",
      "[step: 14456] loss: 0.007508956827223301\n",
      "[step: 14457] loss: 0.007731730118393898\n",
      "[step: 14458] loss: 0.007423347793519497\n",
      "[step: 14459] loss: 0.007709454745054245\n",
      "[step: 14460] loss: 0.007474301382899284\n",
      "[step: 14461] loss: 0.007546367589384317\n",
      "[step: 14462] loss: 0.007512065581977367\n",
      "[step: 14463] loss: 0.007506662514060736\n",
      "[step: 14464] loss: 0.007430747617036104\n",
      "[step: 14465] loss: 0.007419317029416561\n",
      "[step: 14466] loss: 0.007451607845723629\n",
      "[step: 14467] loss: 0.0074284193105995655\n",
      "[step: 14468] loss: 0.007377325091511011\n",
      "[step: 14469] loss: 0.007409603334963322\n",
      "[step: 14470] loss: 0.007392471190541983\n",
      "[step: 14471] loss: 0.007360855117440224\n",
      "[step: 14472] loss: 0.007374430075287819\n",
      "[step: 14473] loss: 0.0073880767449736595\n",
      "[step: 14474] loss: 0.00736809941008687\n",
      "[step: 14475] loss: 0.007344928104430437\n",
      "[step: 14476] loss: 0.0074400524608790874\n",
      "[step: 14477] loss: 0.007651449181139469\n",
      "[step: 14478] loss: 0.008768334984779358\n",
      "[step: 14479] loss: 0.008141870610415936\n",
      "[step: 14480] loss: 0.008709282614290714\n",
      "[step: 14481] loss: 0.008510014973580837\n",
      "[step: 14482] loss: 0.008516251109540462\n",
      "[step: 14483] loss: 0.008058431558310986\n",
      "[step: 14484] loss: 0.008156126365065575\n",
      "[step: 14485] loss: 0.00801111850887537\n",
      "[step: 14486] loss: 0.008134000934660435\n",
      "[step: 14487] loss: 0.007953948341310024\n",
      "[step: 14488] loss: 0.007808194495737553\n",
      "[step: 14489] loss: 0.008012739941477776\n",
      "[step: 14490] loss: 0.007869193330407143\n",
      "[step: 14491] loss: 0.007892479188740253\n",
      "[step: 14492] loss: 0.007731418125331402\n",
      "[step: 14493] loss: 0.007813267409801483\n",
      "[step: 14494] loss: 0.007670809980481863\n",
      "[step: 14495] loss: 0.0076511879451572895\n",
      "[step: 14496] loss: 0.007648542057722807\n",
      "[step: 14497] loss: 0.0076442318968474865\n",
      "[step: 14498] loss: 0.007572806440293789\n",
      "[step: 14499] loss: 0.0075546447187662125\n",
      "[step: 14500] loss: 0.00752879586070776\n",
      "[step: 14501] loss: 0.007571435067802668\n",
      "[step: 14502] loss: 0.007494024466723204\n",
      "[step: 14503] loss: 0.007450350560247898\n",
      "[step: 14504] loss: 0.007464909926056862\n",
      "[step: 14505] loss: 0.007468258496373892\n",
      "[step: 14506] loss: 0.007418432738631964\n",
      "[step: 14507] loss: 0.007462426088750362\n",
      "[step: 14508] loss: 0.007455877494066954\n",
      "[step: 14509] loss: 0.007399189285933971\n",
      "[step: 14510] loss: 0.007411981001496315\n",
      "[step: 14511] loss: 0.00742023391649127\n",
      "[step: 14512] loss: 0.007392286788672209\n",
      "[step: 14513] loss: 0.0074099465273320675\n",
      "[step: 14514] loss: 0.007394320331513882\n",
      "[step: 14515] loss: 0.00738500664010644\n",
      "[step: 14516] loss: 0.007391592487692833\n",
      "[step: 14517] loss: 0.0073929475620388985\n",
      "[step: 14518] loss: 0.00736131239682436\n",
      "[step: 14519] loss: 0.007345797494053841\n",
      "[step: 14520] loss: 0.007339909672737122\n",
      "[step: 14521] loss: 0.007332539185881615\n",
      "[step: 14522] loss: 0.007354513742029667\n",
      "[step: 14523] loss: 0.0073482571169734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 14524] loss: 0.007373972330242395\n",
      "[step: 14525] loss: 0.007340152282267809\n",
      "[step: 14526] loss: 0.007349855732172728\n",
      "[step: 14527] loss: 0.007658965419977903\n",
      "[step: 14528] loss: 0.00748223764821887\n",
      "[step: 14529] loss: 0.0073972707614302635\n",
      "[step: 14530] loss: 0.0073731523007154465\n",
      "[step: 14531] loss: 0.00753794377669692\n",
      "[step: 14532] loss: 0.008128717541694641\n",
      "[step: 14533] loss: 0.007856243290007114\n",
      "[step: 14534] loss: 0.008001478388905525\n",
      "[step: 14535] loss: 0.00791015662252903\n",
      "[step: 14536] loss: 0.007892505265772343\n",
      "[step: 14537] loss: 0.007640729658305645\n",
      "[step: 14538] loss: 0.007454388774931431\n",
      "[step: 14539] loss: 0.007761238608509302\n",
      "[step: 14540] loss: 0.007615706883370876\n",
      "[step: 14541] loss: 0.007462248206138611\n",
      "[step: 14542] loss: 0.007475346326828003\n",
      "[step: 14543] loss: 0.007572040427476168\n",
      "[step: 14544] loss: 0.00748709449544549\n",
      "[step: 14545] loss: 0.007423704024404287\n",
      "[step: 14546] loss: 0.007436144631356001\n",
      "[step: 14547] loss: 0.007460687309503555\n",
      "[step: 14548] loss: 0.007434524595737457\n",
      "[step: 14549] loss: 0.0077539775520563126\n",
      "[step: 14550] loss: 0.007468775846064091\n",
      "[step: 14551] loss: 0.007476138882339001\n",
      "[step: 14552] loss: 0.007768581155687571\n",
      "[step: 14553] loss: 0.007432594895362854\n",
      "[step: 14554] loss: 0.007492362055927515\n",
      "[step: 14555] loss: 0.00803172867745161\n",
      "[step: 14556] loss: 0.007541988510638475\n",
      "[step: 14557] loss: 0.007893934845924377\n",
      "[step: 14558] loss: 0.007464321330189705\n",
      "[step: 14559] loss: 0.007757113315165043\n",
      "[step: 14560] loss: 0.007405009586364031\n",
      "[step: 14561] loss: 0.007583610713481903\n",
      "[step: 14562] loss: 0.007479359861463308\n",
      "[step: 14563] loss: 0.007481501903384924\n",
      "[step: 14564] loss: 0.00754696037620306\n",
      "[step: 14565] loss: 0.0074026919901371\n",
      "[step: 14566] loss: 0.007507018279284239\n",
      "[step: 14567] loss: 0.0073834555223584175\n",
      "[step: 14568] loss: 0.007424143608659506\n",
      "[step: 14569] loss: 0.007361069321632385\n",
      "[step: 14570] loss: 0.007378187961876392\n",
      "[step: 14571] loss: 0.007333590183407068\n",
      "[step: 14572] loss: 0.007327059283852577\n",
      "[step: 14573] loss: 0.007337643764913082\n",
      "[step: 14574] loss: 0.007337986025959253\n",
      "[step: 14575] loss: 0.007281205151230097\n",
      "[step: 14576] loss: 0.007328829728066921\n",
      "[step: 14577] loss: 0.007387781050056219\n",
      "[step: 14578] loss: 0.0074701751582324505\n",
      "[step: 14579] loss: 0.007385759148746729\n",
      "[step: 14580] loss: 0.007368311285972595\n",
      "[step: 14581] loss: 0.007528053596615791\n",
      "[step: 14582] loss: 0.007366788107901812\n",
      "[step: 14583] loss: 0.007351927924901247\n",
      "[step: 14584] loss: 0.0074140047654509544\n",
      "[step: 14585] loss: 0.007273761089891195\n",
      "[step: 14586] loss: 0.0073870024643838406\n",
      "[step: 14587] loss: 0.0074553107842803\n",
      "[step: 14588] loss: 0.007316256873309612\n",
      "[step: 14589] loss: 0.007383722346276045\n",
      "[step: 14590] loss: 0.007333903107792139\n",
      "[step: 14591] loss: 0.007318098098039627\n",
      "[step: 14592] loss: 0.007356471382081509\n",
      "[step: 14593] loss: 0.007290359120815992\n",
      "[step: 14594] loss: 0.007330702617764473\n",
      "[step: 14595] loss: 0.007291481830179691\n",
      "[step: 14596] loss: 0.007285826839506626\n",
      "[step: 14597] loss: 0.007285944651812315\n",
      "[step: 14598] loss: 0.0072486018761992455\n",
      "[step: 14599] loss: 0.007252860348671675\n",
      "[step: 14600] loss: 0.007231920957565308\n",
      "[step: 14601] loss: 0.007240631151944399\n",
      "[step: 14602] loss: 0.007221303880214691\n",
      "[step: 14603] loss: 0.0072943065315485\n",
      "[step: 14604] loss: 0.007381408009678125\n",
      "[step: 14605] loss: 0.007866987958550453\n",
      "[step: 14606] loss: 0.007634669542312622\n",
      "[step: 14607] loss: 0.007439940236508846\n",
      "[step: 14608] loss: 0.007508701644837856\n",
      "[step: 14609] loss: 0.007501383777707815\n",
      "[step: 14610] loss: 0.007406486198306084\n",
      "[step: 14611] loss: 0.007358515169471502\n",
      "[step: 14612] loss: 0.007441957015544176\n",
      "[step: 14613] loss: 0.007417525164783001\n",
      "[step: 14614] loss: 0.0073503670282661915\n",
      "[step: 14615] loss: 0.0073846485465765\n",
      "[step: 14616] loss: 0.007332175504416227\n",
      "[step: 14617] loss: 0.007366283796727657\n",
      "[step: 14618] loss: 0.007336883340030909\n",
      "[step: 14619] loss: 0.007280013989657164\n",
      "[step: 14620] loss: 0.007285339292138815\n",
      "[step: 14621] loss: 0.007270666770637035\n",
      "[step: 14622] loss: 0.00724452082067728\n",
      "[step: 14623] loss: 0.007253237068653107\n",
      "[step: 14624] loss: 0.007305060047656298\n",
      "[step: 14625] loss: 0.007295530755072832\n",
      "[step: 14626] loss: 0.007221648003906012\n",
      "[step: 14627] loss: 0.007218510378152132\n",
      "[step: 14628] loss: 0.007355052512139082\n",
      "[step: 14629] loss: 0.008200017735362053\n",
      "[step: 14630] loss: 0.007826236076653004\n",
      "[step: 14631] loss: 0.007834416814148426\n",
      "[step: 14632] loss: 0.007817830890417099\n",
      "[step: 14633] loss: 0.007635876070708036\n",
      "[step: 14634] loss: 0.007644552271813154\n",
      "[step: 14635] loss: 0.007503180298954248\n",
      "[step: 14636] loss: 0.007678308989852667\n",
      "[step: 14637] loss: 0.007522102445363998\n",
      "[step: 14638] loss: 0.007427544798702002\n",
      "[step: 14639] loss: 0.0074742864817380905\n",
      "[step: 14640] loss: 0.0074167740531265736\n",
      "[step: 14641] loss: 0.007462034467607737\n",
      "[step: 14642] loss: 0.00826044101268053\n",
      "[step: 14643] loss: 0.007983613759279251\n",
      "[step: 14644] loss: 0.007918384857475758\n",
      "[step: 14645] loss: 0.007905052043497562\n",
      "[step: 14646] loss: 0.008080944418907166\n",
      "[step: 14647] loss: 0.0075190989300608635\n",
      "[step: 14648] loss: 0.007936709560453892\n",
      "[step: 14649] loss: 0.007597958203405142\n",
      "[step: 14650] loss: 0.00780102051794529\n",
      "[step: 14651] loss: 0.007687056437134743\n",
      "[step: 14652] loss: 0.007826450280845165\n",
      "[step: 14653] loss: 0.007619291543960571\n",
      "[step: 14654] loss: 0.007494328543543816\n",
      "[step: 14655] loss: 0.007593455258756876\n",
      "[step: 14656] loss: 0.007477729115635157\n",
      "[step: 14657] loss: 0.007395514752715826\n",
      "[step: 14658] loss: 0.007490521762520075\n",
      "[step: 14659] loss: 0.007450491655617952\n",
      "[step: 14660] loss: 0.007436166517436504\n",
      "[step: 14661] loss: 0.00736951595172286\n",
      "[step: 14662] loss: 0.007320767734199762\n",
      "[step: 14663] loss: 0.007299819029867649\n",
      "[step: 14664] loss: 0.007328784093260765\n",
      "[step: 14665] loss: 0.007281428202986717\n",
      "[step: 14666] loss: 0.007287095300853252\n",
      "[step: 14667] loss: 0.007262962870299816\n",
      "[step: 14668] loss: 0.00727185420691967\n",
      "[step: 14669] loss: 0.007229289505630732\n",
      "[step: 14670] loss: 0.007233067881315947\n",
      "[step: 14671] loss: 0.007251303177326918\n",
      "[step: 14672] loss: 0.007223254535347223\n",
      "[step: 14673] loss: 0.007189392577856779\n",
      "[step: 14674] loss: 0.007231168448925018\n",
      "[step: 14675] loss: 0.0072453985922038555\n",
      "[step: 14676] loss: 0.00721282884478569\n",
      "[step: 14677] loss: 0.007174924947321415\n",
      "[step: 14678] loss: 0.007208411116153002\n",
      "[step: 14679] loss: 0.007235333323478699\n",
      "[step: 14680] loss: 0.007649259641766548\n",
      "[step: 14681] loss: 0.007587481290102005\n",
      "[step: 14682] loss: 0.007418293505907059\n",
      "[step: 14683] loss: 0.00745911942794919\n",
      "[step: 14684] loss: 0.007473262958228588\n",
      "[step: 14685] loss: 0.007363146170973778\n",
      "[step: 14686] loss: 0.00733972666785121\n",
      "[step: 14687] loss: 0.007364186923950911\n",
      "[step: 14688] loss: 0.007428715471178293\n",
      "[step: 14689] loss: 0.007266917731612921\n",
      "[step: 14690] loss: 0.007301348261535168\n",
      "[step: 14691] loss: 0.007314402610063553\n",
      "[step: 14692] loss: 0.007307973224669695\n",
      "[step: 14693] loss: 0.007264729589223862\n",
      "[step: 14694] loss: 0.0072303349152207375\n",
      "[step: 14695] loss: 0.007281127385795116\n",
      "[step: 14696] loss: 0.007215238641947508\n",
      "[step: 14697] loss: 0.007218418642878532\n",
      "[step: 14698] loss: 0.007169656455516815\n",
      "[step: 14699] loss: 0.007188182324171066\n",
      "[step: 14700] loss: 0.007173085119575262\n",
      "[step: 14701] loss: 0.007155518513172865\n",
      "[step: 14702] loss: 0.0071630277670919895\n",
      "[step: 14703] loss: 0.007139483001083136\n",
      "[step: 14704] loss: 0.0071338810957968235\n",
      "[step: 14705] loss: 0.007129288744181395\n",
      "[step: 14706] loss: 0.007290932349860668\n",
      "[step: 14707] loss: 0.00897428672760725\n",
      "[step: 14708] loss: 0.007970872335135937\n",
      "[step: 14709] loss: 0.008796541020274162\n",
      "[step: 14710] loss: 0.008622485212981701\n",
      "[step: 14711] loss: 0.008383437059819698\n",
      "[step: 14712] loss: 0.00810093805193901\n",
      "[step: 14713] loss: 0.007850151509046555\n",
      "[step: 14714] loss: 0.008410614915192127\n",
      "[step: 14715] loss: 0.008185051381587982\n",
      "[step: 14716] loss: 0.007719775661826134\n",
      "[step: 14717] loss: 0.008081212639808655\n",
      "[step: 14718] loss: 0.007988723926246166\n",
      "[step: 14719] loss: 0.0080830417573452\n",
      "[step: 14720] loss: 0.00767779303714633\n",
      "[step: 14721] loss: 0.00808549765497446\n",
      "[step: 14722] loss: 0.0074956901371479034\n",
      "[step: 14723] loss: 0.008061030879616737\n",
      "[step: 14724] loss: 0.00827863160520792\n",
      "[step: 14725] loss: 0.008672977797687054\n",
      "[step: 14726] loss: 0.00860435701906681\n",
      "[step: 14727] loss: 0.00903996266424656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 14728] loss: 0.009076797403395176\n",
      "[step: 14729] loss: 0.008859088644385338\n",
      "[step: 14730] loss: 0.008645620197057724\n",
      "[step: 14731] loss: 0.008754530921578407\n",
      "[step: 14732] loss: 0.00881891418248415\n",
      "[step: 14733] loss: 0.009070603176951408\n",
      "[step: 14734] loss: 0.008365110494196415\n",
      "[step: 14735] loss: 0.008311604149639606\n",
      "[step: 14736] loss: 0.009014952927827835\n",
      "[step: 14737] loss: 0.008642935194075108\n",
      "[step: 14738] loss: 0.00873705092817545\n",
      "[step: 14739] loss: 0.008307937532663345\n",
      "[step: 14740] loss: 0.008420445956289768\n",
      "[step: 14741] loss: 0.00816323235630989\n",
      "[step: 14742] loss: 0.007971562445163727\n",
      "[step: 14743] loss: 0.008504273369908333\n",
      "[step: 14744] loss: 0.0078097679652273655\n",
      "[step: 14745] loss: 0.008176123723387718\n",
      "[step: 14746] loss: 0.007674449123442173\n",
      "[step: 14747] loss: 0.00810944102704525\n",
      "[step: 14748] loss: 0.007680743467062712\n",
      "[step: 14749] loss: 0.00789076741784811\n",
      "[step: 14750] loss: 0.007689313031733036\n",
      "[step: 14751] loss: 0.007749826647341251\n",
      "[step: 14752] loss: 0.007707709912210703\n",
      "[step: 14753] loss: 0.007707918994128704\n",
      "[step: 14754] loss: 0.007563738152384758\n",
      "[step: 14755] loss: 0.007677075453102589\n",
      "[step: 14756] loss: 0.007512785494327545\n",
      "[step: 14757] loss: 0.007656787987798452\n",
      "[step: 14758] loss: 0.00746602239087224\n",
      "[step: 14759] loss: 0.007647043559700251\n",
      "[step: 14760] loss: 0.007495115976780653\n",
      "[step: 14761] loss: 0.007629907224327326\n",
      "[step: 14762] loss: 0.007458427920937538\n",
      "[step: 14763] loss: 0.007603362202644348\n",
      "[step: 14764] loss: 0.007452089339494705\n",
      "[step: 14765] loss: 0.007548193447291851\n",
      "[step: 14766] loss: 0.0074154045432806015\n",
      "[step: 14767] loss: 0.007525495253503323\n",
      "[step: 14768] loss: 0.007415125146508217\n",
      "[step: 14769] loss: 0.007490653079003096\n",
      "[step: 14770] loss: 0.007412749342620373\n",
      "[step: 14771] loss: 0.0074417744763195515\n",
      "[step: 14772] loss: 0.007405376993119717\n",
      "[step: 14773] loss: 0.007408491801470518\n",
      "[step: 14774] loss: 0.007410411257296801\n",
      "[step: 14775] loss: 0.007363390177488327\n",
      "[step: 14776] loss: 0.007400459609925747\n",
      "[step: 14777] loss: 0.007352000568062067\n",
      "[step: 14778] loss: 0.007377299480140209\n",
      "[step: 14779] loss: 0.007327421102672815\n",
      "[step: 14780] loss: 0.007351432461291552\n",
      "[step: 14781] loss: 0.007310430519282818\n",
      "[step: 14782] loss: 0.007316409144550562\n",
      "[step: 14783] loss: 0.007288757245987654\n",
      "[step: 14784] loss: 0.007285399362444878\n",
      "[step: 14785] loss: 0.007261801511049271\n",
      "[step: 14786] loss: 0.007254681084305048\n",
      "[step: 14787] loss: 0.007238768972456455\n",
      "[step: 14788] loss: 0.007226552348583937\n",
      "[step: 14789] loss: 0.007213155273348093\n",
      "[step: 14790] loss: 0.0072057293727993965\n",
      "[step: 14791] loss: 0.00719052366912365\n",
      "[step: 14792] loss: 0.007186518982052803\n",
      "[step: 14793] loss: 0.007171873468905687\n",
      "[step: 14794] loss: 0.0071724881418049335\n",
      "[step: 14795] loss: 0.00715959258377552\n",
      "[step: 14796] loss: 0.0071610393933951855\n",
      "[step: 14797] loss: 0.007150118239223957\n",
      "[step: 14798] loss: 0.007150609511882067\n",
      "[step: 14799] loss: 0.007140785921365023\n",
      "[step: 14800] loss: 0.007141354028135538\n",
      "[step: 14801] loss: 0.007134054321795702\n",
      "[step: 14802] loss: 0.007130754180252552\n",
      "[step: 14803] loss: 0.007125158794224262\n",
      "[step: 14804] loss: 0.007120131980627775\n",
      "[step: 14805] loss: 0.007117364555597305\n",
      "[step: 14806] loss: 0.007111556828022003\n",
      "[step: 14807] loss: 0.0071120960637927055\n",
      "[step: 14808] loss: 0.0071104890666902065\n",
      "[step: 14809] loss: 0.007138650398701429\n",
      "[step: 14810] loss: 0.007167431525886059\n",
      "[step: 14811] loss: 0.007393340114504099\n",
      "[step: 14812] loss: 0.007221377454698086\n",
      "[step: 14813] loss: 0.0072715999558568\n",
      "[step: 14814] loss: 0.007627810817211866\n",
      "[step: 14815] loss: 0.007174857892096043\n",
      "[step: 14816] loss: 0.0074091339483857155\n",
      "[step: 14817] loss: 0.0071969591081142426\n",
      "[step: 14818] loss: 0.007285722065716982\n",
      "[step: 14819] loss: 0.007282328326255083\n",
      "[step: 14820] loss: 0.007235505152493715\n",
      "[step: 14821] loss: 0.0072741019539535046\n",
      "[step: 14822] loss: 0.007218269165605307\n",
      "[step: 14823] loss: 0.007189043797552586\n",
      "[step: 14824] loss: 0.007201962638646364\n",
      "[step: 14825] loss: 0.0071426755748689175\n",
      "[step: 14826] loss: 0.007158588618040085\n",
      "[step: 14827] loss: 0.007154202554374933\n",
      "[step: 14828] loss: 0.007087533362209797\n",
      "[step: 14829] loss: 0.0071854665875434875\n",
      "[step: 14830] loss: 0.0071134380996227264\n",
      "[step: 14831] loss: 0.0070854839868843555\n",
      "[step: 14832] loss: 0.0071413516998291016\n",
      "[step: 14833] loss: 0.007077005226165056\n",
      "[step: 14834] loss: 0.0070971595123410225\n",
      "[step: 14835] loss: 0.007065011188387871\n",
      "[step: 14836] loss: 0.0070527223870158195\n",
      "[step: 14837] loss: 0.0070810988545417786\n",
      "[step: 14838] loss: 0.007138445507735014\n",
      "[step: 14839] loss: 0.00716393394395709\n",
      "[step: 14840] loss: 0.007369527127593756\n",
      "[step: 14841] loss: 0.007152465637773275\n",
      "[step: 14842] loss: 0.0072254217229783535\n",
      "[step: 14843] loss: 0.008057118393480778\n",
      "[step: 14844] loss: 0.007440424524247646\n",
      "[step: 14845] loss: 0.00787020567804575\n",
      "[step: 14846] loss: 0.007435343228280544\n",
      "[step: 14847] loss: 0.007630584761500359\n",
      "[step: 14848] loss: 0.007253809366375208\n",
      "[step: 14849] loss: 0.007464627269655466\n",
      "[step: 14850] loss: 0.0073429918847978115\n",
      "[step: 14851] loss: 0.007142724934965372\n",
      "[step: 14852] loss: 0.00727081298828125\n",
      "[step: 14853] loss: 0.007218633312731981\n",
      "[step: 14854] loss: 0.007267724722623825\n",
      "[step: 14855] loss: 0.007201803382486105\n",
      "[step: 14856] loss: 0.007127157878130674\n",
      "[step: 14857] loss: 0.007114411797374487\n",
      "[step: 14858] loss: 0.0071425661444664\n",
      "[step: 14859] loss: 0.007193803787231445\n",
      "[step: 14860] loss: 0.007081403397023678\n",
      "[step: 14861] loss: 0.007094825152307749\n",
      "[step: 14862] loss: 0.007126814220100641\n",
      "[step: 14863] loss: 0.007096500135958195\n",
      "[step: 14864] loss: 0.007164624985307455\n",
      "[step: 14865] loss: 0.007039961870759726\n",
      "[step: 14866] loss: 0.007103643845766783\n",
      "[step: 14867] loss: 0.0071126241236925125\n",
      "[step: 14868] loss: 0.007150961551815271\n",
      "[step: 14869] loss: 0.0070465742610394955\n",
      "[step: 14870] loss: 0.007137311156839132\n",
      "[step: 14871] loss: 0.007071830797940493\n",
      "[step: 14872] loss: 0.0071175494231283665\n",
      "[step: 14873] loss: 0.007093349937349558\n",
      "[step: 14874] loss: 0.007067469414323568\n",
      "[step: 14875] loss: 0.0070936353877186775\n",
      "[step: 14876] loss: 0.007046659477055073\n",
      "[step: 14877] loss: 0.0071322242729365826\n",
      "[step: 14878] loss: 0.0070554460398852825\n",
      "[step: 14879] loss: 0.007067046128213406\n",
      "[step: 14880] loss: 0.0072260149754583836\n",
      "[step: 14881] loss: 0.007358272559940815\n",
      "[step: 14882] loss: 0.007373139262199402\n",
      "[step: 14883] loss: 0.007315768860280514\n",
      "[step: 14884] loss: 0.007335246540606022\n",
      "[step: 14885] loss: 0.007198503706604242\n",
      "[step: 14886] loss: 0.007283322513103485\n",
      "[step: 14887] loss: 0.0071668438613414764\n",
      "[step: 14888] loss: 0.007302474230527878\n",
      "[step: 14889] loss: 0.007408191915601492\n",
      "[step: 14890] loss: 0.00720449723303318\n",
      "[step: 14891] loss: 0.007467343471944332\n",
      "[step: 14892] loss: 0.007094061002135277\n",
      "[step: 14893] loss: 0.007272420916706324\n",
      "[step: 14894] loss: 0.007257610093802214\n",
      "[step: 14895] loss: 0.0075221010483801365\n",
      "[step: 14896] loss: 0.0079170698300004\n",
      "[step: 14897] loss: 0.008225221186876297\n",
      "[step: 14898] loss: 0.00860064197331667\n",
      "[step: 14899] loss: 0.008342032320797443\n",
      "[step: 14900] loss: 0.007922425866127014\n",
      "[step: 14901] loss: 0.008541847579181194\n",
      "[step: 14902] loss: 0.007823443971574306\n",
      "[step: 14903] loss: 0.00813155248761177\n",
      "[step: 14904] loss: 0.008008848875761032\n",
      "[step: 14905] loss: 0.007671497296541929\n",
      "[step: 14906] loss: 0.008282012306153774\n",
      "[step: 14907] loss: 0.007651569787412882\n",
      "[step: 14908] loss: 0.007822924293577671\n",
      "[step: 14909] loss: 0.007989021949470043\n",
      "[step: 14910] loss: 0.007876891642808914\n",
      "[step: 14911] loss: 0.007639682851731777\n",
      "[step: 14912] loss: 0.007669983431696892\n",
      "[step: 14913] loss: 0.00763784209266305\n",
      "[step: 14914] loss: 0.007579562719911337\n",
      "[step: 14915] loss: 0.0075441082008183\n",
      "[step: 14916] loss: 0.007514073513448238\n",
      "[step: 14917] loss: 0.00746106868609786\n",
      "[step: 14918] loss: 0.0075447093695402145\n",
      "[step: 14919] loss: 0.007422045338898897\n",
      "[step: 14920] loss: 0.007434283383190632\n",
      "[step: 14921] loss: 0.007383197080343962\n",
      "[step: 14922] loss: 0.007336266338825226\n",
      "[step: 14923] loss: 0.007379861548542976\n",
      "[step: 14924] loss: 0.007366245612502098\n",
      "[step: 14925] loss: 0.0073687294498085976\n",
      "[step: 14926] loss: 0.007334698457270861\n",
      "[step: 14927] loss: 0.007342030759900808\n",
      "[step: 14928] loss: 0.007287956308573484\n",
      "[step: 14929] loss: 0.007313565816730261\n",
      "[step: 14930] loss: 0.0072932057082653046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 14931] loss: 0.007262994535267353\n",
      "[step: 14932] loss: 0.0072608646005392075\n",
      "[step: 14933] loss: 0.007249508518725634\n",
      "[step: 14934] loss: 0.007256097160279751\n",
      "[step: 14935] loss: 0.007240755949169397\n",
      "[step: 14936] loss: 0.007236402481794357\n",
      "[step: 14937] loss: 0.007223900407552719\n",
      "[step: 14938] loss: 0.007213634438812733\n",
      "[step: 14939] loss: 0.007203179411590099\n",
      "[step: 14940] loss: 0.00720270024612546\n",
      "[step: 14941] loss: 0.007193306926637888\n",
      "[step: 14942] loss: 0.007191064767539501\n",
      "[step: 14943] loss: 0.007172356825321913\n",
      "[step: 14944] loss: 0.007178576663136482\n",
      "[step: 14945] loss: 0.0071597653441131115\n",
      "[step: 14946] loss: 0.00716266268864274\n",
      "[step: 14947] loss: 0.007145407609641552\n",
      "[step: 14948] loss: 0.007146363612264395\n",
      "[step: 14949] loss: 0.007131568156182766\n",
      "[step: 14950] loss: 0.0071270400658249855\n",
      "[step: 14951] loss: 0.00711976271122694\n",
      "[step: 14952] loss: 0.0071124001406133175\n",
      "[step: 14953] loss: 0.007098587229847908\n",
      "[step: 14954] loss: 0.007093112450093031\n",
      "[step: 14955] loss: 0.007078643422573805\n",
      "[step: 14956] loss: 0.007073241751641035\n",
      "[step: 14957] loss: 0.007059144787490368\n",
      "[step: 14958] loss: 0.0070504010654985905\n",
      "[step: 14959] loss: 0.0070383911952376366\n",
      "[step: 14960] loss: 0.0070269457064569\n",
      "[step: 14961] loss: 0.007014727219939232\n",
      "[step: 14962] loss: 0.007006383966654539\n",
      "[step: 14963] loss: 0.0069967228919267654\n",
      "[step: 14964] loss: 0.006985724903643131\n",
      "[step: 14965] loss: 0.006977565586566925\n",
      "[step: 14966] loss: 0.0069703771732747555\n",
      "[step: 14967] loss: 0.0069633168168365955\n",
      "[step: 14968] loss: 0.006956862285733223\n",
      "[step: 14969] loss: 0.006950883660465479\n",
      "[step: 14970] loss: 0.006945548113435507\n",
      "[step: 14971] loss: 0.0069400290958583355\n",
      "[step: 14972] loss: 0.00693369098007679\n",
      "[step: 14973] loss: 0.006927693262696266\n",
      "[step: 14974] loss: 0.006922333501279354\n",
      "[step: 14975] loss: 0.006916812155395746\n",
      "[step: 14976] loss: 0.006911270320415497\n",
      "[step: 14977] loss: 0.006906604859977961\n",
      "[step: 14978] loss: 0.006903508212417364\n",
      "[step: 14979] loss: 0.006906759459525347\n",
      "[step: 14980] loss: 0.00692779291421175\n",
      "[step: 14981] loss: 0.007054393645375967\n",
      "[step: 14982] loss: 0.007151867263019085\n",
      "[step: 14983] loss: 0.007002610247582197\n",
      "[step: 14984] loss: 0.00689755380153656\n",
      "[step: 14985] loss: 0.007056096103042364\n",
      "[step: 14986] loss: 0.007238519378006458\n",
      "[step: 14987] loss: 0.007490306627005339\n",
      "[step: 14988] loss: 0.0070687164552509785\n",
      "[step: 14989] loss: 0.00757045391947031\n",
      "[step: 14990] loss: 0.007031695451587439\n",
      "[step: 14991] loss: 0.007380244787782431\n",
      "[step: 14992] loss: 0.007331838831305504\n",
      "[step: 14993] loss: 0.0071610137820243835\n",
      "[step: 14994] loss: 0.007396715693175793\n",
      "[step: 14995] loss: 0.006947911344468594\n",
      "[step: 14996] loss: 0.007409858517348766\n",
      "[step: 14997] loss: 0.008224180899560452\n",
      "[step: 14998] loss: 0.0075220814906060696\n",
      "[step: 14999] loss: 0.008526217192411423\n",
      "[step: 15000] loss: 0.008005711250007153\n",
      "[step: 15001] loss: 0.008325578644871712\n",
      "[step: 15002] loss: 0.007995669730007648\n",
      "[step: 15003] loss: 0.009079402312636375\n",
      "[step: 15004] loss: 0.009270983748137951\n",
      "[step: 15005] loss: 0.008251287043094635\n",
      "[step: 15006] loss: 0.009119396097958088\n",
      "[step: 15007] loss: 0.008524803444743156\n",
      "[step: 15008] loss: 0.009277083911001682\n",
      "[step: 15009] loss: 0.007817541249096394\n",
      "[step: 15010] loss: 0.009112183004617691\n",
      "[step: 15011] loss: 0.007674870081245899\n",
      "[step: 15012] loss: 0.008769987151026726\n",
      "[step: 15013] loss: 0.007709935307502747\n",
      "[step: 15014] loss: 0.008162230253219604\n",
      "[step: 15015] loss: 0.007544965483248234\n",
      "[step: 15016] loss: 0.007552612107247114\n",
      "[step: 15017] loss: 0.007886002771556377\n",
      "[step: 15018] loss: 0.007241442799568176\n",
      "[step: 15019] loss: 0.00764291500672698\n",
      "[step: 15020] loss: 0.007352080196142197\n",
      "[step: 15021] loss: 0.0075237443670630455\n",
      "[step: 15022] loss: 0.008001952432096004\n",
      "[step: 15023] loss: 0.007348882034420967\n",
      "[step: 15024] loss: 0.007802008185535669\n",
      "[step: 15025] loss: 0.007724971976131201\n",
      "[step: 15026] loss: 0.007673162966966629\n",
      "[step: 15027] loss: 0.00749475322663784\n",
      "[step: 15028] loss: 0.007348139304667711\n",
      "[step: 15029] loss: 0.007414176128804684\n",
      "[step: 15030] loss: 0.007328686770051718\n",
      "[step: 15031] loss: 0.007176978513598442\n",
      "[step: 15032] loss: 0.007352694869041443\n",
      "[step: 15033] loss: 0.007289007771760225\n",
      "[step: 15034] loss: 0.007174172904342413\n",
      "[step: 15035] loss: 0.007314447313547134\n",
      "[step: 15036] loss: 0.007121772039681673\n",
      "[step: 15037] loss: 0.007224607281386852\n",
      "[step: 15038] loss: 0.007104312069714069\n",
      "[step: 15039] loss: 0.00713481567800045\n",
      "[step: 15040] loss: 0.007125126663595438\n",
      "[step: 15041] loss: 0.007099436596035957\n",
      "[step: 15042] loss: 0.007065725512802601\n",
      "[step: 15043] loss: 0.007057943847030401\n",
      "[step: 15044] loss: 0.007071095984429121\n",
      "[step: 15045] loss: 0.007000353187322617\n",
      "[step: 15046] loss: 0.0070401402190327644\n",
      "[step: 15047] loss: 0.007018807344138622\n",
      "[step: 15048] loss: 0.007003989536315203\n",
      "[step: 15049] loss: 0.006988794542849064\n",
      "[step: 15050] loss: 0.007006440311670303\n",
      "[step: 15051] loss: 0.006959779653698206\n",
      "[step: 15052] loss: 0.006962256506085396\n",
      "[step: 15053] loss: 0.006958555895835161\n",
      "[step: 15054] loss: 0.006932705640792847\n",
      "[step: 15055] loss: 0.006936545949429274\n",
      "[step: 15056] loss: 0.0069216471165418625\n",
      "[step: 15057] loss: 0.00689998734742403\n",
      "[step: 15058] loss: 0.006909237243235111\n",
      "[step: 15059] loss: 0.006928961258381605\n",
      "[step: 15060] loss: 0.006894265301525593\n",
      "[step: 15061] loss: 0.0068949488922953606\n",
      "[step: 15062] loss: 0.006930578500032425\n",
      "[step: 15063] loss: 0.006923971232026815\n",
      "[step: 15064] loss: 0.006894080899655819\n",
      "[step: 15065] loss: 0.006855969317257404\n",
      "[step: 15066] loss: 0.006927393842488527\n",
      "[step: 15067] loss: 0.0070626018568873405\n",
      "[step: 15068] loss: 0.007040162570774555\n",
      "[step: 15069] loss: 0.006909793242812157\n",
      "[step: 15070] loss: 0.007089267950505018\n",
      "[step: 15071] loss: 0.0070023974403738976\n",
      "[step: 15072] loss: 0.006970902439206839\n",
      "[step: 15073] loss: 0.007033427711576223\n",
      "[step: 15074] loss: 0.006923209875822067\n",
      "[step: 15075] loss: 0.006901497486978769\n",
      "[step: 15076] loss: 0.006971896160393953\n",
      "[step: 15077] loss: 0.0070483507588505745\n",
      "[step: 15078] loss: 0.0068921418860554695\n",
      "[step: 15079] loss: 0.007022813428193331\n",
      "[step: 15080] loss: 0.006963194347918034\n",
      "[step: 15081] loss: 0.007068206090480089\n",
      "[step: 15082] loss: 0.007028341758996248\n",
      "[step: 15083] loss: 0.006881551817059517\n",
      "[step: 15084] loss: 0.0069922953844070435\n",
      "[step: 15085] loss: 0.006971737835556269\n",
      "[step: 15086] loss: 0.006887464318424463\n",
      "[step: 15087] loss: 0.00690331868827343\n",
      "[step: 15088] loss: 0.006933630909770727\n",
      "[step: 15089] loss: 0.00690207676962018\n",
      "[step: 15090] loss: 0.006835318636149168\n",
      "[step: 15091] loss: 0.006901402957737446\n",
      "[step: 15092] loss: 0.006875577382743359\n",
      "[step: 15093] loss: 0.006816778797656298\n",
      "[step: 15094] loss: 0.006838655099272728\n",
      "[step: 15095] loss: 0.006827231962233782\n",
      "[step: 15096] loss: 0.006891150027513504\n",
      "[step: 15097] loss: 0.0068979100324213505\n",
      "[step: 15098] loss: 0.006963532418012619\n",
      "[step: 15099] loss: 0.006846750155091286\n",
      "[step: 15100] loss: 0.006930878851562738\n",
      "[step: 15101] loss: 0.006958354730159044\n",
      "[step: 15102] loss: 0.007303520105779171\n",
      "[step: 15103] loss: 0.007338494062423706\n",
      "[step: 15104] loss: 0.007043241057544947\n",
      "[step: 15105] loss: 0.00766610587015748\n",
      "[step: 15106] loss: 0.0069036646746098995\n",
      "[step: 15107] loss: 0.0075548007152974606\n",
      "[step: 15108] loss: 0.006954984739422798\n",
      "[step: 15109] loss: 0.00735357915982604\n",
      "[step: 15110] loss: 0.007112168241292238\n",
      "[step: 15111] loss: 0.007174260448664427\n",
      "[step: 15112] loss: 0.006951544433832169\n",
      "[step: 15113] loss: 0.007120382506400347\n",
      "[step: 15114] loss: 0.0071480912156403065\n",
      "[step: 15115] loss: 0.006835788022726774\n",
      "[step: 15116] loss: 0.007260926067829132\n",
      "[step: 15117] loss: 0.007550707086920738\n",
      "[step: 15118] loss: 0.00680766673758626\n",
      "[step: 15119] loss: 0.007749419193714857\n",
      "[step: 15120] loss: 0.00788650382310152\n",
      "[step: 15121] loss: 0.007689226418733597\n",
      "[step: 15122] loss: 0.007538516074419022\n",
      "[step: 15123] loss: 0.007474756799638271\n",
      "[step: 15124] loss: 0.007601987104862928\n",
      "[step: 15125] loss: 0.007717560976743698\n",
      "[step: 15126] loss: 0.007220047060400248\n",
      "[step: 15127] loss: 0.007475519552826881\n",
      "[step: 15128] loss: 0.007161062676459551\n",
      "[step: 15129] loss: 0.007348363753408194\n",
      "[step: 15130] loss: 0.007036500610411167\n",
      "[step: 15131] loss: 0.007293632719665766\n",
      "[step: 15132] loss: 0.007087492849677801\n",
      "[step: 15133] loss: 0.007238017860800028\n",
      "[step: 15134] loss: 0.00688306475058198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 15135] loss: 0.0073303645476698875\n",
      "[step: 15136] loss: 0.007023501675575972\n",
      "[step: 15137] loss: 0.007210610434412956\n",
      "[step: 15138] loss: 0.006956796161830425\n",
      "[step: 15139] loss: 0.007499772123992443\n",
      "[step: 15140] loss: 0.007653010543435812\n",
      "[step: 15141] loss: 0.007783492561429739\n",
      "[step: 15142] loss: 0.007364286575466394\n",
      "[step: 15143] loss: 0.007523024454712868\n",
      "[step: 15144] loss: 0.006968799512833357\n",
      "[step: 15145] loss: 0.007430680096149445\n",
      "[step: 15146] loss: 0.007355821318924427\n",
      "[step: 15147] loss: 0.007266082335263491\n",
      "[step: 15148] loss: 0.007190106436610222\n",
      "[step: 15149] loss: 0.0071781100705266\n",
      "[step: 15150] loss: 0.007145103998482227\n",
      "[step: 15151] loss: 0.006954723037779331\n",
      "[step: 15152] loss: 0.0073409657925367355\n",
      "[step: 15153] loss: 0.007566855289041996\n",
      "[step: 15154] loss: 0.007152479141950607\n",
      "[step: 15155] loss: 0.0076585859060287476\n",
      "[step: 15156] loss: 0.007361738942563534\n",
      "[step: 15157] loss: 0.007484087720513344\n",
      "[step: 15158] loss: 0.007275525480508804\n",
      "[step: 15159] loss: 0.00795276090502739\n",
      "[step: 15160] loss: 0.0077444720081985\n",
      "[step: 15161] loss: 0.008052020333707333\n",
      "[step: 15162] loss: 0.008352724835276604\n",
      "[step: 15163] loss: 0.008728434331715107\n",
      "[step: 15164] loss: 0.00820110272616148\n",
      "[step: 15165] loss: 0.008335473015904427\n",
      "[step: 15166] loss: 0.008146663196384907\n",
      "[step: 15167] loss: 0.008073185570538044\n",
      "[step: 15168] loss: 0.008445211686193943\n",
      "[step: 15169] loss: 0.007979382760822773\n",
      "[step: 15170] loss: 0.007415676489472389\n",
      "[step: 15171] loss: 0.007979488000273705\n",
      "[step: 15172] loss: 0.007103307172656059\n",
      "[step: 15173] loss: 0.007714743260294199\n",
      "[step: 15174] loss: 0.007635927759110928\n",
      "[step: 15175] loss: 0.007440598215907812\n",
      "[step: 15176] loss: 0.007225564680993557\n",
      "[step: 15177] loss: 0.007331492379307747\n",
      "[step: 15178] loss: 0.0072524771094322205\n",
      "[step: 15179] loss: 0.007183580659329891\n",
      "[step: 15180] loss: 0.007111492101103067\n",
      "[step: 15181] loss: 0.007194028235971928\n",
      "[step: 15182] loss: 0.006999956909567118\n",
      "[step: 15183] loss: 0.007148170378059149\n",
      "[step: 15184] loss: 0.0069963871501386166\n",
      "[step: 15185] loss: 0.00703270873054862\n",
      "[step: 15186] loss: 0.007003379054367542\n",
      "[step: 15187] loss: 0.0069599756971001625\n",
      "[step: 15188] loss: 0.006919641047716141\n",
      "[step: 15189] loss: 0.006971601862460375\n",
      "[step: 15190] loss: 0.006892479956150055\n",
      "[step: 15191] loss: 0.006932295858860016\n",
      "[step: 15192] loss: 0.006853335537016392\n",
      "[step: 15193] loss: 0.006880049128085375\n",
      "[step: 15194] loss: 0.006871635094285011\n",
      "[step: 15195] loss: 0.006901712156832218\n",
      "[step: 15196] loss: 0.006922654341906309\n",
      "[step: 15197] loss: 0.006818781141191721\n",
      "[step: 15198] loss: 0.006886685267090797\n",
      "[step: 15199] loss: 0.0068231141194701195\n",
      "[step: 15200] loss: 0.0068065389059484005\n",
      "[step: 15201] loss: 0.006816720589995384\n",
      "[step: 15202] loss: 0.006785320583730936\n",
      "[step: 15203] loss: 0.006799762137234211\n",
      "[step: 15204] loss: 0.0067706783302128315\n",
      "[step: 15205] loss: 0.006798602174967527\n",
      "[step: 15206] loss: 0.006736265029758215\n",
      "[step: 15207] loss: 0.006755838170647621\n",
      "[step: 15208] loss: 0.006733706686645746\n",
      "[step: 15209] loss: 0.006731732748448849\n",
      "[step: 15210] loss: 0.006704274099320173\n",
      "[step: 15211] loss: 0.006720672827214003\n",
      "[step: 15212] loss: 0.006719676777720451\n",
      "[step: 15213] loss: 0.00674897525459528\n",
      "[step: 15214] loss: 0.006819610018283129\n",
      "[step: 15215] loss: 0.006745420396327972\n",
      "[step: 15216] loss: 0.0068397847935557365\n",
      "[step: 15217] loss: 0.0067443554289639\n",
      "[step: 15218] loss: 0.006826688069850206\n",
      "[step: 15219] loss: 0.00689249113202095\n",
      "[step: 15220] loss: 0.0067655062302947044\n",
      "[step: 15221] loss: 0.006880417000502348\n",
      "[step: 15222] loss: 0.006905927322804928\n",
      "[step: 15223] loss: 0.0067318459041416645\n",
      "[step: 15224] loss: 0.00679801544174552\n",
      "[step: 15225] loss: 0.0068048895336687565\n",
      "[step: 15226] loss: 0.006715006660670042\n",
      "[step: 15227] loss: 0.006810641381889582\n",
      "[step: 15228] loss: 0.006792494095861912\n",
      "[step: 15229] loss: 0.006736223120242357\n",
      "[step: 15230] loss: 0.006828954443335533\n",
      "[step: 15231] loss: 0.006980473175644875\n",
      "[step: 15232] loss: 0.006830110214650631\n",
      "[step: 15233] loss: 0.006974995601922274\n",
      "[step: 15234] loss: 0.006760626565665007\n",
      "[step: 15235] loss: 0.006802617572247982\n",
      "[step: 15236] loss: 0.006904357112944126\n",
      "[step: 15237] loss: 0.0069846902042627335\n",
      "[step: 15238] loss: 0.007060324773192406\n",
      "[step: 15239] loss: 0.006888149306178093\n",
      "[step: 15240] loss: 0.0068094199523329735\n",
      "[step: 15241] loss: 0.0068629346787929535\n",
      "[step: 15242] loss: 0.006791120860725641\n",
      "[step: 15243] loss: 0.006754799280315638\n",
      "[step: 15244] loss: 0.006803500931710005\n",
      "[step: 15245] loss: 0.006756536662578583\n",
      "[step: 15246] loss: 0.00667227990925312\n",
      "[step: 15247] loss: 0.006702162325382233\n",
      "[step: 15248] loss: 0.006786352954804897\n",
      "[step: 15249] loss: 0.007028224878013134\n",
      "[step: 15250] loss: 0.00682324543595314\n",
      "[step: 15251] loss: 0.006840428803116083\n",
      "[step: 15252] loss: 0.006975987926125526\n",
      "[step: 15253] loss: 0.0067518618889153\n",
      "[step: 15254] loss: 0.006935632321983576\n",
      "[step: 15255] loss: 0.006869378499686718\n",
      "[step: 15256] loss: 0.006879311054944992\n",
      "[step: 15257] loss: 0.006850066594779491\n",
      "[step: 15258] loss: 0.006894942373037338\n",
      "[step: 15259] loss: 0.006777562666684389\n",
      "[step: 15260] loss: 0.006840527057647705\n",
      "[step: 15261] loss: 0.007074343040585518\n",
      "[step: 15262] loss: 0.006821532268077135\n",
      "[step: 15263] loss: 0.0067176977172493935\n",
      "[step: 15264] loss: 0.00682193273678422\n",
      "[step: 15265] loss: 0.006739619188010693\n",
      "[step: 15266] loss: 0.006696148309856653\n",
      "[step: 15267] loss: 0.006747717969119549\n",
      "[step: 15268] loss: 0.006721386685967445\n",
      "[step: 15269] loss: 0.0066877310164272785\n",
      "[step: 15270] loss: 0.006606772541999817\n",
      "[step: 15271] loss: 0.006918386090546846\n",
      "[step: 15272] loss: 0.008276292122900486\n",
      "[step: 15273] loss: 0.007573357317596674\n",
      "[step: 15274] loss: 0.00775607954710722\n",
      "[step: 15275] loss: 0.0076737180352211\n",
      "[step: 15276] loss: 0.00833345577120781\n",
      "[step: 15277] loss: 0.007397282402962446\n",
      "[step: 15278] loss: 0.00811441708356142\n",
      "[step: 15279] loss: 0.007793462369590998\n",
      "[step: 15280] loss: 0.007557818200439215\n",
      "[step: 15281] loss: 0.007576275616884232\n",
      "[step: 15282] loss: 0.007471619173884392\n",
      "[step: 15283] loss: 0.007480889093130827\n",
      "[step: 15284] loss: 0.007372445426881313\n",
      "[step: 15285] loss: 0.008185019716620445\n",
      "[step: 15286] loss: 0.008076163940131664\n",
      "[step: 15287] loss: 0.008030240423977375\n",
      "[step: 15288] loss: 0.007769439835101366\n",
      "[step: 15289] loss: 0.007597227580845356\n",
      "[step: 15290] loss: 0.007453943602740765\n",
      "[step: 15291] loss: 0.007556746248155832\n",
      "[step: 15292] loss: 0.0072385091334581375\n",
      "[step: 15293] loss: 0.0075289043597877026\n",
      "[step: 15294] loss: 0.007269087713211775\n",
      "[step: 15295] loss: 0.007018303032964468\n",
      "[step: 15296] loss: 0.007110502105206251\n",
      "[step: 15297] loss: 0.007193618454039097\n",
      "[step: 15298] loss: 0.007123040035367012\n",
      "[step: 15299] loss: 0.007018235512077808\n",
      "[step: 15300] loss: 0.007127587683498859\n",
      "[step: 15301] loss: 0.006890163756906986\n",
      "[step: 15302] loss: 0.00696180947124958\n",
      "[step: 15303] loss: 0.006960575468838215\n",
      "[step: 15304] loss: 0.006918599363416433\n",
      "[step: 15305] loss: 0.006850224453955889\n",
      "[step: 15306] loss: 0.00684506818652153\n",
      "[step: 15307] loss: 0.00683583552017808\n",
      "[step: 15308] loss: 0.006805925630033016\n",
      "[step: 15309] loss: 0.006760408170521259\n",
      "[step: 15310] loss: 0.00679022166877985\n",
      "[step: 15311] loss: 0.006835237145423889\n",
      "[step: 15312] loss: 0.006796606816351414\n",
      "[step: 15313] loss: 0.006695202086120844\n",
      "[step: 15314] loss: 0.006717942655086517\n",
      "[step: 15315] loss: 0.006738424766808748\n",
      "[step: 15316] loss: 0.0067714969627559185\n",
      "[step: 15317] loss: 0.006729121319949627\n",
      "[step: 15318] loss: 0.006827156059443951\n",
      "[step: 15319] loss: 0.006913541816174984\n",
      "[step: 15320] loss: 0.007310143206268549\n",
      "[step: 15321] loss: 0.007168994285166264\n",
      "[step: 15322] loss: 0.007119227200746536\n",
      "[step: 15323] loss: 0.006844357121735811\n",
      "[step: 15324] loss: 0.0070099057629704475\n",
      "[step: 15325] loss: 0.0073648071847856045\n",
      "[step: 15326] loss: 0.00681740278378129\n",
      "[step: 15327] loss: 0.006969808600842953\n",
      "[step: 15328] loss: 0.007436179555952549\n",
      "[step: 15329] loss: 0.006697533652186394\n",
      "[step: 15330] loss: 0.00762524176388979\n",
      "[step: 15331] loss: 0.007990476675331593\n",
      "[step: 15332] loss: 0.007317082490772009\n",
      "[step: 15333] loss: 0.007812692783772945\n",
      "[step: 15334] loss: 0.006846338510513306\n",
      "[step: 15335] loss: 0.007316797040402889\n",
      "[step: 15336] loss: 0.006999666336923838\n",
      "[step: 15337] loss: 0.007169645745307207\n",
      "[step: 15338] loss: 0.006934560369700193\n",
      "[step: 15339] loss: 0.0070463321171700954\n",
      "[step: 15340] loss: 0.006821813527494669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 15341] loss: 0.0070445784367620945\n",
      "[step: 15342] loss: 0.0071852849796414375\n",
      "[step: 15343] loss: 0.006868727970868349\n",
      "[step: 15344] loss: 0.007398403249680996\n",
      "[step: 15345] loss: 0.007010909263044596\n",
      "[step: 15346] loss: 0.007365202531218529\n",
      "[step: 15347] loss: 0.006879414897412062\n",
      "[step: 15348] loss: 0.007054770365357399\n",
      "[step: 15349] loss: 0.0073014735244214535\n",
      "[step: 15350] loss: 0.007516670040786266\n",
      "[step: 15351] loss: 0.006892220117151737\n",
      "[step: 15352] loss: 0.007778904866427183\n",
      "[step: 15353] loss: 0.007712214719504118\n",
      "[step: 15354] loss: 0.00771742220968008\n",
      "[step: 15355] loss: 0.007457568310201168\n",
      "[step: 15356] loss: 0.007639361545443535\n",
      "[step: 15357] loss: 0.007621999830007553\n",
      "[step: 15358] loss: 0.00725152064114809\n",
      "[step: 15359] loss: 0.007709460332989693\n",
      "[step: 15360] loss: 0.007810458540916443\n",
      "[step: 15361] loss: 0.00792254600673914\n",
      "[step: 15362] loss: 0.007721009198576212\n",
      "[step: 15363] loss: 0.007407033815979958\n",
      "[step: 15364] loss: 0.007716943044215441\n",
      "[step: 15365] loss: 0.007067203056067228\n",
      "[step: 15366] loss: 0.007447327021509409\n",
      "[step: 15367] loss: 0.0068595572374761105\n",
      "[step: 15368] loss: 0.007216593250632286\n",
      "[step: 15369] loss: 0.006900366395711899\n",
      "[step: 15370] loss: 0.006983288563787937\n",
      "[step: 15371] loss: 0.006972829345613718\n",
      "[step: 15372] loss: 0.00684752594679594\n",
      "[step: 15373] loss: 0.007347033824771643\n",
      "[step: 15374] loss: 0.007119239307940006\n",
      "[step: 15375] loss: 0.0077537065371870995\n",
      "[step: 15376] loss: 0.0076062665320932865\n",
      "[step: 15377] loss: 0.0072804647497832775\n",
      "[step: 15378] loss: 0.007765553425997496\n",
      "[step: 15379] loss: 0.007472093217074871\n",
      "[step: 15380] loss: 0.007636958733201027\n",
      "[step: 15381] loss: 0.007466020528227091\n",
      "[step: 15382] loss: 0.007434519473463297\n",
      "[step: 15383] loss: 0.007374807260930538\n",
      "[step: 15384] loss: 0.0072059184312820435\n",
      "[step: 15385] loss: 0.007406051270663738\n",
      "[step: 15386] loss: 0.007133473176509142\n",
      "[step: 15387] loss: 0.007587594911456108\n",
      "[step: 15388] loss: 0.00874420441687107\n",
      "[step: 15389] loss: 0.007855987176299095\n",
      "[step: 15390] loss: 0.008211215026676655\n",
      "[step: 15391] loss: 0.007981841452419758\n",
      "[step: 15392] loss: 0.007420184090733528\n",
      "[step: 15393] loss: 0.008206307888031006\n",
      "[step: 15394] loss: 0.0071425000205636024\n",
      "[step: 15395] loss: 0.00769856758415699\n",
      "[step: 15396] loss: 0.0073162573389709\n",
      "[step: 15397] loss: 0.007708355784416199\n",
      "[step: 15398] loss: 0.007227759342640638\n",
      "[step: 15399] loss: 0.007338867522776127\n",
      "[step: 15400] loss: 0.007156409788876772\n",
      "[step: 15401] loss: 0.007288110442459583\n",
      "[step: 15402] loss: 0.0073136333376169205\n",
      "[step: 15403] loss: 0.007223973050713539\n",
      "[step: 15404] loss: 0.007143658120185137\n",
      "[step: 15405] loss: 0.007035469636321068\n",
      "[step: 15406] loss: 0.007229615468531847\n",
      "[step: 15407] loss: 0.007101032417267561\n",
      "[step: 15408] loss: 0.007087518461048603\n",
      "[step: 15409] loss: 0.007092404644936323\n",
      "[step: 15410] loss: 0.006903891451656818\n",
      "[step: 15411] loss: 0.007007235661149025\n",
      "[step: 15412] loss: 0.0069270203821361065\n",
      "[step: 15413] loss: 0.00697732251137495\n",
      "[step: 15414] loss: 0.006894864607602358\n",
      "[step: 15415] loss: 0.006858146749436855\n",
      "[step: 15416] loss: 0.0068000974133610725\n",
      "[step: 15417] loss: 0.00681918952614069\n",
      "[step: 15418] loss: 0.00677253445610404\n",
      "[step: 15419] loss: 0.0067799026146531105\n",
      "[step: 15420] loss: 0.0066979555413126945\n",
      "[step: 15421] loss: 0.0067182076163589954\n",
      "[step: 15422] loss: 0.00667535001412034\n",
      "[step: 15423] loss: 0.006673112511634827\n",
      "[step: 15424] loss: 0.006690256297588348\n",
      "[step: 15425] loss: 0.00666142487898469\n",
      "[step: 15426] loss: 0.0066397227346897125\n",
      "[step: 15427] loss: 0.006627188064157963\n",
      "[step: 15428] loss: 0.006628710776567459\n",
      "[step: 15429] loss: 0.006615926045924425\n",
      "[step: 15430] loss: 0.006604781374335289\n",
      "[step: 15431] loss: 0.006610345095396042\n",
      "[step: 15432] loss: 0.006595130544155836\n",
      "[step: 15433] loss: 0.006579290609806776\n",
      "[step: 15434] loss: 0.006578922271728516\n",
      "[step: 15435] loss: 0.006571085192263126\n",
      "[step: 15436] loss: 0.0065588015131652355\n",
      "[step: 15437] loss: 0.006559425964951515\n",
      "[step: 15438] loss: 0.00655340263620019\n",
      "[step: 15439] loss: 0.006541477981954813\n",
      "[step: 15440] loss: 0.0065374355763196945\n",
      "[step: 15441] loss: 0.006535403896123171\n",
      "[step: 15442] loss: 0.0065247537568211555\n",
      "[step: 15443] loss: 0.006516201887279749\n",
      "[step: 15444] loss: 0.006513443309813738\n",
      "[step: 15445] loss: 0.00650849798694253\n",
      "[step: 15446] loss: 0.006501977797597647\n",
      "[step: 15447] loss: 0.006497793365269899\n",
      "[step: 15448] loss: 0.006496653892099857\n",
      "[step: 15449] loss: 0.006490328349173069\n",
      "[step: 15450] loss: 0.006485291291028261\n",
      "[step: 15451] loss: 0.006481842137873173\n",
      "[step: 15452] loss: 0.006481050048023462\n",
      "[step: 15453] loss: 0.006482718512415886\n",
      "[step: 15454] loss: 0.006485174410045147\n",
      "[step: 15455] loss: 0.00649331184104085\n",
      "[step: 15456] loss: 0.006520064081996679\n",
      "[step: 15457] loss: 0.006578909698873758\n",
      "[step: 15458] loss: 0.006741655059158802\n",
      "[step: 15459] loss: 0.006648910231888294\n",
      "[step: 15460] loss: 0.00653811264783144\n",
      "[step: 15461] loss: 0.0065389820374548435\n",
      "[step: 15462] loss: 0.0065693603828549385\n",
      "[step: 15463] loss: 0.0064588021486997604\n",
      "[step: 15464] loss: 0.0065704332664608955\n",
      "[step: 15465] loss: 0.006627620197832584\n",
      "[step: 15466] loss: 0.00681254081428051\n",
      "[step: 15467] loss: 0.006625091191381216\n",
      "[step: 15468] loss: 0.006569430232048035\n",
      "[step: 15469] loss: 0.0066184536553919315\n",
      "[step: 15470] loss: 0.006547082681208849\n",
      "[step: 15471] loss: 0.006818053778260946\n",
      "[step: 15472] loss: 0.006978692952543497\n",
      "[step: 15473] loss: 0.007178561296314001\n",
      "[step: 15474] loss: 0.0069525837898254395\n",
      "[step: 15475] loss: 0.00743085565045476\n",
      "[step: 15476] loss: 0.007510734256356955\n",
      "[step: 15477] loss: 0.007137757260352373\n",
      "[step: 15478] loss: 0.008222172036767006\n",
      "[step: 15479] loss: 0.00700517650693655\n",
      "[step: 15480] loss: 0.007861809805035591\n",
      "[step: 15481] loss: 0.007961088791489601\n",
      "[step: 15482] loss: 0.007221294101327658\n",
      "[step: 15483] loss: 0.010276127606630325\n",
      "[step: 15484] loss: 0.008217605762183666\n",
      "[step: 15485] loss: 0.008946118876338005\n",
      "[step: 15486] loss: 0.008632934652268887\n",
      "[step: 15487] loss: 0.008514465764164925\n",
      "[step: 15488] loss: 0.0076742107048630714\n",
      "[step: 15489] loss: 0.008652973920106888\n",
      "[step: 15490] loss: 0.007514884229749441\n",
      "[step: 15491] loss: 0.008126850239932537\n",
      "[step: 15492] loss: 0.007565015461295843\n",
      "[step: 15493] loss: 0.008018868044018745\n",
      "[step: 15494] loss: 0.00821318943053484\n",
      "[step: 15495] loss: 0.007429363671690226\n",
      "[step: 15496] loss: 0.0077169518917799\n",
      "[step: 15497] loss: 0.007243276108056307\n",
      "[step: 15498] loss: 0.007653341628611088\n",
      "[step: 15499] loss: 0.008984449319541454\n",
      "[step: 15500] loss: 0.008436854928731918\n",
      "[step: 15501] loss: 0.008178568445146084\n",
      "[step: 15502] loss: 0.009056231006979942\n",
      "[step: 15503] loss: 0.007864128798246384\n",
      "[step: 15504] loss: 0.008518445305526257\n",
      "[step: 15505] loss: 0.008197665214538574\n",
      "[step: 15506] loss: 0.007707573939114809\n",
      "[step: 15507] loss: 0.008152325637638569\n",
      "[step: 15508] loss: 0.0073025901801884174\n",
      "[step: 15509] loss: 0.007951878011226654\n",
      "[step: 15510] loss: 0.00797494500875473\n",
      "[step: 15511] loss: 0.00738552724942565\n",
      "[step: 15512] loss: 0.007595202885568142\n",
      "[step: 15513] loss: 0.007440455257892609\n",
      "[step: 15514] loss: 0.007632391061633825\n",
      "[step: 15515] loss: 0.0072123515419662\n",
      "[step: 15516] loss: 0.0073731932789087296\n",
      "[step: 15517] loss: 0.007220152299851179\n",
      "[step: 15518] loss: 0.00722774863243103\n",
      "[step: 15519] loss: 0.0070483689196407795\n",
      "[step: 15520] loss: 0.007195998914539814\n",
      "[step: 15521] loss: 0.006982641294598579\n",
      "[step: 15522] loss: 0.007044284604489803\n",
      "[step: 15523] loss: 0.00704262824729085\n",
      "[step: 15524] loss: 0.007019649725407362\n",
      "[step: 15525] loss: 0.006965688429772854\n",
      "[step: 15526] loss: 0.006987845525145531\n",
      "[step: 15527] loss: 0.006941760424524546\n",
      "[step: 15528] loss: 0.006905052345246077\n",
      "[step: 15529] loss: 0.006910181604325771\n",
      "[step: 15530] loss: 0.0068205432035028934\n",
      "[step: 15531] loss: 0.00688260467723012\n",
      "[step: 15532] loss: 0.006822486408054829\n",
      "[step: 15533] loss: 0.00685529038310051\n",
      "[step: 15534] loss: 0.006859075278043747\n",
      "[step: 15535] loss: 0.006833758670836687\n",
      "[step: 15536] loss: 0.006784077268093824\n",
      "[step: 15537] loss: 0.00679904967546463\n",
      "[step: 15538] loss: 0.00675928546115756\n",
      "[step: 15539] loss: 0.00678277388215065\n",
      "[step: 15540] loss: 0.006745598278939724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 15541] loss: 0.006752365734428167\n",
      "[step: 15542] loss: 0.00673675024881959\n",
      "[step: 15543] loss: 0.006724650971591473\n",
      "[step: 15544] loss: 0.006738685537129641\n",
      "[step: 15545] loss: 0.006696187891066074\n",
      "[step: 15546] loss: 0.006710848305374384\n",
      "[step: 15547] loss: 0.006678204517811537\n",
      "[step: 15548] loss: 0.0066946749575436115\n",
      "[step: 15549] loss: 0.006658217869699001\n",
      "[step: 15550] loss: 0.006677007768303156\n",
      "[step: 15551] loss: 0.006652368698269129\n",
      "[step: 15552] loss: 0.00666252663359046\n",
      "[step: 15553] loss: 0.00663983728736639\n",
      "[step: 15554] loss: 0.006633214186877012\n",
      "[step: 15555] loss: 0.006625832989811897\n",
      "[step: 15556] loss: 0.006617376115173101\n",
      "[step: 15557] loss: 0.006619379855692387\n",
      "[step: 15558] loss: 0.006603463552892208\n",
      "[step: 15559] loss: 0.006605362053960562\n",
      "[step: 15560] loss: 0.006597583647817373\n",
      "[step: 15561] loss: 0.006590891629457474\n",
      "[step: 15562] loss: 0.006593364756554365\n",
      "[step: 15563] loss: 0.0066087753511965275\n",
      "[step: 15564] loss: 0.006621908862143755\n",
      "[step: 15565] loss: 0.0066955541260540485\n",
      "[step: 15566] loss: 0.006837033666670322\n",
      "[step: 15567] loss: 0.006812504958361387\n",
      "[step: 15568] loss: 0.006613465957343578\n",
      "[step: 15569] loss: 0.006532670930027962\n",
      "[step: 15570] loss: 0.0065073128789663315\n",
      "[step: 15571] loss: 0.006517298985272646\n",
      "[step: 15572] loss: 0.00656307814642787\n",
      "[step: 15573] loss: 0.006607803516089916\n",
      "[step: 15574] loss: 0.00682158162817359\n",
      "[step: 15575] loss: 0.006909897085279226\n",
      "[step: 15576] loss: 0.006622412241995335\n",
      "[step: 15577] loss: 0.0066455453634262085\n",
      "[step: 15578] loss: 0.0068771434016525745\n",
      "[step: 15579] loss: 0.006533509120345116\n",
      "[step: 15580] loss: 0.006967475172132254\n",
      "[step: 15581] loss: 0.0070410361513495445\n",
      "[step: 15582] loss: 0.006646185647696257\n",
      "[step: 15583] loss: 0.0074389223009347916\n",
      "[step: 15584] loss: 0.006613569334149361\n",
      "[step: 15585] loss: 0.006919515319168568\n",
      "[step: 15586] loss: 0.007097449619323015\n",
      "[step: 15587] loss: 0.006593915168195963\n",
      "[step: 15588] loss: 0.007361182011663914\n",
      "[step: 15589] loss: 0.006522020790725946\n",
      "[step: 15590] loss: 0.0071796453557908535\n",
      "[step: 15591] loss: 0.007447955664247274\n",
      "[step: 15592] loss: 0.006747869774699211\n",
      "[step: 15593] loss: 0.007523882668465376\n",
      "[step: 15594] loss: 0.006635167635977268\n",
      "[step: 15595] loss: 0.007030826527625322\n",
      "[step: 15596] loss: 0.007068412844091654\n",
      "[step: 15597] loss: 0.006771104875952005\n",
      "[step: 15598] loss: 0.0070138308219611645\n",
      "[step: 15599] loss: 0.006693931762129068\n",
      "[step: 15600] loss: 0.007230876944959164\n",
      "[step: 15601] loss: 0.006648166570812464\n",
      "[step: 15602] loss: 0.007429147604852915\n",
      "[step: 15603] loss: 0.006755053997039795\n",
      "[step: 15604] loss: 0.007245092652738094\n",
      "[step: 15605] loss: 0.006747595965862274\n",
      "[step: 15606] loss: 0.00705693569034338\n",
      "[step: 15607] loss: 0.006907679606229067\n",
      "[step: 15608] loss: 0.006789283826947212\n",
      "[step: 15609] loss: 0.006614659912884235\n",
      "[step: 15610] loss: 0.006805801298469305\n",
      "[step: 15611] loss: 0.006778824143111706\n",
      "[step: 15612] loss: 0.006683120969682932\n",
      "[step: 15613] loss: 0.006731037516146898\n",
      "[step: 15614] loss: 0.006517124827951193\n",
      "[step: 15615] loss: 0.0066156634129583836\n",
      "[step: 15616] loss: 0.0065706283785402775\n",
      "[step: 15617] loss: 0.006534446496516466\n",
      "[step: 15618] loss: 0.0065771727822721004\n",
      "[step: 15619] loss: 0.006450419779866934\n",
      "[step: 15620] loss: 0.006507871672511101\n",
      "[step: 15621] loss: 0.006504555232822895\n",
      "[step: 15622] loss: 0.006441161502152681\n",
      "[step: 15623] loss: 0.006483230274170637\n",
      "[step: 15624] loss: 0.006455664522945881\n",
      "[step: 15625] loss: 0.006404135376214981\n",
      "[step: 15626] loss: 0.006505869794636965\n",
      "[step: 15627] loss: 0.006411929614841938\n",
      "[step: 15628] loss: 0.006400899030268192\n",
      "[step: 15629] loss: 0.006440142635256052\n",
      "[step: 15630] loss: 0.006442125421017408\n",
      "[step: 15631] loss: 0.006375130265951157\n",
      "[step: 15632] loss: 0.006385861895978451\n",
      "[step: 15633] loss: 0.006412715185433626\n",
      "[step: 15634] loss: 0.006356419529765844\n",
      "[step: 15635] loss: 0.00636020814999938\n",
      "[step: 15636] loss: 0.0063692438416182995\n",
      "[step: 15637] loss: 0.006373539101332426\n",
      "[step: 15638] loss: 0.00635402649641037\n",
      "[step: 15639] loss: 0.006352644879370928\n",
      "[step: 15640] loss: 0.006366835907101631\n",
      "[step: 15641] loss: 0.006380636245012283\n",
      "[step: 15642] loss: 0.006396293640136719\n",
      "[step: 15643] loss: 0.006393071264028549\n",
      "[step: 15644] loss: 0.0064850919879972935\n",
      "[step: 15645] loss: 0.00667845644056797\n",
      "[step: 15646] loss: 0.006395800970494747\n",
      "[step: 15647] loss: 0.006485248915851116\n",
      "[step: 15648] loss: 0.006580262910574675\n",
      "[step: 15649] loss: 0.006694907788187265\n",
      "[step: 15650] loss: 0.006361702922731638\n",
      "[step: 15651] loss: 0.006499528884887695\n",
      "[step: 15652] loss: 0.006704399362206459\n",
      "[step: 15653] loss: 0.006609003990888596\n",
      "[step: 15654] loss: 0.00655782176181674\n",
      "[step: 15655] loss: 0.006464582402259111\n",
      "[step: 15656] loss: 0.0066010900773108006\n",
      "[step: 15657] loss: 0.00637426134198904\n",
      "[step: 15658] loss: 0.0065047768875956535\n",
      "[step: 15659] loss: 0.006370809860527515\n",
      "[step: 15660] loss: 0.006651074625551701\n",
      "[step: 15661] loss: 0.006904127541929483\n",
      "[step: 15662] loss: 0.006448687985539436\n",
      "[step: 15663] loss: 0.007186125498265028\n",
      "[step: 15664] loss: 0.006925912573933601\n",
      "[step: 15665] loss: 0.007158858235925436\n",
      "[step: 15666] loss: 0.0104895094409585\n",
      "[step: 15667] loss: 0.008739647455513477\n",
      "[step: 15668] loss: 0.008985414169728756\n",
      "[step: 15669] loss: 0.010647408664226532\n",
      "[step: 15670] loss: 0.008956653997302055\n",
      "[step: 15671] loss: 0.009199029766023159\n",
      "[step: 15672] loss: 0.009859556332230568\n",
      "[step: 15673] loss: 0.00911798793822527\n",
      "[step: 15674] loss: 0.009729154407978058\n",
      "[step: 15675] loss: 0.00908981915563345\n",
      "[step: 15676] loss: 0.009526348672807217\n",
      "[step: 15677] loss: 0.009178584441542625\n",
      "[step: 15678] loss: 0.009009812958538532\n",
      "[step: 15679] loss: 0.008523792028427124\n",
      "[step: 15680] loss: 0.00827812496572733\n",
      "[step: 15681] loss: 0.008041095919907093\n",
      "[step: 15682] loss: 0.007784571032971144\n",
      "[step: 15683] loss: 0.007615001872181892\n",
      "[step: 15684] loss: 0.00748499995097518\n",
      "[step: 15685] loss: 0.0072211334481835365\n",
      "[step: 15686] loss: 0.007150021847337484\n",
      "[step: 15687] loss: 0.007194561418145895\n",
      "[step: 15688] loss: 0.007377962581813335\n",
      "[step: 15689] loss: 0.006916185840964317\n",
      "[step: 15690] loss: 0.007806769572198391\n",
      "[step: 15691] loss: 0.01032793615013361\n",
      "[step: 15692] loss: 0.010365449823439121\n",
      "[step: 15693] loss: 0.008467684499919415\n",
      "[step: 15694] loss: 0.010104503482580185\n",
      "[step: 15695] loss: 0.008443408645689487\n",
      "[step: 15696] loss: 0.009056312963366508\n",
      "[step: 15697] loss: 0.009464267641305923\n",
      "[step: 15698] loss: 0.008974479511380196\n",
      "[step: 15699] loss: 0.008314492180943489\n",
      "[step: 15700] loss: 0.008602390065789223\n",
      "[step: 15701] loss: 0.008548215962946415\n",
      "[step: 15702] loss: 0.008435887284576893\n",
      "[step: 15703] loss: 0.00866524688899517\n",
      "[step: 15704] loss: 0.008471633307635784\n",
      "[step: 15705] loss: 0.008309458382427692\n",
      "[step: 15706] loss: 0.008241144940257072\n",
      "[step: 15707] loss: 0.008147446438670158\n",
      "[step: 15708] loss: 0.007999191991984844\n",
      "[step: 15709] loss: 0.007958346046507359\n",
      "[step: 15710] loss: 0.00798533484339714\n",
      "[step: 15711] loss: 0.008163845166563988\n",
      "[step: 15712] loss: 0.007889501750469208\n",
      "[step: 15713] loss: 0.007695225533097982\n",
      "[step: 15714] loss: 0.007791262120008469\n",
      "[step: 15715] loss: 0.007743739988654852\n",
      "[step: 15716] loss: 0.007666969206184149\n",
      "[step: 15717] loss: 0.007639446295797825\n",
      "[step: 15718] loss: 0.007600764743983746\n",
      "[step: 15719] loss: 0.007564296945929527\n",
      "[step: 15720] loss: 0.0075018578208982944\n",
      "[step: 15721] loss: 0.007461325265467167\n",
      "[step: 15722] loss: 0.007364179007709026\n",
      "[step: 15723] loss: 0.007386290468275547\n",
      "[step: 15724] loss: 0.007311290130019188\n",
      "[step: 15725] loss: 0.007210400886833668\n",
      "[step: 15726] loss: 0.007297424133867025\n",
      "[step: 15727] loss: 0.007161595392972231\n",
      "[step: 15728] loss: 0.007168843410909176\n",
      "[step: 15729] loss: 0.007185796275734901\n",
      "[step: 15730] loss: 0.007088443264365196\n",
      "[step: 15731] loss: 0.007115994114428759\n",
      "[step: 15732] loss: 0.007048755418509245\n",
      "[step: 15733] loss: 0.007079023867845535\n",
      "[step: 15734] loss: 0.007008269429206848\n",
      "[step: 15735] loss: 0.007047215010970831\n",
      "[step: 15736] loss: 0.00696505606174469\n",
      "[step: 15737] loss: 0.0070008523762226105\n",
      "[step: 15738] loss: 0.00692219752818346\n",
      "[step: 15739] loss: 0.00696083577349782\n",
      "[step: 15740] loss: 0.006894649472087622\n",
      "[step: 15741] loss: 0.006901193410158157\n",
      "[step: 15742] loss: 0.006867887452244759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 15743] loss: 0.006867985241115093\n",
      "[step: 15744] loss: 0.006836167071014643\n",
      "[step: 15745] loss: 0.006833776831626892\n",
      "[step: 15746] loss: 0.0068062422797083855\n",
      "[step: 15747] loss: 0.0068129743449389935\n",
      "[step: 15748] loss: 0.006781393196433783\n",
      "[step: 15749] loss: 0.006785348989069462\n",
      "[step: 15750] loss: 0.006762847304344177\n",
      "[step: 15751] loss: 0.006760876160115004\n",
      "[step: 15752] loss: 0.00674434844404459\n",
      "[step: 15753] loss: 0.006740683689713478\n",
      "[step: 15754] loss: 0.0067282565869390965\n",
      "[step: 15755] loss: 0.0067192488349974155\n",
      "[step: 15756] loss: 0.006714203394949436\n",
      "[step: 15757] loss: 0.00670241704210639\n",
      "[step: 15758] loss: 0.006692082621157169\n",
      "[step: 15759] loss: 0.006686865817755461\n",
      "[step: 15760] loss: 0.006679454352706671\n",
      "[step: 15761] loss: 0.006666385568678379\n",
      "[step: 15762] loss: 0.006659614387899637\n",
      "[step: 15763] loss: 0.006650300696492195\n",
      "[step: 15764] loss: 0.006646374706178904\n",
      "[step: 15765] loss: 0.006637290585786104\n",
      "[step: 15766] loss: 0.006625903304666281\n",
      "[step: 15767] loss: 0.006618646439164877\n",
      "[step: 15768] loss: 0.006610408425331116\n",
      "[step: 15769] loss: 0.006605847738683224\n",
      "[step: 15770] loss: 0.006600402761250734\n",
      "[step: 15771] loss: 0.006593180354684591\n",
      "[step: 15772] loss: 0.006585460156202316\n",
      "[step: 15773] loss: 0.006575492210686207\n",
      "[step: 15774] loss: 0.00656762532889843\n",
      "[step: 15775] loss: 0.006560294423252344\n",
      "[step: 15776] loss: 0.00655127689242363\n",
      "[step: 15777] loss: 0.006545630749315023\n",
      "[step: 15778] loss: 0.006537165492773056\n",
      "[step: 15779] loss: 0.006530243903398514\n",
      "[step: 15780] loss: 0.006523645017296076\n",
      "[step: 15781] loss: 0.00651607196778059\n",
      "[step: 15782] loss: 0.006509349215775728\n",
      "[step: 15783] loss: 0.006503114942461252\n",
      "[step: 15784] loss: 0.00649550324305892\n",
      "[step: 15785] loss: 0.006489559076726437\n",
      "[step: 15786] loss: 0.006483124103397131\n",
      "[step: 15787] loss: 0.006476820912212133\n",
      "[step: 15788] loss: 0.006471928209066391\n",
      "[step: 15789] loss: 0.006470187567174435\n",
      "[step: 15790] loss: 0.006475111935287714\n",
      "[step: 15791] loss: 0.006498189177364111\n",
      "[step: 15792] loss: 0.006587413139641285\n",
      "[step: 15793] loss: 0.0067033180966973305\n",
      "[step: 15794] loss: 0.006958331447094679\n",
      "[step: 15795] loss: 0.00671366835013032\n",
      "[step: 15796] loss: 0.006488746032118797\n",
      "[step: 15797] loss: 0.006473653484135866\n",
      "[step: 15798] loss: 0.006623179651796818\n",
      "[step: 15799] loss: 0.0069611407816410065\n",
      "[step: 15800] loss: 0.006869748700410128\n",
      "[step: 15801] loss: 0.006533148232847452\n",
      "[step: 15802] loss: 0.006829097401350737\n",
      "[step: 15803] loss: 0.007397163193672895\n",
      "[step: 15804] loss: 0.006946936249732971\n",
      "[step: 15805] loss: 0.0077613466419279575\n",
      "[step: 15806] loss: 0.007072698790580034\n",
      "[step: 15807] loss: 0.007280347403138876\n",
      "[step: 15808] loss: 0.007056690752506256\n",
      "[step: 15809] loss: 0.007848012261092663\n",
      "[step: 15810] loss: 0.007019757758826017\n",
      "[step: 15811] loss: 0.006896310951560736\n",
      "[step: 15812] loss: 0.0070699225179851055\n",
      "[step: 15813] loss: 0.006600136868655682\n",
      "[step: 15814] loss: 0.006979462690651417\n",
      "[step: 15815] loss: 0.006678159348666668\n",
      "[step: 15816] loss: 0.006632968317717314\n",
      "[step: 15817] loss: 0.0066491374745965\n",
      "[step: 15818] loss: 0.006644960027188063\n",
      "[step: 15819] loss: 0.006583680864423513\n",
      "[step: 15820] loss: 0.0067102862522006035\n",
      "[step: 15821] loss: 0.0065032984130084515\n",
      "[step: 15822] loss: 0.006549460347741842\n",
      "[step: 15823] loss: 0.006615295074880123\n",
      "[step: 15824] loss: 0.00646488182246685\n",
      "[step: 15825] loss: 0.006538050249218941\n",
      "[step: 15826] loss: 0.0065988642163574696\n",
      "[step: 15827] loss: 0.0064671956934034824\n",
      "[step: 15828] loss: 0.0064642769284546375\n",
      "[step: 15829] loss: 0.006479958537966013\n",
      "[step: 15830] loss: 0.006441867910325527\n",
      "[step: 15831] loss: 0.006441339384764433\n",
      "[step: 15832] loss: 0.006375069264322519\n",
      "[step: 15833] loss: 0.006428630091249943\n",
      "[step: 15834] loss: 0.0064186109229922295\n",
      "[step: 15835] loss: 0.006370087154209614\n",
      "[step: 15836] loss: 0.006384862121194601\n",
      "[step: 15837] loss: 0.006375209894031286\n",
      "[step: 15838] loss: 0.006388544104993343\n",
      "[step: 15839] loss: 0.006370641756802797\n",
      "[step: 15840] loss: 0.006336627993732691\n",
      "[step: 15841] loss: 0.006343727465718985\n",
      "[step: 15842] loss: 0.006344572175294161\n",
      "[step: 15843] loss: 0.006352492142468691\n",
      "[step: 15844] loss: 0.006367701105773449\n",
      "[step: 15845] loss: 0.006338668055832386\n",
      "[step: 15846] loss: 0.006335835438221693\n",
      "[step: 15847] loss: 0.0063334801234304905\n",
      "[step: 15848] loss: 0.006355406250804663\n",
      "[step: 15849] loss: 0.0063579813577234745\n",
      "[step: 15850] loss: 0.006408370099961758\n",
      "[step: 15851] loss: 0.006415932904928923\n",
      "[step: 15852] loss: 0.006537005305290222\n",
      "[step: 15853] loss: 0.006479720119386911\n",
      "[step: 15854] loss: 0.006407096050679684\n",
      "[step: 15855] loss: 0.006290487479418516\n",
      "[step: 15856] loss: 0.0062697106041014194\n",
      "[step: 15857] loss: 0.006282977759838104\n",
      "[step: 15858] loss: 0.006354951299726963\n",
      "[step: 15859] loss: 0.006537961773574352\n",
      "[step: 15860] loss: 0.00652510579675436\n",
      "[step: 15861] loss: 0.006583614274859428\n",
      "[step: 15862] loss: 0.006356679834425449\n",
      "[step: 15863] loss: 0.006261361297219992\n",
      "[step: 15864] loss: 0.006248771212995052\n",
      "[step: 15865] loss: 0.006318644154816866\n",
      "[step: 15866] loss: 0.006518062669783831\n",
      "[step: 15867] loss: 0.006492672488093376\n",
      "[step: 15868] loss: 0.006488077342510223\n",
      "[step: 15869] loss: 0.006262681446969509\n",
      "[step: 15870] loss: 0.006220092996954918\n",
      "[step: 15871] loss: 0.006257049739360809\n",
      "[step: 15872] loss: 0.006350904703140259\n",
      "[step: 15873] loss: 0.00650498503819108\n",
      "[step: 15874] loss: 0.006350526120513678\n",
      "[step: 15875] loss: 0.006270114798098803\n",
      "[step: 15876] loss: 0.00619917269796133\n",
      "[step: 15877] loss: 0.006206754595041275\n",
      "[step: 15878] loss: 0.006276369094848633\n",
      "[step: 15879] loss: 0.006355004850775003\n",
      "[step: 15880] loss: 0.0065681361593306065\n",
      "[step: 15881] loss: 0.00640628719702363\n",
      "[step: 15882] loss: 0.006331615149974823\n",
      "[step: 15883] loss: 0.006185896694660187\n",
      "[step: 15884] loss: 0.006243450101464987\n",
      "[step: 15885] loss: 0.0064533487893640995\n",
      "[step: 15886] loss: 0.006421990226954222\n",
      "[step: 15887] loss: 0.006399738136678934\n",
      "[step: 15888] loss: 0.006205017678439617\n",
      "[step: 15889] loss: 0.0062172673642635345\n",
      "[step: 15890] loss: 0.006428850814700127\n",
      "[step: 15891] loss: 0.0065666064620018005\n",
      "[step: 15892] loss: 0.006554275751113892\n",
      "[step: 15893] loss: 0.006188094150274992\n",
      "[step: 15894] loss: 0.00636874046176672\n",
      "[step: 15895] loss: 0.006910375785082579\n",
      "[step: 15896] loss: 0.0065092178992927074\n",
      "[step: 15897] loss: 0.006313385907560587\n",
      "[step: 15898] loss: 0.006795287597924471\n",
      "[step: 15899] loss: 0.006670556962490082\n",
      "[step: 15900] loss: 0.0064294892363250256\n",
      "[step: 15901] loss: 0.006417055614292622\n",
      "[step: 15902] loss: 0.006499385926872492\n",
      "[step: 15903] loss: 0.00706845335662365\n",
      "[step: 15904] loss: 0.006212977226823568\n",
      "[step: 15905] loss: 0.006793692708015442\n",
      "[step: 15906] loss: 0.007550076115876436\n",
      "[step: 15907] loss: 0.00683030067011714\n",
      "[step: 15908] loss: 0.007766877766698599\n",
      "[step: 15909] loss: 0.006738029420375824\n",
      "[step: 15910] loss: 0.0070897554978728294\n",
      "[step: 15911] loss: 0.006855429615825415\n",
      "[step: 15912] loss: 0.006615398917347193\n",
      "[step: 15913] loss: 0.006744290702044964\n",
      "[step: 15914] loss: 0.006366644520312548\n",
      "[step: 15915] loss: 0.0066680023446679115\n",
      "[step: 15916] loss: 0.006508541759103537\n",
      "[step: 15917] loss: 0.006501207128167152\n",
      "[step: 15918] loss: 0.006608409807085991\n",
      "[step: 15919] loss: 0.006374184973537922\n",
      "[step: 15920] loss: 0.0063712443225085735\n",
      "[step: 15921] loss: 0.006366408430039883\n",
      "[step: 15922] loss: 0.006283946800976992\n",
      "[step: 15923] loss: 0.0063686976209282875\n",
      "[step: 15924] loss: 0.006243028677999973\n",
      "[step: 15925] loss: 0.006261569447815418\n",
      "[step: 15926] loss: 0.006249572616070509\n",
      "[step: 15927] loss: 0.0062277368269860744\n",
      "[step: 15928] loss: 0.006193357519805431\n",
      "[step: 15929] loss: 0.006231269333511591\n",
      "[step: 15930] loss: 0.006189689505845308\n",
      "[step: 15931] loss: 0.0061631579883396626\n",
      "[step: 15932] loss: 0.006179071497172117\n",
      "[step: 15933] loss: 0.006186097860336304\n",
      "[step: 15934] loss: 0.0061524310149252415\n",
      "[step: 15935] loss: 0.0061410595662891865\n",
      "[step: 15936] loss: 0.00616466673091054\n",
      "[step: 15937] loss: 0.006147733889520168\n",
      "[step: 15938] loss: 0.006135965697467327\n",
      "[step: 15939] loss: 0.006104628089815378\n",
      "[step: 15940] loss: 0.0061201672069728374\n",
      "[step: 15941] loss: 0.00612718565389514\n",
      "[step: 15942] loss: 0.0061406628228724\n",
      "[step: 15943] loss: 0.006118785124272108\n",
      "[step: 15944] loss: 0.006097021978348494\n",
      "[step: 15945] loss: 0.006094041280448437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 15946] loss: 0.00607554754242301\n",
      "[step: 15947] loss: 0.006075098644942045\n",
      "[step: 15948] loss: 0.006068677641451359\n",
      "[step: 15949] loss: 0.006055341102182865\n",
      "[step: 15950] loss: 0.006062279921025038\n",
      "[step: 15951] loss: 0.006074158940464258\n",
      "[step: 15952] loss: 0.006058403756469488\n",
      "[step: 15953] loss: 0.006069920025765896\n",
      "[step: 15954] loss: 0.0060906861908733845\n",
      "[step: 15955] loss: 0.006107497029006481\n",
      "[step: 15956] loss: 0.006317503750324249\n",
      "[step: 15957] loss: 0.006586899980902672\n",
      "[step: 15958] loss: 0.006970439106225967\n",
      "[step: 15959] loss: 0.006199796684086323\n",
      "[step: 15960] loss: 0.006512477528303862\n",
      "[step: 15961] loss: 0.007822258397936821\n",
      "[step: 15962] loss: 0.0066154953092336655\n",
      "[step: 15963] loss: 0.008202637545764446\n",
      "[step: 15964] loss: 0.007996850647032261\n",
      "[step: 15965] loss: 0.011929254047572613\n",
      "[step: 15966] loss: 0.012059849686920643\n",
      "[step: 15967] loss: 0.010855652391910553\n",
      "[step: 15968] loss: 0.01436122227460146\n",
      "[step: 15969] loss: 0.013948377221822739\n",
      "[step: 15970] loss: 0.013466697186231613\n",
      "[step: 15971] loss: 0.01626949943602085\n",
      "[step: 15972] loss: 0.014922400936484337\n",
      "[step: 15973] loss: 0.014780830591917038\n",
      "[step: 15974] loss: 0.01715659536421299\n",
      "[step: 15975] loss: 0.01570095494389534\n",
      "[step: 15976] loss: 0.013030804693698883\n",
      "[step: 15977] loss: 0.014047404751181602\n",
      "[step: 15978] loss: 0.016373926773667336\n",
      "[step: 15979] loss: 0.01386541873216629\n",
      "[step: 15980] loss: 0.012381322681903839\n",
      "[step: 15981] loss: 0.01202472485601902\n",
      "[step: 15982] loss: 0.010823040269315243\n",
      "[step: 15983] loss: 0.00977420900017023\n",
      "[step: 15984] loss: 0.010448095388710499\n",
      "[step: 15985] loss: 0.010564056225121021\n",
      "[step: 15986] loss: 0.010056295432150364\n",
      "[step: 15987] loss: 0.010395335033535957\n",
      "[step: 15988] loss: 0.010528533719480038\n",
      "[step: 15989] loss: 0.010113408789038658\n",
      "[step: 15990] loss: 0.01006683986634016\n",
      "[step: 15991] loss: 0.009899635799229145\n",
      "[step: 15992] loss: 0.009418657049536705\n",
      "[step: 15993] loss: 0.009484381414949894\n",
      "[step: 15994] loss: 0.009573902934789658\n",
      "[step: 15995] loss: 0.009263850748538971\n",
      "[step: 15996] loss: 0.009040005505084991\n",
      "[step: 15997] loss: 0.00923193246126175\n",
      "[step: 15998] loss: 0.009000908583402634\n",
      "[step: 15999] loss: 0.008866415359079838\n",
      "[step: 16000] loss: 0.008993269875645638\n",
      "[step: 16001] loss: 0.008951286785304546\n",
      "[step: 16002] loss: 0.008772925473749638\n",
      "[step: 16003] loss: 0.008596736006438732\n",
      "[step: 16004] loss: 0.008748877793550491\n",
      "[step: 16005] loss: 0.00855681300163269\n",
      "[step: 16006] loss: 0.008552839048206806\n",
      "[step: 16007] loss: 0.008454220369458199\n",
      "[step: 16008] loss: 0.008491307497024536\n",
      "[step: 16009] loss: 0.008369811810553074\n",
      "[step: 16010] loss: 0.008445100858807564\n",
      "[step: 16011] loss: 0.00880693830549717\n",
      "[step: 16012] loss: 0.008655382320284843\n",
      "[step: 16013] loss: 0.008244448341429234\n",
      "[step: 16014] loss: 0.00835451204329729\n",
      "[step: 16015] loss: 0.008177250623703003\n",
      "[step: 16016] loss: 0.008289125747978687\n",
      "[step: 16017] loss: 0.008343726396560669\n",
      "[step: 16018] loss: 0.008170615881681442\n",
      "[step: 16019] loss: 0.008170641958713531\n",
      "[step: 16020] loss: 0.008095254190266132\n",
      "[step: 16021] loss: 0.008025506511330605\n",
      "[step: 16022] loss: 0.008006302639842033\n",
      "[step: 16023] loss: 0.007932111620903015\n",
      "[step: 16024] loss: 0.007724251132458448\n",
      "[step: 16025] loss: 0.007827782072126865\n",
      "[step: 16026] loss: 0.007758473977446556\n",
      "[step: 16027] loss: 0.007613734342157841\n",
      "[step: 16028] loss: 0.007723498158156872\n",
      "[step: 16029] loss: 0.0075243697501719\n",
      "[step: 16030] loss: 0.00765701150521636\n",
      "[step: 16031] loss: 0.007621781434863806\n",
      "[step: 16032] loss: 0.007464275695383549\n",
      "[step: 16033] loss: 0.007672186940908432\n",
      "[step: 16034] loss: 0.007507713511586189\n",
      "[step: 16035] loss: 0.007508301641792059\n",
      "[step: 16036] loss: 0.007733986712992191\n",
      "[step: 16037] loss: 0.008404340595006943\n",
      "[step: 16038] loss: 0.008188352920114994\n",
      "[step: 16039] loss: 0.00894920714199543\n",
      "[step: 16040] loss: 0.008906216360628605\n",
      "[step: 16041] loss: 0.008550579659640789\n",
      "[step: 16042] loss: 0.008797100745141506\n",
      "[step: 16043] loss: 0.008941787295043468\n",
      "[step: 16044] loss: 0.008483076468110085\n",
      "[step: 16045] loss: 0.008251463063061237\n",
      "[step: 16046] loss: 0.008176105096936226\n",
      "[step: 16047] loss: 0.008578171953558922\n",
      "[step: 16048] loss: 0.008103297092020512\n",
      "[step: 16049] loss: 0.0077355955727398396\n",
      "[step: 16050] loss: 0.008268310688436031\n",
      "[step: 16051] loss: 0.00788726843893528\n",
      "[step: 16052] loss: 0.00822542142122984\n",
      "[step: 16053] loss: 0.008470332249999046\n",
      "[step: 16054] loss: 0.007950804196298122\n",
      "[step: 16055] loss: 0.008573655970394611\n",
      "[step: 16056] loss: 0.008632174693048\n",
      "[step: 16057] loss: 0.007818186655640602\n",
      "[step: 16058] loss: 0.008649907074868679\n",
      "[step: 16059] loss: 0.008046652190387249\n",
      "[step: 16060] loss: 0.009506276808679104\n",
      "[step: 16061] loss: 0.007919331081211567\n",
      "[step: 16062] loss: 0.011393986642360687\n",
      "[step: 16063] loss: 0.009849727153778076\n",
      "[step: 16064] loss: 0.00910594779998064\n",
      "[step: 16065] loss: 0.011257914826273918\n",
      "[step: 16066] loss: 0.009981886483728886\n",
      "[step: 16067] loss: 0.00999197643250227\n",
      "[step: 16068] loss: 0.009740082547068596\n",
      "[step: 16069] loss: 0.009616026654839516\n",
      "[step: 16070] loss: 0.009677527472376823\n",
      "[step: 16071] loss: 0.009640160016715527\n",
      "[step: 16072] loss: 0.009567334316670895\n",
      "[step: 16073] loss: 0.009199746884405613\n",
      "[step: 16074] loss: 0.008913577534258366\n",
      "[step: 16075] loss: 0.00882574263960123\n",
      "[step: 16076] loss: 0.009196829050779343\n",
      "[step: 16077] loss: 0.008961510844528675\n",
      "[step: 16078] loss: 0.008328625932335854\n",
      "[step: 16079] loss: 0.008248805068433285\n",
      "[step: 16080] loss: 0.008317137137055397\n",
      "[step: 16081] loss: 0.008363033644855022\n",
      "[step: 16082] loss: 0.008215985260903835\n",
      "[step: 16083] loss: 0.007990336045622826\n",
      "[step: 16084] loss: 0.008067923597991467\n",
      "[step: 16085] loss: 0.008254064247012138\n",
      "[step: 16086] loss: 0.008334183134138584\n",
      "[step: 16087] loss: 0.008182892575860023\n",
      "[step: 16088] loss: 0.007761157583445311\n",
      "[step: 16089] loss: 0.007703914772719145\n",
      "[step: 16090] loss: 0.00925636850297451\n",
      "[step: 16091] loss: 0.007736419327557087\n",
      "[step: 16092] loss: 0.008081147447228432\n",
      "[step: 16093] loss: 0.008536743000149727\n",
      "[step: 16094] loss: 0.008811444975435734\n",
      "[step: 16095] loss: 0.00878911092877388\n",
      "[step: 16096] loss: 0.009473980404436588\n",
      "[step: 16097] loss: 0.00891590304672718\n",
      "[step: 16098] loss: 0.00810072012245655\n",
      "[step: 16099] loss: 0.008521877229213715\n",
      "[step: 16100] loss: 0.008885192684829235\n",
      "[step: 16101] loss: 0.008318830281496048\n",
      "[step: 16102] loss: 0.008167222142219543\n",
      "[step: 16103] loss: 0.00839347206056118\n",
      "[step: 16104] loss: 0.008550222963094711\n",
      "[step: 16105] loss: 0.008237317204475403\n",
      "[step: 16106] loss: 0.00794131774455309\n",
      "[step: 16107] loss: 0.007973648607730865\n",
      "[step: 16108] loss: 0.00805538147687912\n",
      "[step: 16109] loss: 0.00798120629042387\n",
      "[step: 16110] loss: 0.007892327383160591\n",
      "[step: 16111] loss: 0.007906769402325153\n",
      "[step: 16112] loss: 0.007890685461461544\n",
      "[step: 16113] loss: 0.00783204659819603\n",
      "[step: 16114] loss: 0.007796982768923044\n",
      "[step: 16115] loss: 0.007758802734315395\n",
      "[step: 16116] loss: 0.0077025676146149635\n",
      "[step: 16117] loss: 0.007733565289527178\n",
      "[step: 16118] loss: 0.007762529421597719\n",
      "[step: 16119] loss: 0.007659350987523794\n",
      "[step: 16120] loss: 0.007568533066660166\n",
      "[step: 16121] loss: 0.007610606960952282\n",
      "[step: 16122] loss: 0.007601380813866854\n",
      "[step: 16123] loss: 0.007564861327409744\n",
      "[step: 16124] loss: 0.0075594172812998295\n",
      "[step: 16125] loss: 0.007515538949519396\n",
      "[step: 16126] loss: 0.007490973919630051\n",
      "[step: 16127] loss: 0.007508025970309973\n",
      "[step: 16128] loss: 0.007482047192752361\n",
      "[step: 16129] loss: 0.00745304673910141\n",
      "[step: 16130] loss: 0.007451692596077919\n",
      "[step: 16131] loss: 0.007437249179929495\n",
      "[step: 16132] loss: 0.007422030437737703\n",
      "[step: 16133] loss: 0.007404825650155544\n",
      "[step: 16134] loss: 0.00738820806145668\n",
      "[step: 16135] loss: 0.007386753801256418\n",
      "[step: 16136] loss: 0.00737041374668479\n",
      "[step: 16137] loss: 0.007350733038038015\n",
      "[step: 16138] loss: 0.007346145808696747\n",
      "[step: 16139] loss: 0.007329063024371862\n",
      "[step: 16140] loss: 0.007319737691432238\n",
      "[step: 16141] loss: 0.007316498551517725\n",
      "[step: 16142] loss: 0.007293179165571928\n",
      "[step: 16143] loss: 0.0072798659093678\n",
      "[step: 16144] loss: 0.007280820515006781\n",
      "[step: 16145] loss: 0.007266823668032885\n",
      "[step: 16146] loss: 0.007253123447299004\n",
      "[step: 16147] loss: 0.007243640720844269\n",
      "[step: 16148] loss: 0.007232481613755226\n",
      "[step: 16149] loss: 0.007225226145237684\n",
      "[step: 16150] loss: 0.0072159660048782825\n",
      "[step: 16151] loss: 0.0072037712670862675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 16152] loss: 0.0071936738677322865\n",
      "[step: 16153] loss: 0.007186271715909243\n",
      "[step: 16154] loss: 0.00717796990647912\n",
      "[step: 16155] loss: 0.007166682276874781\n",
      "[step: 16156] loss: 0.007158088032156229\n",
      "[step: 16157] loss: 0.007150318007916212\n",
      "[step: 16158] loss: 0.007140811067074537\n",
      "[step: 16159] loss: 0.0071329763159155846\n",
      "[step: 16160] loss: 0.007123476359993219\n",
      "[step: 16161] loss: 0.007115413900464773\n",
      "[step: 16162] loss: 0.007108206860721111\n",
      "[step: 16163] loss: 0.007098505273461342\n",
      "[step: 16164] loss: 0.007090664468705654\n",
      "[step: 16165] loss: 0.007082599680870771\n",
      "[step: 16166] loss: 0.0070746662095189095\n",
      "[step: 16167] loss: 0.0070667448453605175\n",
      "[step: 16168] loss: 0.007058157119899988\n",
      "[step: 16169] loss: 0.0070508201606571674\n",
      "[step: 16170] loss: 0.007042957004159689\n",
      "[step: 16171] loss: 0.007035092916339636\n",
      "[step: 16172] loss: 0.007027126848697662\n",
      "[step: 16173] loss: 0.0070195975713431835\n",
      "[step: 16174] loss: 0.007012231275439262\n",
      "[step: 16175] loss: 0.00700435321778059\n",
      "[step: 16176] loss: 0.006996820215135813\n",
      "[step: 16177] loss: 0.006989278830587864\n",
      "[step: 16178] loss: 0.006981994956731796\n",
      "[step: 16179] loss: 0.006974360439926386\n",
      "[step: 16180] loss: 0.006966901011765003\n",
      "[step: 16181] loss: 0.006959480233490467\n",
      "[step: 16182] loss: 0.006952208001166582\n",
      "[step: 16183] loss: 0.006944694556295872\n",
      "[step: 16184] loss: 0.006937268190085888\n",
      "[step: 16185] loss: 0.006929968483746052\n",
      "[step: 16186] loss: 0.006922564469277859\n",
      "[step: 16187] loss: 0.006915236823260784\n",
      "[step: 16188] loss: 0.006907723844051361\n",
      "[step: 16189] loss: 0.006900472566485405\n",
      "[step: 16190] loss: 0.006893066689372063\n",
      "[step: 16191] loss: 0.006885690614581108\n",
      "[step: 16192] loss: 0.006878272630274296\n",
      "[step: 16193] loss: 0.006870879791676998\n",
      "[step: 16194] loss: 0.006863525602966547\n",
      "[step: 16195] loss: 0.006856051739305258\n",
      "[step: 16196] loss: 0.006848614662885666\n",
      "[step: 16197] loss: 0.006841183174401522\n",
      "[step: 16198] loss: 0.0068337274715304375\n",
      "[step: 16199] loss: 0.006826235447078943\n",
      "[step: 16200] loss: 0.0068187289871275425\n",
      "[step: 16201] loss: 0.0068112025037407875\n",
      "[step: 16202] loss: 0.006803686730563641\n",
      "[step: 16203] loss: 0.006796097848564386\n",
      "[step: 16204] loss: 0.006788522005081177\n",
      "[step: 16205] loss: 0.006780905183404684\n",
      "[step: 16206] loss: 0.006773300003260374\n",
      "[step: 16207] loss: 0.006765630096197128\n",
      "[step: 16208] loss: 0.0067579615861177444\n",
      "[step: 16209] loss: 0.0067502534948289394\n",
      "[step: 16210] loss: 0.006742531433701515\n",
      "[step: 16211] loss: 0.006734777241945267\n",
      "[step: 16212] loss: 0.0067270053550601006\n",
      "[step: 16213] loss: 0.006719215307384729\n",
      "[step: 16214] loss: 0.006711406633257866\n",
      "[step: 16215] loss: 0.006703636609017849\n",
      "[step: 16216] loss: 0.00669614365324378\n",
      "[step: 16217] loss: 0.006690318696200848\n",
      "[step: 16218] loss: 0.00669625960290432\n",
      "[step: 16219] loss: 0.006740469019860029\n",
      "[step: 16220] loss: 0.006871778517961502\n",
      "[step: 16221] loss: 0.00691770575940609\n",
      "[step: 16222] loss: 0.006813017651438713\n",
      "[step: 16223] loss: 0.006822403986006975\n",
      "[step: 16224] loss: 0.006815173197537661\n",
      "[step: 16225] loss: 0.006887850817292929\n",
      "[step: 16226] loss: 0.006808808539062738\n",
      "[step: 16227] loss: 0.006659456994384527\n",
      "[step: 16228] loss: 0.007149390410631895\n",
      "[step: 16229] loss: 0.006776816677302122\n",
      "[step: 16230] loss: 0.007194728124886751\n",
      "[step: 16231] loss: 0.0074171265587210655\n",
      "[step: 16232] loss: 0.007235484663397074\n",
      "[step: 16233] loss: 0.007162453141063452\n",
      "[step: 16234] loss: 0.00713961198925972\n",
      "[step: 16235] loss: 0.0070593529380857944\n",
      "[step: 16236] loss: 0.007043956313282251\n",
      "[step: 16237] loss: 0.007041740231215954\n",
      "[step: 16238] loss: 0.006882501300424337\n",
      "[step: 16239] loss: 0.006788656581193209\n",
      "[step: 16240] loss: 0.006827332079410553\n",
      "[step: 16241] loss: 0.006830161903053522\n",
      "[step: 16242] loss: 0.006850644014775753\n",
      "[step: 16243] loss: 0.006832360755652189\n",
      "[step: 16244] loss: 0.006795799359679222\n",
      "[step: 16245] loss: 0.006707200314849615\n",
      "[step: 16246] loss: 0.006675933487713337\n",
      "[step: 16247] loss: 0.006664353422820568\n",
      "[step: 16248] loss: 0.006684277206659317\n",
      "[step: 16249] loss: 0.006684963591396809\n",
      "[step: 16250] loss: 0.006649978458881378\n",
      "[step: 16251] loss: 0.006631509866565466\n",
      "[step: 16252] loss: 0.00659280177205801\n",
      "[step: 16253] loss: 0.006572003476321697\n",
      "[step: 16254] loss: 0.006582486908882856\n",
      "[step: 16255] loss: 0.00662561971694231\n",
      "[step: 16256] loss: 0.006577837746590376\n",
      "[step: 16257] loss: 0.006767211016267538\n",
      "[step: 16258] loss: 0.006807737518101931\n",
      "[step: 16259] loss: 0.0069836718030273914\n",
      "[step: 16260] loss: 0.006906885653734207\n",
      "[step: 16261] loss: 0.006920716725289822\n",
      "[step: 16262] loss: 0.006854136940091848\n",
      "[step: 16263] loss: 0.006739130709320307\n",
      "[step: 16264] loss: 0.006772801745682955\n",
      "[step: 16265] loss: 0.006679650396108627\n",
      "[step: 16266] loss: 0.006705139763653278\n",
      "[step: 16267] loss: 0.006605441682040691\n",
      "[step: 16268] loss: 0.006646824534982443\n",
      "[step: 16269] loss: 0.00679189944639802\n",
      "[step: 16270] loss: 0.006724901497364044\n",
      "[step: 16271] loss: 0.006761671043932438\n",
      "[step: 16272] loss: 0.006667628884315491\n",
      "[step: 16273] loss: 0.006574247032403946\n",
      "[step: 16274] loss: 0.0066154696978628635\n",
      "[step: 16275] loss: 0.006737252231687307\n",
      "[step: 16276] loss: 0.0066500939428806305\n",
      "[step: 16277] loss: 0.006825052201747894\n",
      "[step: 16278] loss: 0.006832515355199575\n",
      "[step: 16279] loss: 0.006838546600192785\n",
      "[step: 16280] loss: 0.006891043856739998\n",
      "[step: 16281] loss: 0.00684310169890523\n",
      "[step: 16282] loss: 0.006699052173644304\n",
      "[step: 16283] loss: 0.006806384306401014\n",
      "[step: 16284] loss: 0.006799998227506876\n",
      "[step: 16285] loss: 0.006693725008517504\n",
      "[step: 16286] loss: 0.006852850317955017\n",
      "[step: 16287] loss: 0.006779895164072514\n",
      "[step: 16288] loss: 0.006618274375796318\n",
      "[step: 16289] loss: 0.006831690669059753\n",
      "[step: 16290] loss: 0.006685127038508654\n",
      "[step: 16291] loss: 0.0066454228945076466\n",
      "[step: 16292] loss: 0.0068103545345366\n",
      "[step: 16293] loss: 0.006596170831471682\n",
      "[step: 16294] loss: 0.006547102704644203\n",
      "[step: 16295] loss: 0.006710777524858713\n",
      "[step: 16296] loss: 0.006554866209626198\n",
      "[step: 16297] loss: 0.006490782368928194\n",
      "[step: 16298] loss: 0.0066335443407297134\n",
      "[step: 16299] loss: 0.006569997873157263\n",
      "[step: 16300] loss: 0.0065740495920181274\n",
      "[step: 16301] loss: 0.0068269516341388226\n",
      "[step: 16302] loss: 0.00672651594504714\n",
      "[step: 16303] loss: 0.006644494365900755\n",
      "[step: 16304] loss: 0.006607542280107737\n",
      "[step: 16305] loss: 0.006597200408577919\n",
      "[step: 16306] loss: 0.006569993682205677\n",
      "[step: 16307] loss: 0.006803055293858051\n",
      "[step: 16308] loss: 0.006603604182600975\n",
      "[step: 16309] loss: 0.006654879543930292\n",
      "[step: 16310] loss: 0.0067006624303758144\n",
      "[step: 16311] loss: 0.006535214371979237\n",
      "[step: 16312] loss: 0.0064802817068994045\n",
      "[step: 16313] loss: 0.00659366836771369\n",
      "[step: 16314] loss: 0.006559496745467186\n",
      "[step: 16315] loss: 0.006431751884520054\n",
      "[step: 16316] loss: 0.006433192640542984\n",
      "[step: 16317] loss: 0.006517029367387295\n",
      "[step: 16318] loss: 0.006514215841889381\n",
      "[step: 16319] loss: 0.006428409367799759\n",
      "[step: 16320] loss: 0.006405489984899759\n",
      "[step: 16321] loss: 0.006371691823005676\n",
      "[step: 16322] loss: 0.006523358169943094\n",
      "[step: 16323] loss: 0.006441744975745678\n",
      "[step: 16324] loss: 0.006548373028635979\n",
      "[step: 16325] loss: 0.0065588802099227905\n",
      "[step: 16326] loss: 0.006569989956915379\n",
      "[step: 16327] loss: 0.006420487072318792\n",
      "[step: 16328] loss: 0.0063666654750704765\n",
      "[step: 16329] loss: 0.006397816352546215\n",
      "[step: 16330] loss: 0.006451049819588661\n",
      "[step: 16331] loss: 0.006391253788024187\n",
      "[step: 16332] loss: 0.006339432671666145\n",
      "[step: 16333] loss: 0.00635624211281538\n",
      "[step: 16334] loss: 0.006366525311022997\n",
      "[step: 16335] loss: 0.006293138489127159\n",
      "[step: 16336] loss: 0.006280961912125349\n",
      "[step: 16337] loss: 0.006265411619096994\n",
      "[step: 16338] loss: 0.006266461685299873\n",
      "[step: 16339] loss: 0.006222578696906567\n",
      "[step: 16340] loss: 0.006236688233911991\n",
      "[step: 16341] loss: 0.006235042121261358\n",
      "[step: 16342] loss: 0.006202107295393944\n",
      "[step: 16343] loss: 0.006286499090492725\n",
      "[step: 16344] loss: 0.006207708269357681\n",
      "[step: 16345] loss: 0.0066043660044670105\n",
      "[step: 16346] loss: 0.006413526367396116\n",
      "[step: 16347] loss: 0.006667605135589838\n",
      "[step: 16348] loss: 0.006658526603132486\n",
      "[step: 16349] loss: 0.0065597835928201675\n",
      "[step: 16350] loss: 0.006375995464622974\n",
      "[step: 16351] loss: 0.006313306279480457\n",
      "[step: 16352] loss: 0.006690379697829485\n",
      "[step: 16353] loss: 0.006583864334970713\n",
      "[step: 16354] loss: 0.006736994255334139\n",
      "[step: 16355] loss: 0.006685501895844936\n",
      "[step: 16356] loss: 0.006549813784658909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 16357] loss: 0.006616738624870777\n",
      "[step: 16358] loss: 0.006495683453977108\n",
      "[step: 16359] loss: 0.006423762533813715\n",
      "[step: 16360] loss: 0.006494170520454645\n",
      "[step: 16361] loss: 0.00645966874435544\n",
      "[step: 16362] loss: 0.006697601638734341\n",
      "[step: 16363] loss: 0.007234680000692606\n",
      "[step: 16364] loss: 0.0077734580263495445\n",
      "[step: 16365] loss: 0.006955452263355255\n",
      "[step: 16366] loss: 0.0100508788600564\n",
      "[step: 16367] loss: 0.006897369399666786\n",
      "[step: 16368] loss: 0.007985160686075687\n",
      "[step: 16369] loss: 0.00656536128371954\n",
      "[step: 16370] loss: 0.008091065101325512\n",
      "[step: 16371] loss: 0.006886126473546028\n",
      "[step: 16372] loss: 0.0075858221389353275\n",
      "[step: 16373] loss: 0.007137902081012726\n",
      "[step: 16374] loss: 0.007448185235261917\n",
      "[step: 16375] loss: 0.006753482855856419\n",
      "[step: 16376] loss: 0.007063032127916813\n",
      "[step: 16377] loss: 0.007032332010567188\n",
      "[step: 16378] loss: 0.006427874322980642\n",
      "[step: 16379] loss: 0.007062132935971022\n",
      "[step: 16380] loss: 0.006479080766439438\n",
      "[step: 16381] loss: 0.006634046323597431\n",
      "[step: 16382] loss: 0.006442582700401545\n",
      "[step: 16383] loss: 0.0065229544416069984\n",
      "[step: 16384] loss: 0.00643438333645463\n",
      "[step: 16385] loss: 0.006548135541379452\n",
      "[step: 16386] loss: 0.006787142250686884\n",
      "[step: 16387] loss: 0.0066706412471830845\n",
      "[step: 16388] loss: 0.006421296391636133\n",
      "[step: 16389] loss: 0.006351006682962179\n",
      "[step: 16390] loss: 0.006469693034887314\n",
      "[step: 16391] loss: 0.006551234517246485\n",
      "[step: 16392] loss: 0.006553794257342815\n",
      "[step: 16393] loss: 0.006488954648375511\n",
      "[step: 16394] loss: 0.006480604410171509\n",
      "[step: 16395] loss: 0.0064813317731022835\n",
      "[step: 16396] loss: 0.0063268812373280525\n",
      "[step: 16397] loss: 0.006335198879241943\n",
      "[step: 16398] loss: 0.00633743591606617\n",
      "[step: 16399] loss: 0.006339441053569317\n",
      "[step: 16400] loss: 0.006193192210048437\n",
      "[step: 16401] loss: 0.0061934092082083225\n",
      "[step: 16402] loss: 0.006309857591986656\n",
      "[step: 16403] loss: 0.0061073899269104\n",
      "[step: 16404] loss: 0.006219520699232817\n",
      "[step: 16405] loss: 0.006479460746049881\n",
      "[step: 16406] loss: 0.006286478601396084\n",
      "[step: 16407] loss: 0.0063799163326621056\n",
      "[step: 16408] loss: 0.006538265850394964\n",
      "[step: 16409] loss: 0.006384115666151047\n",
      "[step: 16410] loss: 0.006315968465059996\n",
      "[step: 16411] loss: 0.006579335313290358\n",
      "[step: 16412] loss: 0.00650857063010335\n",
      "[step: 16413] loss: 0.006188863422721624\n",
      "[step: 16414] loss: 0.006370775867253542\n",
      "[step: 16415] loss: 0.006507398094981909\n",
      "[step: 16416] loss: 0.006266342476010323\n",
      "[step: 16417] loss: 0.006508169695734978\n",
      "[step: 16418] loss: 0.006337110884487629\n",
      "[step: 16419] loss: 0.006217540707439184\n",
      "[step: 16420] loss: 0.006429251283407211\n",
      "[step: 16421] loss: 0.0066674863919615746\n",
      "[step: 16422] loss: 0.007334780879318714\n",
      "[step: 16423] loss: 0.006486020982265472\n",
      "[step: 16424] loss: 0.007927586324512959\n",
      "[step: 16425] loss: 0.013476844877004623\n",
      "[step: 16426] loss: 0.013568379916250706\n",
      "[step: 16427] loss: 0.011337018571794033\n",
      "[step: 16428] loss: 0.011767703108489513\n",
      "[step: 16429] loss: 0.009166580624878407\n",
      "[step: 16430] loss: 0.0089408652856946\n",
      "[step: 16431] loss: 0.009756630286574364\n",
      "[step: 16432] loss: 0.009661342017352581\n",
      "[step: 16433] loss: 0.0088990842923522\n",
      "[step: 16434] loss: 0.008987125009298325\n",
      "[step: 16435] loss: 0.009163684211671352\n",
      "[step: 16436] loss: 0.008201067335903645\n",
      "[step: 16437] loss: 0.0077963946387171745\n",
      "[step: 16438] loss: 0.008224223740398884\n",
      "[step: 16439] loss: 0.007703747600317001\n",
      "[step: 16440] loss: 0.007399736437946558\n",
      "[step: 16441] loss: 0.00742710055783391\n",
      "[step: 16442] loss: 0.007221767213195562\n",
      "[step: 16443] loss: 0.006782651413232088\n",
      "[step: 16444] loss: 0.0072983428835868835\n",
      "[step: 16445] loss: 0.0073039354756474495\n",
      "[step: 16446] loss: 0.007525126915425062\n",
      "[step: 16447] loss: 0.006893405225127935\n",
      "[step: 16448] loss: 0.00718897907063365\n",
      "[step: 16449] loss: 0.007287223823368549\n",
      "[step: 16450] loss: 0.007600710727274418\n",
      "[step: 16451] loss: 0.006946086883544922\n",
      "[step: 16452] loss: 0.007718639448285103\n",
      "[step: 16453] loss: 0.008998379111289978\n",
      "[step: 16454] loss: 0.00717825535684824\n",
      "[step: 16455] loss: 0.00907452218234539\n",
      "[step: 16456] loss: 0.006818474270403385\n",
      "[step: 16457] loss: 0.007657174486666918\n",
      "[step: 16458] loss: 0.007151938043534756\n",
      "[step: 16459] loss: 0.007025558035820723\n",
      "[step: 16460] loss: 0.0065157487988471985\n",
      "[step: 16461] loss: 0.0065928190015256405\n",
      "[step: 16462] loss: 0.0065734051167964935\n",
      "[step: 16463] loss: 0.006595338229089975\n",
      "[step: 16464] loss: 0.006562682334333658\n",
      "[step: 16465] loss: 0.006405271589756012\n",
      "[step: 16466] loss: 0.006497547961771488\n",
      "[step: 16467] loss: 0.00636278884485364\n",
      "[step: 16468] loss: 0.006423000246286392\n",
      "[step: 16469] loss: 0.006381385028362274\n",
      "[step: 16470] loss: 0.006357818841934204\n",
      "[step: 16471] loss: 0.006331963464617729\n",
      "[step: 16472] loss: 0.006279225926846266\n",
      "[step: 16473] loss: 0.006239932030439377\n",
      "[step: 16474] loss: 0.006244781892746687\n",
      "[step: 16475] loss: 0.006211280357092619\n",
      "[step: 16476] loss: 0.006182197947055101\n",
      "[step: 16477] loss: 0.006149640306830406\n",
      "[step: 16478] loss: 0.006140302866697311\n",
      "[step: 16479] loss: 0.006111465394496918\n",
      "[step: 16480] loss: 0.006109233014285564\n",
      "[step: 16481] loss: 0.00609076302498579\n",
      "[step: 16482] loss: 0.006075361743569374\n",
      "[step: 16483] loss: 0.006072608288377523\n",
      "[step: 16484] loss: 0.006016728468239307\n",
      "[step: 16485] loss: 0.006037025712430477\n",
      "[step: 16486] loss: 0.006025505252182484\n",
      "[step: 16487] loss: 0.005984701216220856\n",
      "[step: 16488] loss: 0.005978285800665617\n",
      "[step: 16489] loss: 0.005994288716465235\n",
      "[step: 16490] loss: 0.005940278992056847\n",
      "[step: 16491] loss: 0.005936401896178722\n",
      "[step: 16492] loss: 0.005953348241746426\n",
      "[step: 16493] loss: 0.005941573064774275\n",
      "[step: 16494] loss: 0.005991846323013306\n",
      "[step: 16495] loss: 0.005918714217841625\n",
      "[step: 16496] loss: 0.005913897883147001\n",
      "[step: 16497] loss: 0.005985585507005453\n",
      "[step: 16498] loss: 0.00601331377401948\n",
      "[step: 16499] loss: 0.006174381822347641\n",
      "[step: 16500] loss: 0.00630231574177742\n",
      "[step: 16501] loss: 0.006111325696110725\n",
      "[step: 16502] loss: 0.005977535620331764\n",
      "[step: 16503] loss: 0.006359871942549944\n",
      "[step: 16504] loss: 0.006344566587358713\n",
      "[step: 16505] loss: 0.006630440708249807\n",
      "[step: 16506] loss: 0.0065826550126075745\n",
      "[step: 16507] loss: 0.006628814153373241\n",
      "[step: 16508] loss: 0.006410880014300346\n",
      "[step: 16509] loss: 0.00621242867782712\n",
      "[step: 16510] loss: 0.006033126264810562\n",
      "[step: 16511] loss: 0.006250767502933741\n",
      "[step: 16512] loss: 0.00617566704750061\n",
      "[step: 16513] loss: 0.0061519998125731945\n",
      "[step: 16514] loss: 0.006005376577377319\n",
      "[step: 16515] loss: 0.006068223621696234\n",
      "[step: 16516] loss: 0.006178659852594137\n",
      "[step: 16517] loss: 0.006845204159617424\n",
      "[step: 16518] loss: 0.006371812429279089\n",
      "[step: 16519] loss: 0.0065476540476083755\n",
      "[step: 16520] loss: 0.006495233159512281\n",
      "[step: 16521] loss: 0.006299368105828762\n",
      "[step: 16522] loss: 0.0072478982619941235\n",
      "[step: 16523] loss: 0.006196308881044388\n",
      "[step: 16524] loss: 0.009311688132584095\n",
      "[step: 16525] loss: 0.009596549905836582\n",
      "[step: 16526] loss: 0.012189902365207672\n",
      "[step: 16527] loss: 0.011603704653680325\n",
      "[step: 16528] loss: 0.01113841775804758\n",
      "[step: 16529] loss: 0.010090273804962635\n",
      "[step: 16530] loss: 0.0088919373229146\n",
      "[step: 16531] loss: 0.009482732973992825\n",
      "[step: 16532] loss: 0.00949506089091301\n",
      "[step: 16533] loss: 0.00923395436257124\n",
      "[step: 16534] loss: 0.009226564317941666\n",
      "[step: 16535] loss: 0.00907345674932003\n",
      "[step: 16536] loss: 0.009344594553112984\n",
      "[step: 16537] loss: 0.008597835898399353\n",
      "[step: 16538] loss: 0.008626016788184643\n",
      "[step: 16539] loss: 0.008558453060686588\n",
      "[step: 16540] loss: 0.008466899394989014\n",
      "[step: 16541] loss: 0.008884027600288391\n",
      "[step: 16542] loss: 0.008554368279874325\n",
      "[step: 16543] loss: 0.008650125935673714\n",
      "[step: 16544] loss: 0.00863594375550747\n",
      "[step: 16545] loss: 0.008618460968136787\n",
      "[step: 16546] loss: 0.008646453730762005\n",
      "[step: 16547] loss: 0.008693952113389969\n",
      "[step: 16548] loss: 0.00851264875382185\n",
      "[step: 16549] loss: 0.008297228254377842\n",
      "[step: 16550] loss: 0.008317080326378345\n",
      "[step: 16551] loss: 0.008110081776976585\n",
      "[step: 16552] loss: 0.008240110240876675\n",
      "[step: 16553] loss: 0.008244390599429607\n",
      "[step: 16554] loss: 0.008153912611305714\n",
      "[step: 16555] loss: 0.007977823726832867\n",
      "[step: 16556] loss: 0.007874390110373497\n",
      "[step: 16557] loss: 0.007783228065818548\n",
      "[step: 16558] loss: 0.007775861769914627\n",
      "[step: 16559] loss: 0.00787239521741867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 16560] loss: 0.008103210479021072\n",
      "[step: 16561] loss: 0.008257213048636913\n",
      "[step: 16562] loss: 0.008089541457593441\n",
      "[step: 16563] loss: 0.007949281483888626\n",
      "[step: 16564] loss: 0.007787564769387245\n",
      "[step: 16565] loss: 0.007673091255128384\n",
      "[step: 16566] loss: 0.007762540597468615\n",
      "[step: 16567] loss: 0.008148184977471828\n",
      "[step: 16568] loss: 0.007738695479929447\n",
      "[step: 16569] loss: 0.007840357720851898\n",
      "[step: 16570] loss: 0.007930997759103775\n",
      "[step: 16571] loss: 0.008048354648053646\n",
      "[step: 16572] loss: 0.008000911213457584\n",
      "[step: 16573] loss: 0.007748601958155632\n",
      "[step: 16574] loss: 0.00759928859770298\n",
      "[step: 16575] loss: 0.007482436485588551\n",
      "[step: 16576] loss: 0.0074672033078968525\n",
      "[step: 16577] loss: 0.008823981508612633\n",
      "[step: 16578] loss: 0.0088658407330513\n",
      "[step: 16579] loss: 0.008761960081756115\n",
      "[step: 16580] loss: 0.008393608033657074\n",
      "[step: 16581] loss: 0.00770953856408596\n",
      "[step: 16582] loss: 0.007465254049748182\n",
      "[step: 16583] loss: 0.007969874888658524\n",
      "[step: 16584] loss: 0.007973061874508858\n",
      "[step: 16585] loss: 0.007694598753005266\n",
      "[step: 16586] loss: 0.007950257509946823\n",
      "[step: 16587] loss: 0.007455545011907816\n",
      "[step: 16588] loss: 0.007540031801909208\n",
      "[step: 16589] loss: 0.007867898792028427\n",
      "[step: 16590] loss: 0.007433165330439806\n",
      "[step: 16591] loss: 0.007162594702094793\n",
      "[step: 16592] loss: 0.007404890842735767\n",
      "[step: 16593] loss: 0.007209738250821829\n",
      "[step: 16594] loss: 0.007282373495399952\n",
      "[step: 16595] loss: 0.007270046975463629\n",
      "[step: 16596] loss: 0.006920370273292065\n",
      "[step: 16597] loss: 0.007022583391517401\n",
      "[step: 16598] loss: 0.0069304462522268295\n",
      "[step: 16599] loss: 0.006829105317592621\n",
      "[step: 16600] loss: 0.006843665149062872\n",
      "[step: 16601] loss: 0.0067564211785793304\n",
      "[step: 16602] loss: 0.00679032551124692\n",
      "[step: 16603] loss: 0.006629828829318285\n",
      "[step: 16604] loss: 0.006699885241687298\n",
      "[step: 16605] loss: 0.006690187379717827\n",
      "[step: 16606] loss: 0.006593257188796997\n",
      "[step: 16607] loss: 0.006576391868293285\n",
      "[step: 16608] loss: 0.006535712629556656\n",
      "[step: 16609] loss: 0.006531707942485809\n",
      "[step: 16610] loss: 0.006488049868494272\n",
      "[step: 16611] loss: 0.006497113034129143\n",
      "[step: 16612] loss: 0.006424201186746359\n",
      "[step: 16613] loss: 0.006471017375588417\n",
      "[step: 16614] loss: 0.006389179266989231\n",
      "[step: 16615] loss: 0.006384850945323706\n",
      "[step: 16616] loss: 0.006378813646733761\n",
      "[step: 16617] loss: 0.006356355734169483\n",
      "[step: 16618] loss: 0.006347296759486198\n",
      "[step: 16619] loss: 0.006316634826362133\n",
      "[step: 16620] loss: 0.006311616860330105\n",
      "[step: 16621] loss: 0.006282812915742397\n",
      "[step: 16622] loss: 0.006276268977671862\n",
      "[step: 16623] loss: 0.006243813317269087\n",
      "[step: 16624] loss: 0.006251037120819092\n",
      "[step: 16625] loss: 0.006221751682460308\n",
      "[step: 16626] loss: 0.0062118228524923325\n",
      "[step: 16627] loss: 0.006186571437865496\n",
      "[step: 16628] loss: 0.006178269162774086\n",
      "[step: 16629] loss: 0.006165300030261278\n",
      "[step: 16630] loss: 0.006151081062853336\n",
      "[step: 16631] loss: 0.006136109586805105\n",
      "[step: 16632] loss: 0.006124819628894329\n",
      "[step: 16633] loss: 0.0061100623570382595\n",
      "[step: 16634] loss: 0.006092510186135769\n",
      "[step: 16635] loss: 0.0060827601701021194\n",
      "[step: 16636] loss: 0.006069710943847895\n",
      "[step: 16637] loss: 0.006060085725039244\n",
      "[step: 16638] loss: 0.00604071281850338\n",
      "[step: 16639] loss: 0.006031551863998175\n",
      "[step: 16640] loss: 0.006018141750246286\n",
      "[step: 16641] loss: 0.006005913484841585\n",
      "[step: 16642] loss: 0.005994571838527918\n",
      "[step: 16643] loss: 0.005981645546853542\n",
      "[step: 16644] loss: 0.005972064565867186\n",
      "[step: 16645] loss: 0.005957814399152994\n",
      "[step: 16646] loss: 0.005946670193225145\n",
      "[step: 16647] loss: 0.005936143454164267\n",
      "[step: 16648] loss: 0.00592477573081851\n",
      "[step: 16649] loss: 0.005913968663662672\n",
      "[step: 16650] loss: 0.005902842618525028\n",
      "[step: 16651] loss: 0.005892541259527206\n",
      "[step: 16652] loss: 0.005882332567125559\n",
      "[step: 16653] loss: 0.005872153677046299\n",
      "[step: 16654] loss: 0.005861727520823479\n",
      "[step: 16655] loss: 0.005852900445461273\n",
      "[step: 16656] loss: 0.005842553451657295\n",
      "[step: 16657] loss: 0.005832832772284746\n",
      "[step: 16658] loss: 0.005823861341923475\n",
      "[step: 16659] loss: 0.00581433204934001\n",
      "[step: 16660] loss: 0.005805356428027153\n",
      "[step: 16661] loss: 0.005796784069389105\n",
      "[step: 16662] loss: 0.005787981674075127\n",
      "[step: 16663] loss: 0.00577934505417943\n",
      "[step: 16664] loss: 0.005771019961684942\n",
      "[step: 16665] loss: 0.0057625798508524895\n",
      "[step: 16666] loss: 0.005754369776695967\n",
      "[step: 16667] loss: 0.00574627798050642\n",
      "[step: 16668] loss: 0.005738135892897844\n",
      "[step: 16669] loss: 0.005729983560740948\n",
      "[step: 16670] loss: 0.005722195375710726\n",
      "[step: 16671] loss: 0.005714492406696081\n",
      "[step: 16672] loss: 0.005706689786165953\n",
      "[step: 16673] loss: 0.005698878318071365\n",
      "[step: 16674] loss: 0.005691157653927803\n",
      "[step: 16675] loss: 0.005683519411832094\n",
      "[step: 16676] loss: 0.005675892811268568\n",
      "[step: 16677] loss: 0.005668371915817261\n",
      "[step: 16678] loss: 0.0056608873419463634\n",
      "[step: 16679] loss: 0.00565358018502593\n",
      "[step: 16680] loss: 0.0056468104012310505\n",
      "[step: 16681] loss: 0.005642468109726906\n",
      "[step: 16682] loss: 0.005649274215102196\n",
      "[step: 16683] loss: 0.005707428324967623\n",
      "[step: 16684] loss: 0.005944299045950174\n",
      "[step: 16685] loss: 0.00615641288459301\n",
      "[step: 16686] loss: 0.006266945507377386\n",
      "[step: 16687] loss: 0.00630540493875742\n",
      "[step: 16688] loss: 0.005734668113291264\n",
      "[step: 16689] loss: 0.005759992636740208\n",
      "[step: 16690] loss: 0.006139438599348068\n",
      "[step: 16691] loss: 0.005976817570626736\n",
      "[step: 16692] loss: 0.005728635471314192\n",
      "[step: 16693] loss: 0.005836227908730507\n",
      "[step: 16694] loss: 0.005814572796225548\n",
      "[step: 16695] loss: 0.005668134894222021\n",
      "[step: 16696] loss: 0.005684783682227135\n",
      "[step: 16697] loss: 0.005979323759675026\n",
      "[step: 16698] loss: 0.006354320794343948\n",
      "[step: 16699] loss: 0.00596807012334466\n",
      "[step: 16700] loss: 0.006026045419275761\n",
      "[step: 16701] loss: 0.006344903726130724\n",
      "[step: 16702] loss: 0.005849609617143869\n",
      "[step: 16703] loss: 0.0070943767204880714\n",
      "[step: 16704] loss: 0.007421534508466721\n",
      "[step: 16705] loss: 0.007689219433814287\n",
      "[step: 16706] loss: 0.006722383666783571\n",
      "[step: 16707] loss: 0.009132160805165768\n",
      "[step: 16708] loss: 0.006969410926103592\n",
      "[step: 16709] loss: 0.008331194519996643\n",
      "[step: 16710] loss: 0.006365319713950157\n",
      "[step: 16711] loss: 0.007771910168230534\n",
      "[step: 16712] loss: 0.0066581047140061855\n",
      "[step: 16713] loss: 0.008441313169896603\n",
      "[step: 16714] loss: 0.009053369984030724\n",
      "[step: 16715] loss: 0.007704060059040785\n",
      "[step: 16716] loss: 0.009024525061249733\n",
      "[step: 16717] loss: 0.00837563443928957\n",
      "[step: 16718] loss: 0.008223571814596653\n",
      "[step: 16719] loss: 0.009746363386511803\n",
      "[step: 16720] loss: 0.013784029521048069\n",
      "[step: 16721] loss: 0.008137277327477932\n",
      "[step: 16722] loss: 0.01609891839325428\n",
      "[step: 16723] loss: 0.011162236332893372\n",
      "[step: 16724] loss: 0.012203694321215153\n",
      "[step: 16725] loss: 0.012433943338692188\n",
      "[step: 16726] loss: 0.01787487231194973\n",
      "[step: 16727] loss: 0.016885144636034966\n",
      "[step: 16728] loss: 0.01622902601957321\n",
      "[step: 16729] loss: 0.014079619199037552\n",
      "[step: 16730] loss: 0.014113745652139187\n",
      "[step: 16731] loss: 0.0141619136556983\n",
      "[step: 16732] loss: 0.01296438742429018\n",
      "[step: 16733] loss: 0.012122289277613163\n",
      "[step: 16734] loss: 0.010716513730585575\n",
      "[step: 16735] loss: 0.00984918512403965\n",
      "[step: 16736] loss: 0.010959155857563019\n",
      "[step: 16737] loss: 0.011698241345584393\n",
      "[step: 16738] loss: 0.01120210811495781\n",
      "[step: 16739] loss: 0.01097226794809103\n",
      "[step: 16740] loss: 0.010954203084111214\n",
      "[step: 16741] loss: 0.010962184518575668\n",
      "[step: 16742] loss: 0.010924331843852997\n",
      "[step: 16743] loss: 0.01088196411728859\n",
      "[step: 16744] loss: 0.010722409002482891\n",
      "[step: 16745] loss: 0.010595157742500305\n",
      "[step: 16746] loss: 0.01036105491220951\n",
      "[step: 16747] loss: 0.010009128600358963\n",
      "[step: 16748] loss: 0.009817864745855331\n",
      "[step: 16749] loss: 0.009801852516829967\n",
      "[step: 16750] loss: 0.00965785887092352\n",
      "[step: 16751] loss: 0.00949131790548563\n",
      "[step: 16752] loss: 0.009514685720205307\n",
      "[step: 16753] loss: 0.009530306793749332\n",
      "[step: 16754] loss: 0.009431343525648117\n",
      "[step: 16755] loss: 0.009386320598423481\n",
      "[step: 16756] loss: 0.009343182668089867\n",
      "[step: 16757] loss: 0.009189549833536148\n",
      "[step: 16758] loss: 0.009110711514949799\n",
      "[step: 16759] loss: 0.009021260775625706\n",
      "[step: 16760] loss: 0.008968960493803024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 16761] loss: 0.008972801268100739\n",
      "[step: 16762] loss: 0.008936516009271145\n",
      "[step: 16763] loss: 0.008918541483581066\n",
      "[step: 16764] loss: 0.008893716149032116\n",
      "[step: 16765] loss: 0.008817117661237717\n",
      "[step: 16766] loss: 0.00872437097132206\n",
      "[step: 16767] loss: 0.008624623529613018\n",
      "[step: 16768] loss: 0.008504686877131462\n",
      "[step: 16769] loss: 0.008413542993366718\n",
      "[step: 16770] loss: 0.008311372250318527\n",
      "[step: 16771] loss: 0.008235979825258255\n",
      "[step: 16772] loss: 0.008138371631503105\n",
      "[step: 16773] loss: 0.008070762269198895\n",
      "[step: 16774] loss: 0.007984458468854427\n",
      "[step: 16775] loss: 0.00794470589607954\n",
      "[step: 16776] loss: 0.00787600688636303\n",
      "[step: 16777] loss: 0.007836082018911839\n",
      "[step: 16778] loss: 0.007829210720956326\n",
      "[step: 16779] loss: 0.00779116852208972\n",
      "[step: 16780] loss: 0.007770169991999865\n",
      "[step: 16781] loss: 0.007744650822132826\n",
      "[step: 16782] loss: 0.007708562538027763\n",
      "[step: 16783] loss: 0.007721936795860529\n",
      "[step: 16784] loss: 0.007722070440649986\n",
      "[step: 16785] loss: 0.007679412607103586\n",
      "[step: 16786] loss: 0.007657566573470831\n",
      "[step: 16787] loss: 0.007640737574547529\n",
      "[step: 16788] loss: 0.007590537425130606\n",
      "[step: 16789] loss: 0.00759527925401926\n",
      "[step: 16790] loss: 0.007598668336868286\n",
      "[step: 16791] loss: 0.007514205761253834\n",
      "[step: 16792] loss: 0.0075842575170099735\n",
      "[step: 16793] loss: 0.007642155978828669\n",
      "[step: 16794] loss: 0.007519233506172895\n",
      "[step: 16795] loss: 0.007782252039760351\n",
      "[step: 16796] loss: 0.007819670252501965\n",
      "[step: 16797] loss: 0.007864090614020824\n",
      "[step: 16798] loss: 0.007472324185073376\n",
      "[step: 16799] loss: 0.00831541046500206\n",
      "[step: 16800] loss: 0.008154049515724182\n",
      "[step: 16801] loss: 0.008786829188466072\n",
      "[step: 16802] loss: 0.00814678892493248\n",
      "[step: 16803] loss: 0.00817233044654131\n",
      "[step: 16804] loss: 0.008193266578018665\n",
      "[step: 16805] loss: 0.008382666856050491\n",
      "[step: 16806] loss: 0.007848617620766163\n",
      "[step: 16807] loss: 0.008002685382962227\n",
      "[step: 16808] loss: 0.008164219558238983\n",
      "[step: 16809] loss: 0.007847215980291367\n",
      "[step: 16810] loss: 0.007718339562416077\n",
      "[step: 16811] loss: 0.00794968567788601\n",
      "[step: 16812] loss: 0.007716900669038296\n",
      "[step: 16813] loss: 0.007600152865052223\n",
      "[step: 16814] loss: 0.007810601498931646\n",
      "[step: 16815] loss: 0.00764425890520215\n",
      "[step: 16816] loss: 0.007558297365903854\n",
      "[step: 16817] loss: 0.00767542002722621\n",
      "[step: 16818] loss: 0.007605062332004309\n",
      "[step: 16819] loss: 0.0075303022749722\n",
      "[step: 16820] loss: 0.00756489671766758\n",
      "[step: 16821] loss: 0.007550300098955631\n",
      "[step: 16822] loss: 0.00748197827488184\n",
      "[step: 16823] loss: 0.007539052516222\n",
      "[step: 16824] loss: 0.007482158485800028\n",
      "[step: 16825] loss: 0.007432558108121157\n",
      "[step: 16826] loss: 0.007464796304702759\n",
      "[step: 16827] loss: 0.007438003085553646\n",
      "[step: 16828] loss: 0.0073783122934401035\n",
      "[step: 16829] loss: 0.007405860349535942\n",
      "[step: 16830] loss: 0.007394557353109121\n",
      "[step: 16831] loss: 0.0073516457341611385\n",
      "[step: 16832] loss: 0.007362227886915207\n",
      "[step: 16833] loss: 0.007350591942667961\n",
      "[step: 16834] loss: 0.007318447344005108\n",
      "[step: 16835] loss: 0.007319493219256401\n",
      "[step: 16836] loss: 0.00730631360784173\n",
      "[step: 16837] loss: 0.0072850752621889114\n",
      "[step: 16838] loss: 0.007285572122782469\n",
      "[step: 16839] loss: 0.007263333071023226\n",
      "[step: 16840] loss: 0.00725005054846406\n",
      "[step: 16841] loss: 0.007249159272760153\n",
      "[step: 16842] loss: 0.007229672744870186\n",
      "[step: 16843] loss: 0.0072156162932515144\n",
      "[step: 16844] loss: 0.007214914541691542\n",
      "[step: 16845] loss: 0.0071960994973778725\n",
      "[step: 16846] loss: 0.007186362985521555\n",
      "[step: 16847] loss: 0.007176230661571026\n",
      "[step: 16848] loss: 0.007161261513829231\n",
      "[step: 16849] loss: 0.007153156213462353\n",
      "[step: 16850] loss: 0.007140311412513256\n",
      "[step: 16851] loss: 0.007126045413315296\n",
      "[step: 16852] loss: 0.007118978537619114\n",
      "[step: 16853] loss: 0.007103773765265942\n",
      "[step: 16854] loss: 0.007092099171131849\n",
      "[step: 16855] loss: 0.007082521915435791\n",
      "[step: 16856] loss: 0.007068500388413668\n",
      "[step: 16857] loss: 0.00705754803493619\n",
      "[step: 16858] loss: 0.007046316750347614\n",
      "[step: 16859] loss: 0.007032804191112518\n",
      "[step: 16860] loss: 0.007021802943199873\n",
      "[step: 16861] loss: 0.007008254528045654\n",
      "[step: 16862] loss: 0.006996377371251583\n",
      "[step: 16863] loss: 0.00698451604694128\n",
      "[step: 16864] loss: 0.006970691028982401\n",
      "[step: 16865] loss: 0.006959006190299988\n",
      "[step: 16866] loss: 0.006946059409528971\n",
      "[step: 16867] loss: 0.006932524032890797\n",
      "[step: 16868] loss: 0.006920172832906246\n",
      "[step: 16869] loss: 0.0069065517745912075\n",
      "[step: 16870] loss: 0.006893431767821312\n",
      "[step: 16871] loss: 0.006880197208374739\n",
      "[step: 16872] loss: 0.006866194307804108\n",
      "[step: 16873] loss: 0.00685320608317852\n",
      "[step: 16874] loss: 0.006839381530880928\n",
      "[step: 16875] loss: 0.006825657095760107\n",
      "[step: 16876] loss: 0.006812073290348053\n",
      "[step: 16877] loss: 0.00679814675822854\n",
      "[step: 16878] loss: 0.006784379482269287\n",
      "[step: 16879] loss: 0.006770306266844273\n",
      "[step: 16880] loss: 0.0067564030177891254\n",
      "[step: 16881] loss: 0.006742299534380436\n",
      "[step: 16882] loss: 0.00672812620177865\n",
      "[step: 16883] loss: 0.00671396916732192\n",
      "[step: 16884] loss: 0.006699732970446348\n",
      "[step: 16885] loss: 0.006685507949441671\n",
      "[step: 16886] loss: 0.006671111565083265\n",
      "[step: 16887] loss: 0.0066568367183208466\n",
      "[step: 16888] loss: 0.006642383988946676\n",
      "[step: 16889] loss: 0.006627968978136778\n",
      "[step: 16890] loss: 0.006613501813262701\n",
      "[step: 16891] loss: 0.006599005311727524\n",
      "[step: 16892] loss: 0.006584516726434231\n",
      "[step: 16893] loss: 0.006569886580109596\n",
      "[step: 16894] loss: 0.0065554240718483925\n",
      "[step: 16895] loss: 0.006540792994201183\n",
      "[step: 16896] loss: 0.006526248995214701\n",
      "[step: 16897] loss: 0.0065116481855511665\n",
      "[step: 16898] loss: 0.006497120019048452\n",
      "[step: 16899] loss: 0.006482592783868313\n",
      "[step: 16900] loss: 0.006468087434768677\n",
      "[step: 16901] loss: 0.006453658454120159\n",
      "[step: 16902] loss: 0.0064392597414553165\n",
      "[step: 16903] loss: 0.00642492575570941\n",
      "[step: 16904] loss: 0.006410688627511263\n",
      "[step: 16905] loss: 0.006396560929715633\n",
      "[step: 16906] loss: 0.006382502615451813\n",
      "[step: 16907] loss: 0.006368598900735378\n",
      "[step: 16908] loss: 0.006354873068630695\n",
      "[step: 16909] loss: 0.006341726519167423\n",
      "[step: 16910] loss: 0.006331039126962423\n",
      "[step: 16911] loss: 0.006334810517728329\n",
      "[step: 16912] loss: 0.006423876155167818\n",
      "[step: 16913] loss: 0.006854591425508261\n",
      "[step: 16914] loss: 0.007223610766232014\n",
      "[step: 16915] loss: 0.006451221648603678\n",
      "[step: 16916] loss: 0.007789753843098879\n",
      "[step: 16917] loss: 0.007695035543292761\n",
      "[step: 16918] loss: 0.007065045181661844\n",
      "[step: 16919] loss: 0.007253167685121298\n",
      "[step: 16920] loss: 0.0070001292042434216\n",
      "[step: 16921] loss: 0.006753678899258375\n",
      "[step: 16922] loss: 0.006815923843532801\n",
      "[step: 16923] loss: 0.007071647327393293\n",
      "[step: 16924] loss: 0.006810023915022612\n",
      "[step: 16925] loss: 0.00662838714197278\n",
      "[step: 16926] loss: 0.006976293865591288\n",
      "[step: 16927] loss: 0.006860323250293732\n",
      "[step: 16928] loss: 0.006467015016824007\n",
      "[step: 16929] loss: 0.006916429847478867\n",
      "[step: 16930] loss: 0.006885123904794455\n",
      "[step: 16931] loss: 0.007637637201696634\n",
      "[step: 16932] loss: 0.006457905750721693\n",
      "[step: 16933] loss: 0.0066758086904883385\n",
      "[step: 16934] loss: 0.007508300244808197\n",
      "[step: 16935] loss: 0.007004731334745884\n",
      "[step: 16936] loss: 0.006889292038977146\n",
      "[step: 16937] loss: 0.007998908869922161\n",
      "[step: 16938] loss: 0.006573296617716551\n",
      "[step: 16939] loss: 0.008171623572707176\n",
      "[step: 16940] loss: 0.006881312932819128\n",
      "[step: 16941] loss: 0.006794963032007217\n",
      "[step: 16942] loss: 0.006602436304092407\n",
      "[step: 16943] loss: 0.006964168045669794\n",
      "[step: 16944] loss: 0.006385082844644785\n",
      "[step: 16945] loss: 0.006666176486760378\n",
      "[step: 16946] loss: 0.006439230404794216\n",
      "[step: 16947] loss: 0.0065915645100176334\n",
      "[step: 16948] loss: 0.006434781476855278\n",
      "[step: 16949] loss: 0.006400087848305702\n",
      "[step: 16950] loss: 0.0064128669910132885\n",
      "[step: 16951] loss: 0.006248721852898598\n",
      "[step: 16952] loss: 0.006414283998310566\n",
      "[step: 16953] loss: 0.0062202587723731995\n",
      "[step: 16954] loss: 0.006293236277997494\n",
      "[step: 16955] loss: 0.006277279928326607\n",
      "[step: 16956] loss: 0.006656337063759565\n",
      "[step: 16957] loss: 0.007026008330285549\n",
      "[step: 16958] loss: 0.006264967378228903\n",
      "[step: 16959] loss: 0.006637840997427702\n",
      "[step: 16960] loss: 0.0071789356879889965\n",
      "[step: 16961] loss: 0.0065650069154798985\n",
      "[step: 16962] loss: 0.008062371052801609\n",
      "[step: 16963] loss: 0.0077784075401723385\n",
      "[step: 16964] loss: 0.006886023096740246\n",
      "[step: 16965] loss: 0.0071559869684278965\n",
      "[step: 16966] loss: 0.006861780304461718\n",
      "[step: 16967] loss: 0.00669740280136466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 16968] loss: 0.006756938528269529\n",
      "[step: 16969] loss: 0.006772475317120552\n",
      "[step: 16970] loss: 0.0064836908131837845\n",
      "[step: 16971] loss: 0.006835206877440214\n",
      "[step: 16972] loss: 0.006577841006219387\n",
      "[step: 16973] loss: 0.0066902972757816315\n",
      "[step: 16974] loss: 0.006495618261396885\n",
      "[step: 16975] loss: 0.006710411049425602\n",
      "[step: 16976] loss: 0.006306902505457401\n",
      "[step: 16977] loss: 0.006651648785918951\n",
      "[step: 16978] loss: 0.006315058097243309\n",
      "[step: 16979] loss: 0.006445609033107758\n",
      "[step: 16980] loss: 0.006267275661230087\n",
      "[step: 16981] loss: 0.00666471105068922\n",
      "[step: 16982] loss: 0.006743654143065214\n",
      "[step: 16983] loss: 0.006191018968820572\n",
      "[step: 16984] loss: 0.0065280902199447155\n",
      "[step: 16985] loss: 0.007122857496142387\n",
      "[step: 16986] loss: 0.006329180207103491\n",
      "[step: 16987] loss: 0.007600639946758747\n",
      "[step: 16988] loss: 0.006757898721843958\n",
      "[step: 16989] loss: 0.006730609107762575\n",
      "[step: 16990] loss: 0.007098252885043621\n",
      "[step: 16991] loss: 0.006319879554212093\n",
      "[step: 16992] loss: 0.0066647278144955635\n",
      "[step: 16993] loss: 0.007288337219506502\n",
      "[step: 16994] loss: 0.00629185326397419\n",
      "[step: 16995] loss: 0.007046736776828766\n",
      "[step: 16996] loss: 0.008141329512000084\n",
      "[step: 16997] loss: 0.00661087641492486\n",
      "[step: 16998] loss: 0.007458593230694532\n",
      "[step: 16999] loss: 0.006796819157898426\n",
      "[step: 17000] loss: 0.00705364253371954\n",
      "[step: 17001] loss: 0.006592275574803352\n",
      "[step: 17002] loss: 0.006571462843567133\n",
      "[step: 17003] loss: 0.0065400428138673306\n",
      "[step: 17004] loss: 0.006548409815877676\n",
      "[step: 17005] loss: 0.006485422141849995\n",
      "[step: 17006] loss: 0.006521330215036869\n",
      "[step: 17007] loss: 0.0064092776738107204\n",
      "[step: 17008] loss: 0.0061644297093153\n",
      "[step: 17009] loss: 0.006399876903742552\n",
      "[step: 17010] loss: 0.006472188513725996\n",
      "[step: 17011] loss: 0.006213546264916658\n",
      "[step: 17012] loss: 0.0061417436227202415\n",
      "[step: 17013] loss: 0.006254627835005522\n",
      "[step: 17014] loss: 0.0062563661485910416\n",
      "[step: 17015] loss: 0.006285631097853184\n",
      "[step: 17016] loss: 0.0062228720635175705\n",
      "[step: 17017] loss: 0.006086931563913822\n",
      "[step: 17018] loss: 0.006071141455322504\n",
      "[step: 17019] loss: 0.006134423427283764\n",
      "[step: 17020] loss: 0.006015663500875235\n",
      "[step: 17021] loss: 0.00602759188041091\n",
      "[step: 17022] loss: 0.006086610723286867\n",
      "[step: 17023] loss: 0.0064103566110134125\n",
      "[step: 17024] loss: 0.006508726626634598\n",
      "[step: 17025] loss: 0.006146022584289312\n",
      "[step: 17026] loss: 0.006127888336777687\n",
      "[step: 17027] loss: 0.00614748802036047\n",
      "[step: 17028] loss: 0.006163931451737881\n",
      "[step: 17029] loss: 0.006047358270734549\n",
      "[step: 17030] loss: 0.006144084967672825\n",
      "[step: 17031] loss: 0.006506734061986208\n",
      "[step: 17032] loss: 0.00794699415564537\n",
      "[step: 17033] loss: 0.010912176221609116\n",
      "[step: 17034] loss: 0.013575383462011814\n",
      "[step: 17035] loss: 0.007780489046126604\n",
      "[step: 17036] loss: 0.01149738673120737\n",
      "[step: 17037] loss: 0.008194663561880589\n",
      "[step: 17038] loss: 0.009219302795827389\n",
      "[step: 17039] loss: 0.007759076077491045\n",
      "[step: 17040] loss: 0.009752863086760044\n",
      "[step: 17041] loss: 0.007854774594306946\n",
      "[step: 17042] loss: 0.008527135476469994\n",
      "[step: 17043] loss: 0.008404441177845001\n",
      "[step: 17044] loss: 0.00853132363408804\n",
      "[step: 17045] loss: 0.008192872628569603\n",
      "[step: 17046] loss: 0.0072570499032735825\n",
      "[step: 17047] loss: 0.007495037280023098\n",
      "[step: 17048] loss: 0.007924840785562992\n",
      "[step: 17049] loss: 0.007095769979059696\n",
      "[step: 17050] loss: 0.007252729497849941\n",
      "[step: 17051] loss: 0.007283334154635668\n",
      "[step: 17052] loss: 0.007427130825817585\n",
      "[step: 17053] loss: 0.0072854249738156796\n",
      "[step: 17054] loss: 0.007041980046778917\n",
      "[step: 17055] loss: 0.00682376092299819\n",
      "[step: 17056] loss: 0.0068414658308029175\n",
      "[step: 17057] loss: 0.006897740997374058\n",
      "[step: 17058] loss: 0.006664057727903128\n",
      "[step: 17059] loss: 0.006641169544309378\n",
      "[step: 17060] loss: 0.006746694911271334\n",
      "[step: 17061] loss: 0.006693482864648104\n",
      "[step: 17062] loss: 0.006622015964239836\n",
      "[step: 17063] loss: 0.006541436538100243\n",
      "[step: 17064] loss: 0.006589476950466633\n",
      "[step: 17065] loss: 0.006594256032258272\n",
      "[step: 17066] loss: 0.00651785871013999\n",
      "[step: 17067] loss: 0.006461507175117731\n",
      "[step: 17068] loss: 0.006461643148213625\n",
      "[step: 17069] loss: 0.006441270932555199\n",
      "[step: 17070] loss: 0.006432437337934971\n",
      "[step: 17071] loss: 0.006414640229195356\n",
      "[step: 17072] loss: 0.0063412985764443874\n",
      "[step: 17073] loss: 0.006319329608231783\n",
      "[step: 17074] loss: 0.006331200245767832\n",
      "[step: 17075] loss: 0.006261481903493404\n",
      "[step: 17076] loss: 0.006282541435211897\n",
      "[step: 17077] loss: 0.006291721016168594\n",
      "[step: 17078] loss: 0.006204764824360609\n",
      "[step: 17079] loss: 0.0062036640010774136\n",
      "[step: 17080] loss: 0.006188452709466219\n",
      "[step: 17081] loss: 0.006160512566566467\n",
      "[step: 17082] loss: 0.006117265671491623\n",
      "[step: 17083] loss: 0.0061149634420871735\n",
      "[step: 17084] loss: 0.006114978343248367\n",
      "[step: 17085] loss: 0.006082131061702967\n",
      "[step: 17086] loss: 0.006062529981136322\n",
      "[step: 17087] loss: 0.006041061598807573\n",
      "[step: 17088] loss: 0.006017357110977173\n",
      "[step: 17089] loss: 0.006017306819558144\n",
      "[step: 17090] loss: 0.006036694627255201\n",
      "[step: 17091] loss: 0.006035686004906893\n",
      "[step: 17092] loss: 0.006047793198376894\n",
      "[step: 17093] loss: 0.00598890520632267\n",
      "[step: 17094] loss: 0.005930822808295488\n",
      "[step: 17095] loss: 0.0059210145846009254\n",
      "[step: 17096] loss: 0.005921993870288134\n",
      "[step: 17097] loss: 0.005949682090431452\n",
      "[step: 17098] loss: 0.006021180190145969\n",
      "[step: 17099] loss: 0.006184123922139406\n",
      "[step: 17100] loss: 0.006089626811444759\n",
      "[step: 17101] loss: 0.0061034937389194965\n",
      "[step: 17102] loss: 0.005939596798270941\n",
      "[step: 17103] loss: 0.005858604330569506\n",
      "[step: 17104] loss: 0.005867411382496357\n",
      "[step: 17105] loss: 0.005982840433716774\n",
      "[step: 17106] loss: 0.00606300076469779\n",
      "[step: 17107] loss: 0.006448913365602493\n",
      "[step: 17108] loss: 0.007240453269332647\n",
      "[step: 17109] loss: 0.01199615839868784\n",
      "[step: 17110] loss: 0.011904818005859852\n",
      "[step: 17111] loss: 0.009016111493110657\n",
      "[step: 17112] loss: 0.013140383176505566\n",
      "[step: 17113] loss: 0.013111097738146782\n",
      "[step: 17114] loss: 0.011373721994459629\n",
      "[step: 17115] loss: 0.011377615854144096\n",
      "[step: 17116] loss: 0.012185726314783096\n",
      "[step: 17117] loss: 0.010792707093060017\n",
      "[step: 17118] loss: 0.008542186580598354\n",
      "[step: 17119] loss: 0.010256093926727772\n",
      "[step: 17120] loss: 0.010318336077034473\n",
      "[step: 17121] loss: 0.00961633212864399\n",
      "[step: 17122] loss: 0.008602886460721493\n",
      "[step: 17123] loss: 0.009073592722415924\n",
      "[step: 17124] loss: 0.009073209017515182\n",
      "[step: 17125] loss: 0.007701449561864138\n",
      "[step: 17126] loss: 0.008935170248150826\n",
      "[step: 17127] loss: 0.00803382508456707\n",
      "[step: 17128] loss: 0.008014951832592487\n",
      "[step: 17129] loss: 0.008342349901795387\n",
      "[step: 17130] loss: 0.007792300544679165\n",
      "[step: 17131] loss: 0.008021227084100246\n",
      "[step: 17132] loss: 0.007653938606381416\n",
      "[step: 17133] loss: 0.007958665490150452\n",
      "[step: 17134] loss: 0.007542243227362633\n",
      "[step: 17135] loss: 0.00740788085386157\n",
      "[step: 17136] loss: 0.007719330955296755\n",
      "[step: 17137] loss: 0.007301737554371357\n",
      "[step: 17138] loss: 0.00725768506526947\n",
      "[step: 17139] loss: 0.007445380091667175\n",
      "[step: 17140] loss: 0.007042385637760162\n",
      "[step: 17141] loss: 0.00728214718401432\n",
      "[step: 17142] loss: 0.007150526624172926\n",
      "[step: 17143] loss: 0.00697998097166419\n",
      "[step: 17144] loss: 0.007173014804720879\n",
      "[step: 17145] loss: 0.006930897943675518\n",
      "[step: 17146] loss: 0.00694836862385273\n",
      "[step: 17147] loss: 0.006953221280127764\n",
      "[step: 17148] loss: 0.006790344603359699\n",
      "[step: 17149] loss: 0.006893403362482786\n",
      "[step: 17150] loss: 0.006759944837540388\n",
      "[step: 17151] loss: 0.00675840163603425\n",
      "[step: 17152] loss: 0.006778448820114136\n",
      "[step: 17153] loss: 0.0066842869855463505\n",
      "[step: 17154] loss: 0.006709644570946693\n",
      "[step: 17155] loss: 0.006656009238213301\n",
      "[step: 17156] loss: 0.006628299597650766\n",
      "[step: 17157] loss: 0.006619349122047424\n",
      "[step: 17158] loss: 0.006582761649042368\n",
      "[step: 17159] loss: 0.0065824599005281925\n",
      "[step: 17160] loss: 0.006525679491460323\n",
      "[step: 17161] loss: 0.006529319565743208\n",
      "[step: 17162] loss: 0.00649672606959939\n",
      "[step: 17163] loss: 0.0064694443717598915\n",
      "[step: 17164] loss: 0.006467313971370459\n",
      "[step: 17165] loss: 0.006421121768653393\n",
      "[step: 17166] loss: 0.006425012368708849\n",
      "[step: 17167] loss: 0.006381789688020945\n",
      "[step: 17168] loss: 0.006385702174156904\n",
      "[step: 17169] loss: 0.006350312381982803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 17170] loss: 0.006350971758365631\n",
      "[step: 17171] loss: 0.006320160813629627\n",
      "[step: 17172] loss: 0.0063137779943645\n",
      "[step: 17173] loss: 0.006290242075920105\n",
      "[step: 17174] loss: 0.006284392438828945\n",
      "[step: 17175] loss: 0.006262779235839844\n",
      "[step: 17176] loss: 0.006260785274207592\n",
      "[step: 17177] loss: 0.006238959264010191\n",
      "[step: 17178] loss: 0.006233774125576019\n",
      "[step: 17179] loss: 0.006217782385647297\n",
      "[step: 17180] loss: 0.006211741361767054\n",
      "[step: 17181] loss: 0.006194838788360357\n",
      "[step: 17182] loss: 0.006192271132022142\n",
      "[step: 17183] loss: 0.00617673434317112\n",
      "[step: 17184] loss: 0.006167804356664419\n",
      "[step: 17185] loss: 0.006155704613775015\n",
      "[step: 17186] loss: 0.006149133201688528\n",
      "[step: 17187] loss: 0.006138164084404707\n",
      "[step: 17188] loss: 0.006126650609076023\n",
      "[step: 17189] loss: 0.006117867771536112\n",
      "[step: 17190] loss: 0.006107233930379152\n",
      "[step: 17191] loss: 0.006100784055888653\n",
      "[step: 17192] loss: 0.006088252644985914\n",
      "[step: 17193] loss: 0.006079560145735741\n",
      "[step: 17194] loss: 0.006068827118724585\n",
      "[step: 17195] loss: 0.006059302017092705\n",
      "[step: 17196] loss: 0.0060519385151565075\n",
      "[step: 17197] loss: 0.006041934248059988\n",
      "[step: 17198] loss: 0.006034530233591795\n",
      "[step: 17199] loss: 0.006025454495102167\n",
      "[step: 17200] loss: 0.006016422063112259\n",
      "[step: 17201] loss: 0.006009329576045275\n",
      "[step: 17202] loss: 0.0060004726983606815\n",
      "[step: 17203] loss: 0.005993634462356567\n",
      "[step: 17204] loss: 0.00598857132717967\n",
      "[step: 17205] loss: 0.005985276307910681\n",
      "[step: 17206] loss: 0.0059885731898248196\n",
      "[step: 17207] loss: 0.006016125902533531\n",
      "[step: 17208] loss: 0.006059526931494474\n",
      "[step: 17209] loss: 0.006211979780346155\n",
      "[step: 17210] loss: 0.006186384242027998\n",
      "[step: 17211] loss: 0.00623135082423687\n",
      "[step: 17212] loss: 0.006012912839651108\n",
      "[step: 17213] loss: 0.005936854984611273\n",
      "[step: 17214] loss: 0.0060052708722651005\n",
      "[step: 17215] loss: 0.006129294168204069\n",
      "[step: 17216] loss: 0.006319237407296896\n",
      "[step: 17217] loss: 0.006088393274694681\n",
      "[step: 17218] loss: 0.0059511312283575535\n",
      "[step: 17219] loss: 0.005953454878181219\n",
      "[step: 17220] loss: 0.006062468979507685\n",
      "[step: 17221] loss: 0.006166106555610895\n",
      "[step: 17222] loss: 0.006115485914051533\n",
      "[step: 17223] loss: 0.005995071493089199\n",
      "[step: 17224] loss: 0.005927117075771093\n",
      "[step: 17225] loss: 0.005899845622479916\n",
      "[step: 17226] loss: 0.005970431491732597\n",
      "[step: 17227] loss: 0.006090556271374226\n",
      "[step: 17228] loss: 0.006421390455216169\n",
      "[step: 17229] loss: 0.006131717469543219\n",
      "[step: 17230] loss: 0.006069143768399954\n",
      "[step: 17231] loss: 0.005978160072118044\n",
      "[step: 17232] loss: 0.006022388581186533\n",
      "[step: 17233] loss: 0.006452640518546104\n",
      "[step: 17234] loss: 0.0067144702188670635\n",
      "[step: 17235] loss: 0.006459140684455633\n",
      "[step: 17236] loss: 0.0064735375344753265\n",
      "[step: 17237] loss: 0.0067976717837154865\n",
      "[step: 17238] loss: 0.0067382086999714375\n",
      "[step: 17239] loss: 0.006109447218477726\n",
      "[step: 17240] loss: 0.006671688985079527\n",
      "[step: 17241] loss: 0.0063898139633238316\n",
      "[step: 17242] loss: 0.006184504367411137\n",
      "[step: 17243] loss: 0.0067188674584031105\n",
      "[step: 17244] loss: 0.006133596412837505\n",
      "[step: 17245] loss: 0.006142623722553253\n",
      "[step: 17246] loss: 0.006532228551805019\n",
      "[step: 17247] loss: 0.0061925132758915424\n",
      "[step: 17248] loss: 0.007026784121990204\n",
      "[step: 17249] loss: 0.0064666541293263435\n",
      "[step: 17250] loss: 0.006780657917261124\n",
      "[step: 17251] loss: 0.0063544875010848045\n",
      "[step: 17252] loss: 0.006539101246744394\n",
      "[step: 17253] loss: 0.0064684548415243626\n",
      "[step: 17254] loss: 0.006168695632368326\n",
      "[step: 17255] loss: 0.006319979205727577\n",
      "[step: 17256] loss: 0.006194544956088066\n",
      "[step: 17257] loss: 0.006284873932600021\n",
      "[step: 17258] loss: 0.006076779682189226\n",
      "[step: 17259] loss: 0.006388428155332804\n",
      "[step: 17260] loss: 0.006698785815387964\n",
      "[step: 17261] loss: 0.006243877578526735\n",
      "[step: 17262] loss: 0.0067907399497926235\n",
      "[step: 17263] loss: 0.007472635712474585\n",
      "[step: 17264] loss: 0.006585673429071903\n",
      "[step: 17265] loss: 0.0072205872274935246\n",
      "[step: 17266] loss: 0.00615590950474143\n",
      "[step: 17267] loss: 0.006593621335923672\n",
      "[step: 17268] loss: 0.006178073585033417\n",
      "[step: 17269] loss: 0.007013851776719093\n",
      "[step: 17270] loss: 0.007141991052776575\n",
      "[step: 17271] loss: 0.00661130016669631\n",
      "[step: 17272] loss: 0.0067734792828559875\n",
      "[step: 17273] loss: 0.007054015528410673\n",
      "[step: 17274] loss: 0.007295553106814623\n",
      "[step: 17275] loss: 0.007287425920367241\n",
      "[step: 17276] loss: 0.007666402962058783\n",
      "[step: 17277] loss: 0.006080179009586573\n",
      "[step: 17278] loss: 0.00688153924420476\n",
      "[step: 17279] loss: 0.0061104027554392815\n",
      "[step: 17280] loss: 0.006775729823857546\n",
      "[step: 17281] loss: 0.006955817807465792\n",
      "[step: 17282] loss: 0.0067508649080991745\n",
      "[step: 17283] loss: 0.006755340378731489\n",
      "[step: 17284] loss: 0.006188950501382351\n",
      "[step: 17285] loss: 0.006425285246223211\n",
      "[step: 17286] loss: 0.006062387954443693\n",
      "[step: 17287] loss: 0.006485105026513338\n",
      "[step: 17288] loss: 0.006162845529615879\n",
      "[step: 17289] loss: 0.006189865991473198\n",
      "[step: 17290] loss: 0.0064248633570969105\n",
      "[step: 17291] loss: 0.005950001068413258\n",
      "[step: 17292] loss: 0.00606898358091712\n",
      "[step: 17293] loss: 0.0060965921729803085\n",
      "[step: 17294] loss: 0.005940374918282032\n",
      "[step: 17295] loss: 0.005991537589579821\n",
      "[step: 17296] loss: 0.005886528640985489\n",
      "[step: 17297] loss: 0.005892210174351931\n",
      "[step: 17298] loss: 0.005959702655673027\n",
      "[step: 17299] loss: 0.005937345325946808\n",
      "[step: 17300] loss: 0.005821323487907648\n",
      "[step: 17301] loss: 0.0058630360290408134\n",
      "[step: 17302] loss: 0.005891692359000444\n",
      "[step: 17303] loss: 0.005875758361071348\n",
      "[step: 17304] loss: 0.0058074407279491425\n",
      "[step: 17305] loss: 0.005802845116704702\n",
      "[step: 17306] loss: 0.005898727104067802\n",
      "[step: 17307] loss: 0.006029488053172827\n",
      "[step: 17308] loss: 0.006067609414458275\n",
      "[step: 17309] loss: 0.005833700764924288\n",
      "[step: 17310] loss: 0.005945094861090183\n",
      "[step: 17311] loss: 0.006124500185251236\n",
      "[step: 17312] loss: 0.00594791816547513\n",
      "[step: 17313] loss: 0.005773895885795355\n",
      "[step: 17314] loss: 0.005805244203656912\n",
      "[step: 17315] loss: 0.005922069773077965\n",
      "[step: 17316] loss: 0.006058893632143736\n",
      "[step: 17317] loss: 0.005836511962115765\n",
      "[step: 17318] loss: 0.005879700183868408\n",
      "[step: 17319] loss: 0.006160443648695946\n",
      "[step: 17320] loss: 0.0060661556199193\n",
      "[step: 17321] loss: 0.005936785601079464\n",
      "[step: 17322] loss: 0.0058021158911287785\n",
      "[step: 17323] loss: 0.0060439868830144405\n",
      "[step: 17324] loss: 0.006183300632983446\n",
      "[step: 17325] loss: 0.006099500227719545\n",
      "[step: 17326] loss: 0.006046672351658344\n",
      "[step: 17327] loss: 0.00575955118983984\n",
      "[step: 17328] loss: 0.0059654805809259415\n",
      "[step: 17329] loss: 0.0061268070712685585\n",
      "[step: 17330] loss: 0.006020092871040106\n",
      "[step: 17331] loss: 0.006346328184008598\n",
      "[step: 17332] loss: 0.0064402054995298386\n",
      "[step: 17333] loss: 0.006069287657737732\n",
      "[step: 17334] loss: 0.006689548492431641\n",
      "[step: 17335] loss: 0.0060704476200044155\n",
      "[step: 17336] loss: 0.006809352431446314\n",
      "[step: 17337] loss: 0.006658500991761684\n",
      "[step: 17338] loss: 0.007667717058211565\n",
      "[step: 17339] loss: 0.007215223740786314\n",
      "[step: 17340] loss: 0.006738266441971064\n",
      "[step: 17341] loss: 0.0076448735781013966\n",
      "[step: 17342] loss: 0.0067631639540195465\n",
      "[step: 17343] loss: 0.007128264755010605\n",
      "[step: 17344] loss: 0.006099720485508442\n",
      "[step: 17345] loss: 0.007583438418805599\n",
      "[step: 17346] loss: 0.007701169699430466\n",
      "[step: 17347] loss: 0.008076829835772514\n",
      "[step: 17348] loss: 0.007256456650793552\n",
      "[step: 17349] loss: 0.008168642409145832\n",
      "[step: 17350] loss: 0.007415995001792908\n",
      "[step: 17351] loss: 0.0075179836712777615\n",
      "[step: 17352] loss: 0.007719453889876604\n",
      "[step: 17353] loss: 0.006527337245643139\n",
      "[step: 17354] loss: 0.007427579257637262\n",
      "[step: 17355] loss: 0.007243412081152201\n",
      "[step: 17356] loss: 0.007728270720690489\n",
      "[step: 17357] loss: 0.006928838789463043\n",
      "[step: 17358] loss: 0.007108205463737249\n",
      "[step: 17359] loss: 0.008104084990918636\n",
      "[step: 17360] loss: 0.007130834739655256\n",
      "[step: 17361] loss: 0.007515022065490484\n",
      "[step: 17362] loss: 0.007280374877154827\n",
      "[step: 17363] loss: 0.007155830040574074\n",
      "[step: 17364] loss: 0.008778481744229794\n",
      "[step: 17365] loss: 0.0088682621717453\n",
      "[step: 17366] loss: 0.00669041508808732\n",
      "[step: 17367] loss: 0.008368315175175667\n",
      "[step: 17368] loss: 0.007700091693550348\n",
      "[step: 17369] loss: 0.007653855253010988\n",
      "[step: 17370] loss: 0.007082443684339523\n",
      "[step: 17371] loss: 0.007202554028481245\n",
      "[step: 17372] loss: 0.006739890202879906\n",
      "[step: 17373] loss: 0.007020994089543819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 17374] loss: 0.007220895029604435\n",
      "[step: 17375] loss: 0.006732025183737278\n",
      "[step: 17376] loss: 0.006548900157213211\n",
      "[step: 17377] loss: 0.006647142581641674\n",
      "[step: 17378] loss: 0.006620836444199085\n",
      "[step: 17379] loss: 0.006452249828726053\n",
      "[step: 17380] loss: 0.006455231457948685\n",
      "[step: 17381] loss: 0.006322490517050028\n",
      "[step: 17382] loss: 0.006207714322954416\n",
      "[step: 17383] loss: 0.006357440259307623\n",
      "[step: 17384] loss: 0.006224978249520063\n",
      "[step: 17385] loss: 0.006152804009616375\n",
      "[step: 17386] loss: 0.0062124780379235744\n",
      "[step: 17387] loss: 0.006063015200197697\n",
      "[step: 17388] loss: 0.006089417263865471\n",
      "[step: 17389] loss: 0.006101768929511309\n",
      "[step: 17390] loss: 0.005923098884522915\n",
      "[step: 17391] loss: 0.006035706494003534\n",
      "[step: 17392] loss: 0.005942065268754959\n",
      "[step: 17393] loss: 0.00598740391433239\n",
      "[step: 17394] loss: 0.005897464696317911\n",
      "[step: 17395] loss: 0.0058726235292851925\n",
      "[step: 17396] loss: 0.005877376068383455\n",
      "[step: 17397] loss: 0.005831815768033266\n",
      "[step: 17398] loss: 0.005813687574118376\n",
      "[step: 17399] loss: 0.005824577063322067\n",
      "[step: 17400] loss: 0.005810560192912817\n",
      "[step: 17401] loss: 0.005783594213426113\n",
      "[step: 17402] loss: 0.005774046294391155\n",
      "[step: 17403] loss: 0.005794577766209841\n",
      "[step: 17404] loss: 0.005751971621066332\n",
      "[step: 17405] loss: 0.0057302918285131454\n",
      "[step: 17406] loss: 0.005734310019761324\n",
      "[step: 17407] loss: 0.005708382930606604\n",
      "[step: 17408] loss: 0.005698156543076038\n",
      "[step: 17409] loss: 0.005683881230652332\n",
      "[step: 17410] loss: 0.005680617410689592\n",
      "[step: 17411] loss: 0.0056533897295594215\n",
      "[step: 17412] loss: 0.005647292360663414\n",
      "[step: 17413] loss: 0.005636385176330805\n",
      "[step: 17414] loss: 0.005632701795548201\n",
      "[step: 17415] loss: 0.005621322430670261\n",
      "[step: 17416] loss: 0.005613713059574366\n",
      "[step: 17417] loss: 0.005609540734440088\n",
      "[step: 17418] loss: 0.005602224264293909\n",
      "[step: 17419] loss: 0.005602004937827587\n",
      "[step: 17420] loss: 0.0056025986559689045\n",
      "[step: 17421] loss: 0.005605895537883043\n",
      "[step: 17422] loss: 0.00562001159414649\n",
      "[step: 17423] loss: 0.005655926186591387\n",
      "[step: 17424] loss: 0.005810857284814119\n",
      "[step: 17425] loss: 0.005747082643210888\n",
      "[step: 17426] loss: 0.005656652618199587\n",
      "[step: 17427] loss: 0.005626094993203878\n",
      "[step: 17428] loss: 0.005784549284726381\n",
      "[step: 17429] loss: 0.0061644683592021465\n",
      "[step: 17430] loss: 0.005636135116219521\n",
      "[step: 17431] loss: 0.005869816988706589\n",
      "[step: 17432] loss: 0.006389974150806665\n",
      "[step: 17433] loss: 0.005632465705275536\n",
      "[step: 17434] loss: 0.0066604651510715485\n",
      "[step: 17435] loss: 0.0077050719410181046\n",
      "[step: 17436] loss: 0.007432673126459122\n",
      "[step: 17437] loss: 0.006438048090785742\n",
      "[step: 17438] loss: 0.007461467292159796\n",
      "[step: 17439] loss: 0.006325447466224432\n",
      "[step: 17440] loss: 0.007231873460114002\n",
      "[step: 17441] loss: 0.006068787071853876\n",
      "[step: 17442] loss: 0.00700697535648942\n",
      "[step: 17443] loss: 0.006126174703240395\n",
      "[step: 17444] loss: 0.006670178379863501\n",
      "[step: 17445] loss: 0.005963187664747238\n",
      "[step: 17446] loss: 0.006617231760174036\n",
      "[step: 17447] loss: 0.00624749856069684\n",
      "[step: 17448] loss: 0.006695258896797895\n",
      "[step: 17449] loss: 0.006258135661482811\n",
      "[step: 17450] loss: 0.006746796425431967\n",
      "[step: 17451] loss: 0.005965159274637699\n",
      "[step: 17452] loss: 0.006485087797045708\n",
      "[step: 17453] loss: 0.005843530409038067\n",
      "[step: 17454] loss: 0.006638822611421347\n",
      "[step: 17455] loss: 0.006254555191844702\n",
      "[step: 17456] loss: 0.00645488640293479\n",
      "[step: 17457] loss: 0.006164263468235731\n",
      "[step: 17458] loss: 0.006620640400797129\n",
      "[step: 17459] loss: 0.006126921623945236\n",
      "[step: 17460] loss: 0.006509812548756599\n",
      "[step: 17461] loss: 0.0061365533620119095\n",
      "[step: 17462] loss: 0.006229307036846876\n",
      "[step: 17463] loss: 0.005852726753801107\n",
      "[step: 17464] loss: 0.0061951857060194016\n",
      "[step: 17465] loss: 0.005940210074186325\n",
      "[step: 17466] loss: 0.0060965800657868385\n",
      "[step: 17467] loss: 0.005956478416919708\n",
      "[step: 17468] loss: 0.0060415430925786495\n",
      "[step: 17469] loss: 0.005816464778035879\n",
      "[step: 17470] loss: 0.005938007030636072\n",
      "[step: 17471] loss: 0.0058156284503638744\n",
      "[step: 17472] loss: 0.005853201262652874\n",
      "[step: 17473] loss: 0.005813117604702711\n",
      "[step: 17474] loss: 0.006122972350567579\n",
      "[step: 17475] loss: 0.005849096458405256\n",
      "[step: 17476] loss: 0.005951812025159597\n",
      "[step: 17477] loss: 0.006140528712421656\n",
      "[step: 17478] loss: 0.005707379896193743\n",
      "[step: 17479] loss: 0.005978657864034176\n",
      "[step: 17480] loss: 0.005751480348408222\n",
      "[step: 17481] loss: 0.005859541706740856\n",
      "[step: 17482] loss: 0.005863639526069164\n",
      "[step: 17483] loss: 0.005733159836381674\n",
      "[step: 17484] loss: 0.005756369326263666\n",
      "[step: 17485] loss: 0.0057213325053453445\n",
      "[step: 17486] loss: 0.005701649002730846\n",
      "[step: 17487] loss: 0.00587989017367363\n",
      "[step: 17488] loss: 0.005615957081317902\n",
      "[step: 17489] loss: 0.005912474822252989\n",
      "[step: 17490] loss: 0.005637542810291052\n",
      "[step: 17491] loss: 0.005705663934350014\n",
      "[step: 17492] loss: 0.005678475834429264\n",
      "[step: 17493] loss: 0.005533852614462376\n",
      "[step: 17494] loss: 0.005735156591981649\n",
      "[step: 17495] loss: 0.005741199012845755\n",
      "[step: 17496] loss: 0.005780364386737347\n",
      "[step: 17497] loss: 0.005711360834538937\n",
      "[step: 17498] loss: 0.0058054737746715546\n",
      "[step: 17499] loss: 0.005618804134428501\n",
      "[step: 17500] loss: 0.00581198139116168\n",
      "[step: 17501] loss: 0.0056991539895534515\n",
      "[step: 17502] loss: 0.005613785237073898\n",
      "[step: 17503] loss: 0.005582420155405998\n",
      "[step: 17504] loss: 0.0055692908354103565\n",
      "[step: 17505] loss: 0.005565610248595476\n",
      "[step: 17506] loss: 0.005495942663401365\n",
      "[step: 17507] loss: 0.005568785592913628\n",
      "[step: 17508] loss: 0.0057386551052331924\n",
      "[step: 17509] loss: 0.005672870669513941\n",
      "[step: 17510] loss: 0.005659422837197781\n",
      "[step: 17511] loss: 0.00572394672781229\n",
      "[step: 17512] loss: 0.005586635787039995\n",
      "[step: 17513] loss: 0.005651464685797691\n",
      "[step: 17514] loss: 0.005486149340867996\n",
      "[step: 17515] loss: 0.005585247650742531\n",
      "[step: 17516] loss: 0.005625040270388126\n",
      "[step: 17517] loss: 0.005548407323658466\n",
      "[step: 17518] loss: 0.0055538867600262165\n",
      "[step: 17519] loss: 0.00591529905796051\n",
      "[step: 17520] loss: 0.005650732666254044\n",
      "[step: 17521] loss: 0.005683272611349821\n",
      "[step: 17522] loss: 0.005583681166172028\n",
      "[step: 17523] loss: 0.005517622455954552\n",
      "[step: 17524] loss: 0.005508859176188707\n",
      "[step: 17525] loss: 0.005451695062220097\n",
      "[step: 17526] loss: 0.005533862393349409\n",
      "[step: 17527] loss: 0.005598350893706083\n",
      "[step: 17528] loss: 0.005616676062345505\n",
      "[step: 17529] loss: 0.0061827125027775764\n",
      "[step: 17530] loss: 0.005956161767244339\n",
      "[step: 17531] loss: 0.00595447514206171\n",
      "[step: 17532] loss: 0.006024552043527365\n",
      "[step: 17533] loss: 0.005619544070214033\n",
      "[step: 17534] loss: 0.005712924059480429\n",
      "[step: 17535] loss: 0.005849915556609631\n",
      "[step: 17536] loss: 0.00610171677544713\n",
      "[step: 17537] loss: 0.005894554778933525\n",
      "[step: 17538] loss: 0.0057615251280367374\n",
      "[step: 17539] loss: 0.005748718045651913\n",
      "[step: 17540] loss: 0.0060370308347046375\n",
      "[step: 17541] loss: 0.007405421230942011\n",
      "[step: 17542] loss: 0.006194430869072676\n",
      "[step: 17543] loss: 0.0075812251307070255\n",
      "[step: 17544] loss: 0.006297786254435778\n",
      "[step: 17545] loss: 0.007241500541567802\n",
      "[step: 17546] loss: 0.006371709983795881\n",
      "[step: 17547] loss: 0.007890118286013603\n",
      "[step: 17548] loss: 0.0074834441766142845\n",
      "[step: 17549] loss: 0.007977776229381561\n",
      "[step: 17550] loss: 0.007748409640043974\n",
      "[step: 17551] loss: 0.007079521659761667\n",
      "[step: 17552] loss: 0.007193112745881081\n",
      "[step: 17553] loss: 0.006741633173078299\n",
      "[step: 17554] loss: 0.007064396049827337\n",
      "[step: 17555] loss: 0.006598633714020252\n",
      "[step: 17556] loss: 0.006709624081850052\n",
      "[step: 17557] loss: 0.006682605016976595\n",
      "[step: 17558] loss: 0.006503388751298189\n",
      "[step: 17559] loss: 0.006437381729483604\n",
      "[step: 17560] loss: 0.006594265345484018\n",
      "[step: 17561] loss: 0.006253549363464117\n",
      "[step: 17562] loss: 0.0067282007075846195\n",
      "[step: 17563] loss: 0.006521879229694605\n",
      "[step: 17564] loss: 0.006346201989799738\n",
      "[step: 17565] loss: 0.006537738721817732\n",
      "[step: 17566] loss: 0.0064345053397119045\n",
      "[step: 17567] loss: 0.006314756348729134\n",
      "[step: 17568] loss: 0.006398105528205633\n",
      "[step: 17569] loss: 0.006168335676193237\n",
      "[step: 17570] loss: 0.006270220037549734\n",
      "[step: 17571] loss: 0.006108902860432863\n",
      "[step: 17572] loss: 0.006003148388117552\n",
      "[step: 17573] loss: 0.005892307497560978\n",
      "[step: 17574] loss: 0.00598595105111599\n",
      "[step: 17575] loss: 0.005927725695073605\n",
      "[step: 17576] loss: 0.005975655745714903\n",
      "[step: 17577] loss: 0.005885944701731205\n",
      "[step: 17578] loss: 0.005877564195543528\n",
      "[step: 17579] loss: 0.005821527447551489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 17580] loss: 0.005875502713024616\n",
      "[step: 17581] loss: 0.00582949910312891\n",
      "[step: 17582] loss: 0.005835963878780603\n",
      "[step: 17583] loss: 0.005782937631011009\n",
      "[step: 17584] loss: 0.005775087047368288\n",
      "[step: 17585] loss: 0.005766155198216438\n",
      "[step: 17586] loss: 0.005725307855755091\n",
      "[step: 17587] loss: 0.005778085440397263\n",
      "[step: 17588] loss: 0.005761608015745878\n",
      "[step: 17589] loss: 0.005698847584426403\n",
      "[step: 17590] loss: 0.005733939819037914\n",
      "[step: 17591] loss: 0.00574800418689847\n",
      "[step: 17592] loss: 0.005716334562748671\n",
      "[step: 17593] loss: 0.005727299954742193\n",
      "[step: 17594] loss: 0.005693153478205204\n",
      "[step: 17595] loss: 0.005639680195599794\n",
      "[step: 17596] loss: 0.0057149119675159454\n",
      "[step: 17597] loss: 0.005712671205401421\n",
      "[step: 17598] loss: 0.00562814436852932\n",
      "[step: 17599] loss: 0.00570013327524066\n",
      "[step: 17600] loss: 0.005705477204173803\n",
      "[step: 17601] loss: 0.005612286739051342\n",
      "[step: 17602] loss: 0.00573864346370101\n",
      "[step: 17603] loss: 0.0057548414915800095\n",
      "[step: 17604] loss: 0.005608299747109413\n",
      "[step: 17605] loss: 0.005828353576362133\n",
      "[step: 17606] loss: 0.005898169707506895\n",
      "[step: 17607] loss: 0.005641954485327005\n",
      "[step: 17608] loss: 0.006045604590326548\n",
      "[step: 17609] loss: 0.00593715813010931\n",
      "[step: 17610] loss: 0.005723693408071995\n",
      "[step: 17611] loss: 0.00598196592181921\n",
      "[step: 17612] loss: 0.005738614127039909\n",
      "[step: 17613] loss: 0.006079831160604954\n",
      "[step: 17614] loss: 0.005825893487781286\n",
      "[step: 17615] loss: 0.0058221775107085705\n",
      "[step: 17616] loss: 0.005883673671633005\n",
      "[step: 17617] loss: 0.005593565758317709\n",
      "[step: 17618] loss: 0.00592897180467844\n",
      "[step: 17619] loss: 0.005964453332126141\n",
      "[step: 17620] loss: 0.0056781405583024025\n",
      "[step: 17621] loss: 0.005989728961139917\n",
      "[step: 17622] loss: 0.0057285502552986145\n",
      "[step: 17623] loss: 0.0057901800610125065\n",
      "[step: 17624] loss: 0.005780561827123165\n",
      "[step: 17625] loss: 0.005678022280335426\n",
      "[step: 17626] loss: 0.006078642792999744\n",
      "[step: 17627] loss: 0.00587753439322114\n",
      "[step: 17628] loss: 0.006251606624573469\n",
      "[step: 17629] loss: 0.006266236770898104\n",
      "[step: 17630] loss: 0.006611902266740799\n",
      "[step: 17631] loss: 0.0055480538867414\n",
      "[step: 17632] loss: 0.006925716530531645\n",
      "[step: 17633] loss: 0.007407433353364468\n",
      "[step: 17634] loss: 0.006916026119142771\n",
      "[step: 17635] loss: 0.0073308274149894714\n",
      "[step: 17636] loss: 0.007028249092400074\n",
      "[step: 17637] loss: 0.00681803235784173\n",
      "[step: 17638] loss: 0.005957426968961954\n",
      "[step: 17639] loss: 0.006990429945290089\n",
      "[step: 17640] loss: 0.00705938832834363\n",
      "[step: 17641] loss: 0.006539881695061922\n",
      "[step: 17642] loss: 0.006079132203012705\n",
      "[step: 17643] loss: 0.006436048541218042\n",
      "[step: 17644] loss: 0.0061803641729056835\n",
      "[step: 17645] loss: 0.006775325164198875\n",
      "[step: 17646] loss: 0.005809798836708069\n",
      "[step: 17647] loss: 0.006971057038754225\n",
      "[step: 17648] loss: 0.0064551495015621185\n",
      "[step: 17649] loss: 0.006665207911282778\n",
      "[step: 17650] loss: 0.006926562637090683\n",
      "[step: 17651] loss: 0.007109260652214289\n",
      "[step: 17652] loss: 0.007084589917212725\n",
      "[step: 17653] loss: 0.007511927280575037\n",
      "[step: 17654] loss: 0.006231165025383234\n",
      "[step: 17655] loss: 0.007891089655458927\n",
      "[step: 17656] loss: 0.008037219755351543\n",
      "[step: 17657] loss: 0.008111356757581234\n",
      "[step: 17658] loss: 0.0071520511992275715\n",
      "[step: 17659] loss: 0.00851431768387556\n",
      "[step: 17660] loss: 0.006355215795338154\n",
      "[step: 17661] loss: 0.007540170568972826\n",
      "[step: 17662] loss: 0.007896716706454754\n",
      "[step: 17663] loss: 0.006460017524659634\n",
      "[step: 17664] loss: 0.007053886074572802\n",
      "[step: 17665] loss: 0.007977943867444992\n",
      "[step: 17666] loss: 0.0072671459056437016\n",
      "[step: 17667] loss: 0.008467104285955429\n",
      "[step: 17668] loss: 0.006451092194765806\n",
      "[step: 17669] loss: 0.007528519257903099\n",
      "[step: 17670] loss: 0.0072880396619439125\n",
      "[step: 17671] loss: 0.007217381149530411\n",
      "[step: 17672] loss: 0.008199675008654594\n",
      "[step: 17673] loss: 0.006980966776609421\n",
      "[step: 17674] loss: 0.006959640886634588\n",
      "[step: 17675] loss: 0.006497676949948072\n",
      "[step: 17676] loss: 0.006937236059457064\n",
      "[step: 17677] loss: 0.006385061424225569\n",
      "[step: 17678] loss: 0.007163934409618378\n",
      "[step: 17679] loss: 0.006560071371495724\n",
      "[step: 17680] loss: 0.007091268431395292\n",
      "[step: 17681] loss: 0.007317773066461086\n",
      "[step: 17682] loss: 0.00669063301756978\n",
      "[step: 17683] loss: 0.007313281297683716\n",
      "[step: 17684] loss: 0.007939542643725872\n",
      "[step: 17685] loss: 0.008762311190366745\n",
      "[step: 17686] loss: 0.008463305421173573\n",
      "[step: 17687] loss: 0.007513686548918486\n",
      "[step: 17688] loss: 0.007456002291291952\n",
      "[step: 17689] loss: 0.007487334311008453\n",
      "[step: 17690] loss: 0.006877823732793331\n",
      "[step: 17691] loss: 0.008069885894656181\n",
      "[step: 17692] loss: 0.00841898936778307\n",
      "[step: 17693] loss: 0.009592845104634762\n",
      "[step: 17694] loss: 0.006765877828001976\n",
      "[step: 17695] loss: 0.007512541953474283\n",
      "[step: 17696] loss: 0.007178241852670908\n",
      "[step: 17697] loss: 0.007123162504285574\n",
      "[step: 17698] loss: 0.007503043860197067\n",
      "[step: 17699] loss: 0.006702240090817213\n",
      "[step: 17700] loss: 0.006524085998535156\n",
      "[step: 17701] loss: 0.0073289754800498486\n",
      "[step: 17702] loss: 0.006380923558026552\n",
      "[step: 17703] loss: 0.0070765395648777485\n",
      "[step: 17704] loss: 0.006861822679638863\n",
      "[step: 17705] loss: 0.0064270892180502415\n",
      "[step: 17706] loss: 0.006861626636236906\n",
      "[step: 17707] loss: 0.006297759711742401\n",
      "[step: 17708] loss: 0.006285448558628559\n",
      "[step: 17709] loss: 0.0063721793703734875\n",
      "[step: 17710] loss: 0.006099567282944918\n",
      "[step: 17711] loss: 0.006491432432085276\n",
      "[step: 17712] loss: 0.005956499837338924\n",
      "[step: 17713] loss: 0.006064975634217262\n",
      "[step: 17714] loss: 0.006098502315580845\n",
      "[step: 17715] loss: 0.0059270234778523445\n",
      "[step: 17716] loss: 0.006012096535414457\n",
      "[step: 17717] loss: 0.005733031313866377\n",
      "[step: 17718] loss: 0.005784300155937672\n",
      "[step: 17719] loss: 0.005843218881636858\n",
      "[step: 17720] loss: 0.005693701095879078\n",
      "[step: 17721] loss: 0.005816105753183365\n",
      "[step: 17722] loss: 0.005804203916341066\n",
      "[step: 17723] loss: 0.005635744892060757\n",
      "[step: 17724] loss: 0.005618652328848839\n",
      "[step: 17725] loss: 0.00564012723043561\n",
      "[step: 17726] loss: 0.005657159723341465\n",
      "[step: 17727] loss: 0.005625005811452866\n",
      "[step: 17728] loss: 0.005576360505074263\n",
      "[step: 17729] loss: 0.00567503459751606\n",
      "[step: 17730] loss: 0.00570916011929512\n",
      "[step: 17731] loss: 0.005496528465300798\n",
      "[step: 17732] loss: 0.005790015682578087\n",
      "[step: 17733] loss: 0.005721295718103647\n",
      "[step: 17734] loss: 0.0056296456605196\n",
      "[step: 17735] loss: 0.005747908726334572\n",
      "[step: 17736] loss: 0.005541498307138681\n",
      "[step: 17737] loss: 0.005592765286564827\n",
      "[step: 17738] loss: 0.0057917023077607155\n",
      "[step: 17739] loss: 0.005780308973044157\n",
      "[step: 17740] loss: 0.00550049776211381\n",
      "[step: 17741] loss: 0.005900868680328131\n",
      "[step: 17742] loss: 0.005727623589336872\n",
      "[step: 17743] loss: 0.005795682314783335\n",
      "[step: 17744] loss: 0.005614152178168297\n",
      "[step: 17745] loss: 0.005624035373330116\n",
      "[step: 17746] loss: 0.0057023013941943645\n",
      "[step: 17747] loss: 0.005798056721687317\n",
      "[step: 17748] loss: 0.005579632706940174\n",
      "[step: 17749] loss: 0.005592608358711004\n",
      "[step: 17750] loss: 0.005534326657652855\n",
      "[step: 17751] loss: 0.005717800930142403\n",
      "[step: 17752] loss: 0.0055208466947078705\n",
      "[step: 17753] loss: 0.005867260042577982\n",
      "[step: 17754] loss: 0.005567650310695171\n",
      "[step: 17755] loss: 0.005722845438867807\n",
      "[step: 17756] loss: 0.005648044403642416\n",
      "[step: 17757] loss: 0.005592923611402512\n",
      "[step: 17758] loss: 0.005852197762578726\n",
      "[step: 17759] loss: 0.005826476030051708\n",
      "[step: 17760] loss: 0.005728997755795717\n",
      "[step: 17761] loss: 0.00580946309491992\n",
      "[step: 17762] loss: 0.005814220756292343\n",
      "[step: 17763] loss: 0.005703846458345652\n",
      "[step: 17764] loss: 0.005654565989971161\n",
      "[step: 17765] loss: 0.005895874463021755\n",
      "[step: 17766] loss: 0.005414991173893213\n",
      "[step: 17767] loss: 0.005822306498885155\n",
      "[step: 17768] loss: 0.005638688802719116\n",
      "[step: 17769] loss: 0.005652160849422216\n",
      "[step: 17770] loss: 0.005582327954471111\n",
      "[step: 17771] loss: 0.005615450441837311\n",
      "[step: 17772] loss: 0.005733173806220293\n",
      "[step: 17773] loss: 0.005883714184165001\n",
      "[step: 17774] loss: 0.005546020809561014\n",
      "[step: 17775] loss: 0.005839942954480648\n",
      "[step: 17776] loss: 0.0057440646924078465\n",
      "[step: 17777] loss: 0.005547033157199621\n",
      "[step: 17778] loss: 0.005865447223186493\n",
      "[step: 17779] loss: 0.005635587498545647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 17780] loss: 0.005643808748573065\n",
      "[step: 17781] loss: 0.005795781500637531\n",
      "[step: 17782] loss: 0.005706798285245895\n",
      "[step: 17783] loss: 0.005729587748646736\n",
      "[step: 17784] loss: 0.005761286709457636\n",
      "[step: 17785] loss: 0.005446800962090492\n",
      "[step: 17786] loss: 0.005526331719011068\n",
      "[step: 17787] loss: 0.005555789452046156\n",
      "[step: 17788] loss: 0.005340742878615856\n",
      "[step: 17789] loss: 0.00551504734903574\n",
      "[step: 17790] loss: 0.005534439813345671\n",
      "[step: 17791] loss: 0.005599759053438902\n",
      "[step: 17792] loss: 0.005443974398076534\n",
      "[step: 17793] loss: 0.005621444899588823\n",
      "[step: 17794] loss: 0.005460258107632399\n",
      "[step: 17795] loss: 0.00540237408131361\n",
      "[step: 17796] loss: 0.005591504275798798\n",
      "[step: 17797] loss: 0.005629749037325382\n",
      "[step: 17798] loss: 0.005364234559237957\n",
      "[step: 17799] loss: 0.005546218249946833\n",
      "[step: 17800] loss: 0.005610298831015825\n",
      "[step: 17801] loss: 0.0054044462740421295\n",
      "[step: 17802] loss: 0.005536955315619707\n",
      "[step: 17803] loss: 0.005694191437214613\n",
      "[step: 17804] loss: 0.005405228119343519\n",
      "[step: 17805] loss: 0.005768501665443182\n",
      "[step: 17806] loss: 0.006257924251258373\n",
      "[step: 17807] loss: 0.0053876410238444805\n",
      "[step: 17808] loss: 0.006530180107802153\n",
      "[step: 17809] loss: 0.006648561917245388\n",
      "[step: 17810] loss: 0.006095430813729763\n",
      "[step: 17811] loss: 0.006255241110920906\n",
      "[step: 17812] loss: 0.00547399278730154\n",
      "[step: 17813] loss: 0.005918662995100021\n",
      "[step: 17814] loss: 0.0054316227324306965\n",
      "[step: 17815] loss: 0.005913376342505217\n",
      "[step: 17816] loss: 0.0055650267750024796\n",
      "[step: 17817] loss: 0.005494505632668734\n",
      "[step: 17818] loss: 0.005730633623898029\n",
      "[step: 17819] loss: 0.005516266915947199\n",
      "[step: 17820] loss: 0.005610506050288677\n",
      "[step: 17821] loss: 0.005649396684020758\n",
      "[step: 17822] loss: 0.005366625729948282\n",
      "[step: 17823] loss: 0.005581995937973261\n",
      "[step: 17824] loss: 0.005501502193510532\n",
      "[step: 17825] loss: 0.005365832708775997\n",
      "[step: 17826] loss: 0.005443863105028868\n",
      "[step: 17827] loss: 0.0053670876659452915\n",
      "[step: 17828] loss: 0.005309516564011574\n",
      "[step: 17829] loss: 0.005361604504287243\n",
      "[step: 17830] loss: 0.005315858405083418\n",
      "[step: 17831] loss: 0.005294872913509607\n",
      "[step: 17832] loss: 0.005294875241816044\n",
      "[step: 17833] loss: 0.005295056849718094\n",
      "[step: 17834] loss: 0.005267465952783823\n",
      "[step: 17835] loss: 0.005261232610791922\n",
      "[step: 17836] loss: 0.005274775438010693\n",
      "[step: 17837] loss: 0.005275126080960035\n",
      "[step: 17838] loss: 0.005287644453346729\n",
      "[step: 17839] loss: 0.0053048888221383095\n",
      "[step: 17840] loss: 0.005302743520587683\n",
      "[step: 17841] loss: 0.005289481021463871\n",
      "[step: 17842] loss: 0.0052286903373897076\n",
      "[step: 17843] loss: 0.005300601944327354\n",
      "[step: 17844] loss: 0.005426080897450447\n",
      "[step: 17845] loss: 0.005357534624636173\n",
      "[step: 17846] loss: 0.005270618479698896\n",
      "[step: 17847] loss: 0.005356543231755495\n",
      "[step: 17848] loss: 0.005366096738725901\n",
      "[step: 17849] loss: 0.005397046450525522\n",
      "[step: 17850] loss: 0.0052338396199047565\n",
      "[step: 17851] loss: 0.005348482169210911\n",
      "[step: 17852] loss: 0.005562948994338512\n",
      "[step: 17853] loss: 0.0054310038685798645\n",
      "[step: 17854] loss: 0.0053274561651051044\n",
      "[step: 17855] loss: 0.005398454610258341\n",
      "[step: 17856] loss: 0.005304386373609304\n",
      "[step: 17857] loss: 0.005251619964838028\n",
      "[step: 17858] loss: 0.005226699635386467\n",
      "[step: 17859] loss: 0.005208218935877085\n",
      "[step: 17860] loss: 0.005306005943566561\n",
      "[step: 17861] loss: 0.0054838452488183975\n",
      "[step: 17862] loss: 0.005627849139273167\n",
      "[step: 17863] loss: 0.005278978031128645\n",
      "[step: 17864] loss: 0.005514917429536581\n",
      "[step: 17865] loss: 0.005467988085001707\n",
      "[step: 17866] loss: 0.005337248090654612\n",
      "[step: 17867] loss: 0.005342581309378147\n",
      "[step: 17868] loss: 0.005567174404859543\n",
      "[step: 17869] loss: 0.0053810556419193745\n",
      "[step: 17870] loss: 0.00537410844117403\n",
      "[step: 17871] loss: 0.005338065326213837\n",
      "[step: 17872] loss: 0.005325767677277327\n",
      "[step: 17873] loss: 0.00529326219111681\n",
      "[step: 17874] loss: 0.0052749281749129295\n",
      "[step: 17875] loss: 0.005227071698755026\n",
      "[step: 17876] loss: 0.005271610338240862\n",
      "[step: 17877] loss: 0.005358070135116577\n",
      "[step: 17878] loss: 0.0056293318048119545\n",
      "[step: 17879] loss: 0.0054095229133963585\n",
      "[step: 17880] loss: 0.005479778628796339\n",
      "[step: 17881] loss: 0.005688928533345461\n",
      "[step: 17882] loss: 0.005272307898849249\n",
      "[step: 17883] loss: 0.005930736195296049\n",
      "[step: 17884] loss: 0.005539331119507551\n",
      "[step: 17885] loss: 0.0053797620348632336\n",
      "[step: 17886] loss: 0.005487936083227396\n",
      "[step: 17887] loss: 0.005325553007423878\n",
      "[step: 17888] loss: 0.005447810981422663\n",
      "[step: 17889] loss: 0.005814495962113142\n",
      "[step: 17890] loss: 0.005563042592257261\n",
      "[step: 17891] loss: 0.005376342684030533\n",
      "[step: 17892] loss: 0.005453169811517\n",
      "[step: 17893] loss: 0.005694638472050428\n",
      "[step: 17894] loss: 0.005262359976768494\n",
      "[step: 17895] loss: 0.0055410489439964294\n",
      "[step: 17896] loss: 0.005854688584804535\n",
      "[step: 17897] loss: 0.0055362568236887455\n",
      "[step: 17898] loss: 0.005683199502527714\n",
      "[step: 17899] loss: 0.0054837786592543125\n",
      "[step: 17900] loss: 0.005433313548564911\n",
      "[step: 17901] loss: 0.005461129359900951\n",
      "[step: 17902] loss: 0.005180760286748409\n",
      "[step: 17903] loss: 0.005357326474040747\n",
      "[step: 17904] loss: 0.005203781183809042\n",
      "[step: 17905] loss: 0.005403114017099142\n",
      "[step: 17906] loss: 0.005413451232016087\n",
      "[step: 17907] loss: 0.00530623272061348\n",
      "[step: 17908] loss: 0.005305383820086718\n",
      "[step: 17909] loss: 0.005347178317606449\n",
      "[step: 17910] loss: 0.005720679648220539\n",
      "[step: 17911] loss: 0.00550618814304471\n",
      "[step: 17912] loss: 0.005449673160910606\n",
      "[step: 17913] loss: 0.0055154310539364815\n",
      "[step: 17914] loss: 0.005199391394853592\n",
      "[step: 17915] loss: 0.0054597994312644005\n",
      "[step: 17916] loss: 0.0060202800668776035\n",
      "[step: 17917] loss: 0.005483146291226149\n",
      "[step: 17918] loss: 0.005511526484042406\n",
      "[step: 17919] loss: 0.005277902819216251\n",
      "[step: 17920] loss: 0.005311660002917051\n",
      "[step: 17921] loss: 0.005325564183294773\n",
      "[step: 17922] loss: 0.005620662122964859\n",
      "[step: 17923] loss: 0.006004698574542999\n",
      "[step: 17924] loss: 0.005369783379137516\n",
      "[step: 17925] loss: 0.006151631474494934\n",
      "[step: 17926] loss: 0.008240585215389729\n",
      "[step: 17927] loss: 0.0066360351629555225\n",
      "[step: 17928] loss: 0.008003718219697475\n",
      "[step: 17929] loss: 0.007211059331893921\n",
      "[step: 17930] loss: 0.008167849853634834\n",
      "[step: 17931] loss: 0.008097975514829159\n",
      "[step: 17932] loss: 0.007895449176430702\n",
      "[step: 17933] loss: 0.008408782072365284\n",
      "[step: 17934] loss: 0.009019729681313038\n",
      "[step: 17935] loss: 0.00858266744762659\n",
      "[step: 17936] loss: 0.0097582396119833\n",
      "[step: 17937] loss: 0.00999916810542345\n",
      "[step: 17938] loss: 0.008687499910593033\n",
      "[step: 17939] loss: 0.008479380048811436\n",
      "[step: 17940] loss: 0.008019457571208477\n",
      "[step: 17941] loss: 0.008567874319851398\n",
      "[step: 17942] loss: 0.008691820316016674\n",
      "[step: 17943] loss: 0.00800490751862526\n",
      "[step: 17944] loss: 0.00767629686743021\n",
      "[step: 17945] loss: 0.007804844528436661\n",
      "[step: 17946] loss: 0.00758579233661294\n",
      "[step: 17947] loss: 0.006968725472688675\n",
      "[step: 17948] loss: 0.007183076348155737\n",
      "[step: 17949] loss: 0.007163212634623051\n",
      "[step: 17950] loss: 0.007630657404661179\n",
      "[step: 17951] loss: 0.006884227506816387\n",
      "[step: 17952] loss: 0.007303807884454727\n",
      "[step: 17953] loss: 0.007055176421999931\n",
      "[step: 17954] loss: 0.006737509276717901\n",
      "[step: 17955] loss: 0.006926544941961765\n",
      "[step: 17956] loss: 0.006824413780122995\n",
      "[step: 17957] loss: 0.006619082763791084\n",
      "[step: 17958] loss: 0.006530649494379759\n",
      "[step: 17959] loss: 0.0064439596608281136\n",
      "[step: 17960] loss: 0.0064659155905246735\n",
      "[step: 17961] loss: 0.006341160740703344\n",
      "[step: 17962] loss: 0.00623341603204608\n",
      "[step: 17963] loss: 0.006338785868138075\n",
      "[step: 17964] loss: 0.006263982970267534\n",
      "[step: 17965] loss: 0.006243525072932243\n",
      "[step: 17966] loss: 0.006253764498978853\n",
      "[step: 17967] loss: 0.00616342481225729\n",
      "[step: 17968] loss: 0.006093265954405069\n",
      "[step: 17969] loss: 0.006100157741457224\n",
      "[step: 17970] loss: 0.0060421014204621315\n",
      "[step: 17971] loss: 0.006063392851501703\n",
      "[step: 17972] loss: 0.005972155369818211\n",
      "[step: 17973] loss: 0.005967459641396999\n",
      "[step: 17974] loss: 0.005898701027035713\n",
      "[step: 17975] loss: 0.005876527633517981\n",
      "[step: 17976] loss: 0.005887533538043499\n",
      "[step: 17977] loss: 0.005851869937032461\n",
      "[step: 17978] loss: 0.005830860231071711\n",
      "[step: 17979] loss: 0.005816775839775801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 17980] loss: 0.005774030927568674\n",
      "[step: 17981] loss: 0.00577219994738698\n",
      "[step: 17982] loss: 0.0057330080308020115\n",
      "[step: 17983] loss: 0.005738978274166584\n",
      "[step: 17984] loss: 0.00570316007360816\n",
      "[step: 17985] loss: 0.005700654350221157\n",
      "[step: 17986] loss: 0.005691011901944876\n",
      "[step: 17987] loss: 0.005673381499946117\n",
      "[step: 17988] loss: 0.005668241064995527\n",
      "[step: 17989] loss: 0.005648656748235226\n",
      "[step: 17990] loss: 0.005652523133903742\n",
      "[step: 17991] loss: 0.005635498557239771\n",
      "[step: 17992] loss: 0.005632941611111164\n",
      "[step: 17993] loss: 0.005623641423881054\n",
      "[step: 17994] loss: 0.005609354469925165\n",
      "[step: 17995] loss: 0.005607059691101313\n",
      "[step: 17996] loss: 0.005598905496299267\n",
      "[step: 17997] loss: 0.005593772046267986\n",
      "[step: 17998] loss: 0.005584781523793936\n",
      "[step: 17999] loss: 0.00557888625189662\n",
      "[step: 18000] loss: 0.005571239162236452\n",
      "[step: 18001] loss: 0.005566777661442757\n",
      "[step: 18002] loss: 0.005560572259128094\n",
      "[step: 18003] loss: 0.005555767100304365\n",
      "[step: 18004] loss: 0.0055509875528514385\n",
      "[step: 18005] loss: 0.0055445111356675625\n",
      "[step: 18006] loss: 0.0055421809665858746\n",
      "[step: 18007] loss: 0.005535411648452282\n",
      "[step: 18008] loss: 0.005532160401344299\n",
      "[step: 18009] loss: 0.00552759412676096\n",
      "[step: 18010] loss: 0.00552292400971055\n",
      "[step: 18011] loss: 0.005518984515219927\n",
      "[step: 18012] loss: 0.005513620562851429\n",
      "[step: 18013] loss: 0.0055094147101044655\n",
      "[step: 18014] loss: 0.0055045923218131065\n",
      "[step: 18015] loss: 0.005500756204128265\n",
      "[step: 18016] loss: 0.005496456753462553\n",
      "[step: 18017] loss: 0.005492497235536575\n",
      "[step: 18018] loss: 0.005487997084856033\n",
      "[step: 18019] loss: 0.0054840524680912495\n",
      "[step: 18020] loss: 0.005480085965245962\n",
      "[step: 18021] loss: 0.00547642121091485\n",
      "[step: 18022] loss: 0.005472677294164896\n",
      "[step: 18023] loss: 0.005468990653753281\n",
      "[step: 18024] loss: 0.0054654632695019245\n",
      "[step: 18025] loss: 0.005461546126753092\n",
      "[step: 18026] loss: 0.005458079278469086\n",
      "[step: 18027] loss: 0.00545431999489665\n",
      "[step: 18028] loss: 0.005450713913887739\n",
      "[step: 18029] loss: 0.00544710410758853\n",
      "[step: 18030] loss: 0.005443695932626724\n",
      "[step: 18031] loss: 0.005439961329102516\n",
      "[step: 18032] loss: 0.005436492618173361\n",
      "[step: 18033] loss: 0.005433015525341034\n",
      "[step: 18034] loss: 0.005429624114185572\n",
      "[step: 18035] loss: 0.005426130723208189\n",
      "[step: 18036] loss: 0.0054227993823587894\n",
      "[step: 18037] loss: 0.005419414956122637\n",
      "[step: 18038] loss: 0.005416030529886484\n",
      "[step: 18039] loss: 0.005412695929408073\n",
      "[step: 18040] loss: 0.005409466102719307\n",
      "[step: 18041] loss: 0.005406119395047426\n",
      "[step: 18042] loss: 0.00540283415466547\n",
      "[step: 18043] loss: 0.005399597343057394\n",
      "[step: 18044] loss: 0.005396316759288311\n",
      "[step: 18045] loss: 0.005393047817051411\n",
      "[step: 18046] loss: 0.005389831028878689\n",
      "[step: 18047] loss: 0.005386588629335165\n",
      "[step: 18048] loss: 0.00538338627666235\n",
      "[step: 18049] loss: 0.005380186252295971\n",
      "[step: 18050] loss: 0.005376976914703846\n",
      "[step: 18051] loss: 0.005373779218643904\n",
      "[step: 18052] loss: 0.005370631814002991\n",
      "[step: 18053] loss: 0.005367443896830082\n",
      "[step: 18054] loss: 0.005364280194044113\n",
      "[step: 18055] loss: 0.005361114162951708\n",
      "[step: 18056] loss: 0.005357959307730198\n",
      "[step: 18057] loss: 0.0053548007272183895\n",
      "[step: 18058] loss: 0.005351659841835499\n",
      "[step: 18059] loss: 0.0053485240787267685\n",
      "[step: 18060] loss: 0.005345379933714867\n",
      "[step: 18061] loss: 0.005342232063412666\n",
      "[step: 18062] loss: 0.005339091643691063\n",
      "[step: 18063] loss: 0.005335969850420952\n",
      "[step: 18064] loss: 0.005332835949957371\n",
      "[step: 18065] loss: 0.005329711362719536\n",
      "[step: 18066] loss: 0.005326584912836552\n",
      "[step: 18067] loss: 0.0053234524093568325\n",
      "[step: 18068] loss: 0.005320330616086721\n",
      "[step: 18069] loss: 0.005317205097526312\n",
      "[step: 18070] loss: 0.005314082372933626\n",
      "[step: 18071] loss: 0.005310980603098869\n",
      "[step: 18072] loss: 0.0053078774362802505\n",
      "[step: 18073] loss: 0.005304777529090643\n",
      "[step: 18074] loss: 0.005301698110997677\n",
      "[step: 18075] loss: 0.00529863266274333\n",
      "[step: 18076] loss: 0.00529558164998889\n",
      "[step: 18077] loss: 0.005292545072734356\n",
      "[step: 18078] loss: 0.005289532709866762\n",
      "[step: 18079] loss: 0.005286535248160362\n",
      "[step: 18080] loss: 0.005283554550260305\n",
      "[step: 18081] loss: 0.005280588287860155\n",
      "[step: 18082] loss: 0.005277647171169519\n",
      "[step: 18083] loss: 0.005274733062833548\n",
      "[step: 18084] loss: 0.0052719018422067165\n",
      "[step: 18085] loss: 0.005269225221127272\n",
      "[step: 18086] loss: 0.005266954656690359\n",
      "[step: 18087] loss: 0.0052657220512628555\n",
      "[step: 18088] loss: 0.005267113447189331\n",
      "[step: 18089] loss: 0.005275115370750427\n",
      "[step: 18090] loss: 0.005298554431647062\n",
      "[step: 18091] loss: 0.005346862133592367\n",
      "[step: 18092] loss: 0.005405761767178774\n",
      "[step: 18093] loss: 0.0053996192291378975\n",
      "[step: 18094] loss: 0.005319071467965841\n",
      "[step: 18095] loss: 0.005256722215563059\n",
      "[step: 18096] loss: 0.005246623419225216\n",
      "[step: 18097] loss: 0.005289201159030199\n",
      "[step: 18098] loss: 0.005378950387239456\n",
      "[step: 18099] loss: 0.0054418109357357025\n",
      "[step: 18100] loss: 0.005368403624743223\n",
      "[step: 18101] loss: 0.005253379233181477\n",
      "[step: 18102] loss: 0.00526770856231451\n",
      "[step: 18103] loss: 0.00535623962059617\n",
      "[step: 18104] loss: 0.005368726793676615\n",
      "[step: 18105] loss: 0.005289400927722454\n",
      "[step: 18106] loss: 0.005221988540142775\n",
      "[step: 18107] loss: 0.005237280856817961\n",
      "[step: 18108] loss: 0.00528959883376956\n",
      "[step: 18109] loss: 0.005303541198372841\n",
      "[step: 18110] loss: 0.00527134258300066\n",
      "[step: 18111] loss: 0.005211469251662493\n",
      "[step: 18112] loss: 0.005222374573349953\n",
      "[step: 18113] loss: 0.005268830340355635\n",
      "[step: 18114] loss: 0.00533007038757205\n",
      "[step: 18115] loss: 0.0053357225842773914\n",
      "[step: 18116] loss: 0.005235995166003704\n",
      "[step: 18117] loss: 0.005204633343964815\n",
      "[step: 18118] loss: 0.0052263871766626835\n",
      "[step: 18119] loss: 0.005283842328935862\n",
      "[step: 18120] loss: 0.005325261037796736\n",
      "[step: 18121] loss: 0.005273926537483931\n",
      "[step: 18122] loss: 0.005228693131357431\n",
      "[step: 18123] loss: 0.005183939356356859\n",
      "[step: 18124] loss: 0.005172677803784609\n",
      "[step: 18125] loss: 0.005191732197999954\n",
      "[step: 18126] loss: 0.0052201696671545506\n",
      "[step: 18127] loss: 0.00527923135086894\n",
      "[step: 18128] loss: 0.005334319546818733\n",
      "[step: 18129] loss: 0.005327848717570305\n",
      "[step: 18130] loss: 0.005237026140093803\n",
      "[step: 18131] loss: 0.005170976277440786\n",
      "[step: 18132] loss: 0.005227057263255119\n",
      "[step: 18133] loss: 0.005341671872884035\n",
      "[step: 18134] loss: 0.005389545112848282\n",
      "[step: 18135] loss: 0.005324882455170155\n",
      "[step: 18136] loss: 0.005194801837205887\n",
      "[step: 18137] loss: 0.005170050542801619\n",
      "[step: 18138] loss: 0.005248486064374447\n",
      "[step: 18139] loss: 0.005314724985510111\n",
      "[step: 18140] loss: 0.005281941033899784\n",
      "[step: 18141] loss: 0.00517534464597702\n",
      "[step: 18142] loss: 0.005131826736032963\n",
      "[step: 18143] loss: 0.005160740111023188\n",
      "[step: 18144] loss: 0.0052151745185256\n",
      "[step: 18145] loss: 0.005224660970270634\n",
      "[step: 18146] loss: 0.00517856702208519\n",
      "[step: 18147] loss: 0.00513507891446352\n",
      "[step: 18148] loss: 0.005121092312037945\n",
      "[step: 18149] loss: 0.005130130331963301\n",
      "[step: 18150] loss: 0.005159244406968355\n",
      "[step: 18151] loss: 0.005189709831029177\n",
      "[step: 18152] loss: 0.005243590567260981\n",
      "[step: 18153] loss: 0.005256006494164467\n",
      "[step: 18154] loss: 0.005170910619199276\n",
      "[step: 18155] loss: 0.0051075429655611515\n",
      "[step: 18156] loss: 0.005119960755109787\n",
      "[step: 18157] loss: 0.005215995479375124\n",
      "[step: 18158] loss: 0.005394004750996828\n",
      "[step: 18159] loss: 0.005531342700123787\n",
      "[step: 18160] loss: 0.005392797291278839\n",
      "[step: 18161] loss: 0.0051462650299072266\n",
      "[step: 18162] loss: 0.005318579263985157\n",
      "[step: 18163] loss: 0.0055788722820580006\n",
      "[step: 18164] loss: 0.005375290289521217\n",
      "[step: 18165] loss: 0.0051815868355333805\n",
      "[step: 18166] loss: 0.005340216681361198\n",
      "[step: 18167] loss: 0.005338932387530804\n",
      "[step: 18168] loss: 0.00518516032025218\n",
      "[step: 18169] loss: 0.005099872127175331\n",
      "[step: 18170] loss: 0.00521566066890955\n",
      "[step: 18171] loss: 0.0053566815331578255\n",
      "[step: 18172] loss: 0.005174767691642046\n",
      "[step: 18173] loss: 0.005096562672406435\n",
      "[step: 18174] loss: 0.005147348158061504\n",
      "[step: 18175] loss: 0.005192681215703487\n",
      "[step: 18176] loss: 0.005143673624843359\n",
      "[step: 18177] loss: 0.005074745509773493\n",
      "[step: 18178] loss: 0.005085744429379702\n",
      "[step: 18179] loss: 0.005148463882505894\n",
      "[step: 18180] loss: 0.005161216016858816\n",
      "[step: 18181] loss: 0.005119571927934885\n",
      "[step: 18182] loss: 0.005061769858002663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 18183] loss: 0.005060825031250715\n",
      "[step: 18184] loss: 0.005103017203509808\n",
      "[step: 18185] loss: 0.005159068387001753\n",
      "[step: 18186] loss: 0.005297591909766197\n",
      "[step: 18187] loss: 0.005337271373718977\n",
      "[step: 18188] loss: 0.005224383436143398\n",
      "[step: 18189] loss: 0.005046218633651733\n",
      "[step: 18190] loss: 0.005106759257614613\n",
      "[step: 18191] loss: 0.0052759405225515366\n",
      "[step: 18192] loss: 0.005347422789782286\n",
      "[step: 18193] loss: 0.005457364954054356\n",
      "[step: 18194] loss: 0.005806459579616785\n",
      "[step: 18195] loss: 0.0058852373622357845\n",
      "[step: 18196] loss: 0.0053439876064658165\n",
      "[step: 18197] loss: 0.0050538708455860615\n",
      "[step: 18198] loss: 0.005432819481939077\n",
      "[step: 18199] loss: 0.006029596086591482\n",
      "[step: 18200] loss: 0.005792183801531792\n",
      "[step: 18201] loss: 0.005253829061985016\n",
      "[step: 18202] loss: 0.005388565827161074\n",
      "[step: 18203] loss: 0.005706359166651964\n",
      "[step: 18204] loss: 0.005401724483817816\n",
      "[step: 18205] loss: 0.00508473813533783\n",
      "[step: 18206] loss: 0.005758262705057859\n",
      "[step: 18207] loss: 0.006128786597400904\n",
      "[step: 18208] loss: 0.005563109181821346\n",
      "[step: 18209] loss: 0.006055629812180996\n",
      "[step: 18210] loss: 0.006420331075787544\n",
      "[step: 18211] loss: 0.006912966258823872\n",
      "[step: 18212] loss: 0.005826706998050213\n",
      "[step: 18213] loss: 0.005918796639889479\n",
      "[step: 18214] loss: 0.006009296048432589\n",
      "[step: 18215] loss: 0.005776907317340374\n",
      "[step: 18216] loss: 0.0056825983338057995\n",
      "[step: 18217] loss: 0.006689984817057848\n",
      "[step: 18218] loss: 0.007709643803536892\n",
      "[step: 18219] loss: 0.007343376986682415\n",
      "[step: 18220] loss: 0.005624921061098576\n",
      "[step: 18221] loss: 0.007284336257725954\n",
      "[step: 18222] loss: 0.0069473665207624435\n",
      "[step: 18223] loss: 0.006311677861958742\n",
      "[step: 18224] loss: 0.006900613661855459\n",
      "[step: 18225] loss: 0.0063295066356658936\n",
      "[step: 18226] loss: 0.006063594948500395\n",
      "[step: 18227] loss: 0.006071953102946281\n",
      "[step: 18228] loss: 0.006532480474561453\n",
      "[step: 18229] loss: 0.005764983594417572\n",
      "[step: 18230] loss: 0.006434801500290632\n",
      "[step: 18231] loss: 0.006397317163646221\n",
      "[step: 18232] loss: 0.006232696585357189\n",
      "[step: 18233] loss: 0.00684740673750639\n",
      "[step: 18234] loss: 0.008133451454341412\n",
      "[step: 18235] loss: 0.00640835240483284\n",
      "[step: 18236] loss: 0.007746199611574411\n",
      "[step: 18237] loss: 0.006564976181834936\n",
      "[step: 18238] loss: 0.006894860882312059\n",
      "[step: 18239] loss: 0.006442017387598753\n",
      "[step: 18240] loss: 0.006334003526717424\n",
      "[step: 18241] loss: 0.00719066197052598\n",
      "[step: 18242] loss: 0.005841194652020931\n",
      "[step: 18243] loss: 0.006335584446787834\n",
      "[step: 18244] loss: 0.006241708528250456\n",
      "[step: 18245] loss: 0.005867963191121817\n",
      "[step: 18246] loss: 0.006509121507406235\n",
      "[step: 18247] loss: 0.005733258090913296\n",
      "[step: 18248] loss: 0.005752899684011936\n",
      "[step: 18249] loss: 0.0057948287576437\n",
      "[step: 18250] loss: 0.005440494976937771\n",
      "[step: 18251] loss: 0.00563354417681694\n",
      "[step: 18252] loss: 0.005464023444801569\n",
      "[step: 18253] loss: 0.005381827242672443\n",
      "[step: 18254] loss: 0.00542909000068903\n",
      "[step: 18255] loss: 0.005351785570383072\n",
      "[step: 18256] loss: 0.005266863852739334\n",
      "[step: 18257] loss: 0.0053533585742115974\n",
      "[step: 18258] loss: 0.005286762956529856\n",
      "[step: 18259] loss: 0.005275832489132881\n",
      "[step: 18260] loss: 0.005283615551888943\n",
      "[step: 18261] loss: 0.005238451063632965\n",
      "[step: 18262] loss: 0.005202066153287888\n",
      "[step: 18263] loss: 0.0051417225040495396\n",
      "[step: 18264] loss: 0.0052064815536141396\n",
      "[step: 18265] loss: 0.005134522449225187\n",
      "[step: 18266] loss: 0.005125205963850021\n",
      "[step: 18267] loss: 0.005174545105546713\n",
      "[step: 18268] loss: 0.005111780483275652\n",
      "[step: 18269] loss: 0.005056376568973064\n",
      "[step: 18270] loss: 0.005122100934386253\n",
      "[step: 18271] loss: 0.005109085235744715\n",
      "[step: 18272] loss: 0.005048256833106279\n",
      "[step: 18273] loss: 0.005088052246719599\n",
      "[step: 18274] loss: 0.005063763353973627\n",
      "[step: 18275] loss: 0.005026086233556271\n",
      "[step: 18276] loss: 0.005022419150918722\n",
      "[step: 18277] loss: 0.005044150166213512\n",
      "[step: 18278] loss: 0.005029240157455206\n",
      "[step: 18279] loss: 0.00499327015131712\n",
      "[step: 18280] loss: 0.0049993013963103294\n",
      "[step: 18281] loss: 0.005010969936847687\n",
      "[step: 18282] loss: 0.00498901167884469\n",
      "[step: 18283] loss: 0.004968125838786364\n",
      "[step: 18284] loss: 0.004981239326298237\n",
      "[step: 18285] loss: 0.004981726408004761\n",
      "[step: 18286] loss: 0.004965190310031176\n",
      "[step: 18287] loss: 0.0049548703245818615\n",
      "[step: 18288] loss: 0.004950425121933222\n",
      "[step: 18289] loss: 0.004958285018801689\n",
      "[step: 18290] loss: 0.004948541056364775\n",
      "[step: 18291] loss: 0.004936534911394119\n",
      "[step: 18292] loss: 0.004943212494254112\n",
      "[step: 18293] loss: 0.004949804395437241\n",
      "[step: 18294] loss: 0.004957736935466528\n",
      "[step: 18295] loss: 0.004971751943230629\n",
      "[step: 18296] loss: 0.005002535879611969\n",
      "[step: 18297] loss: 0.0050412388518452644\n",
      "[step: 18298] loss: 0.005041006952524185\n",
      "[step: 18299] loss: 0.005013958550989628\n",
      "[step: 18300] loss: 0.00496041402220726\n",
      "[step: 18301] loss: 0.004914692137390375\n",
      "[step: 18302] loss: 0.004895082674920559\n",
      "[step: 18303] loss: 0.004892464727163315\n",
      "[step: 18304] loss: 0.004891517572104931\n",
      "[step: 18305] loss: 0.004896349739283323\n",
      "[step: 18306] loss: 0.004917834885418415\n",
      "[step: 18307] loss: 0.004963678307831287\n",
      "[step: 18308] loss: 0.005021946504712105\n",
      "[step: 18309] loss: 0.005070341285318136\n",
      "[step: 18310] loss: 0.005087874364107847\n",
      "[step: 18311] loss: 0.00504150427877903\n",
      "[step: 18312] loss: 0.005017281975597143\n",
      "[step: 18313] loss: 0.004926878958940506\n",
      "[step: 18314] loss: 0.004867727402597666\n",
      "[step: 18315] loss: 0.0048571196384727955\n",
      "[step: 18316] loss: 0.004894756246358156\n",
      "[step: 18317] loss: 0.0049810269847512245\n",
      "[step: 18318] loss: 0.005088777747005224\n",
      "[step: 18319] loss: 0.005316723138093948\n",
      "[step: 18320] loss: 0.005413116421550512\n",
      "[step: 18321] loss: 0.005357064306735992\n",
      "[step: 18322] loss: 0.005067767575383186\n",
      "[step: 18323] loss: 0.004899251740425825\n",
      "[step: 18324] loss: 0.004955182783305645\n",
      "[step: 18325] loss: 0.005090672522783279\n",
      "[step: 18326] loss: 0.0051292842254042625\n",
      "[step: 18327] loss: 0.005088904872536659\n",
      "[step: 18328] loss: 0.005035507958382368\n",
      "[step: 18329] loss: 0.004947564098984003\n",
      "[step: 18330] loss: 0.005011415109038353\n",
      "[step: 18331] loss: 0.005003015510737896\n",
      "[step: 18332] loss: 0.004985637962818146\n",
      "[step: 18333] loss: 0.005021756514906883\n",
      "[step: 18334] loss: 0.005024016369134188\n",
      "[step: 18335] loss: 0.005032057408243418\n",
      "[step: 18336] loss: 0.004907081834971905\n",
      "[step: 18337] loss: 0.004826045129448175\n",
      "[step: 18338] loss: 0.004802832845598459\n",
      "[step: 18339] loss: 0.004838216584175825\n",
      "[step: 18340] loss: 0.004925714805722237\n",
      "[step: 18341] loss: 0.005056122317910194\n",
      "[step: 18342] loss: 0.0053128995932638645\n",
      "[step: 18343] loss: 0.005503256339579821\n",
      "[step: 18344] loss: 0.005683442577719688\n",
      "[step: 18345] loss: 0.006108634639531374\n",
      "[step: 18346] loss: 0.005327626131474972\n",
      "[step: 18347] loss: 0.004955001641064882\n",
      "[step: 18348] loss: 0.005381462164223194\n",
      "[step: 18349] loss: 0.005523069761693478\n",
      "[step: 18350] loss: 0.005309345666319132\n",
      "[step: 18351] loss: 0.005020965356379747\n",
      "[step: 18352] loss: 0.005356853827834129\n",
      "[step: 18353] loss: 0.005381926894187927\n",
      "[step: 18354] loss: 0.0052947248332202435\n",
      "[step: 18355] loss: 0.0065503204241395\n",
      "[step: 18356] loss: 0.009138274937868118\n",
      "[step: 18357] loss: 0.007677910849452019\n",
      "[step: 18358] loss: 0.010335130617022514\n",
      "[step: 18359] loss: 0.009219922125339508\n",
      "[step: 18360] loss: 0.011004232801496983\n",
      "[step: 18361] loss: 0.008053967729210854\n",
      "[step: 18362] loss: 0.008464730344712734\n",
      "[step: 18363] loss: 0.007208160124719143\n",
      "[step: 18364] loss: 0.00793408788740635\n",
      "[step: 18365] loss: 0.008130799978971481\n",
      "[step: 18366] loss: 0.0074503738433122635\n",
      "[step: 18367] loss: 0.006158603820949793\n",
      "[step: 18368] loss: 0.008027076721191406\n",
      "[step: 18369] loss: 0.007296521682292223\n",
      "[step: 18370] loss: 0.007192934863269329\n",
      "[step: 18371] loss: 0.007091308943927288\n",
      "[step: 18372] loss: 0.00659321341663599\n",
      "[step: 18373] loss: 0.006759313400834799\n",
      "[step: 18374] loss: 0.006727911997586489\n",
      "[step: 18375] loss: 0.006806026212871075\n",
      "[step: 18376] loss: 0.006361921783536673\n",
      "[step: 18377] loss: 0.005746247246861458\n",
      "[step: 18378] loss: 0.006240329705178738\n",
      "[step: 18379] loss: 0.0056687649339437485\n",
      "[step: 18380] loss: 0.006324188783764839\n",
      "[step: 18381] loss: 0.006450626067817211\n",
      "[step: 18382] loss: 0.005661061964929104\n",
      "[step: 18383] loss: 0.006439792923629284\n",
      "[step: 18384] loss: 0.005607488565146923\n",
      "[step: 18385] loss: 0.006068713963031769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 18386] loss: 0.005373804830014706\n",
      "[step: 18387] loss: 0.005805568303912878\n",
      "[step: 18388] loss: 0.005846248008310795\n",
      "[step: 18389] loss: 0.005716491024941206\n",
      "[step: 18390] loss: 0.005352425388991833\n",
      "[step: 18391] loss: 0.0056671882048249245\n",
      "[step: 18392] loss: 0.005424015689641237\n",
      "[step: 18393] loss: 0.005658600479364395\n",
      "[step: 18394] loss: 0.00531777972355485\n",
      "[step: 18395] loss: 0.005459029693156481\n",
      "[step: 18396] loss: 0.005363434553146362\n",
      "[step: 18397] loss: 0.0056128730066120625\n",
      "[step: 18398] loss: 0.005443727131932974\n",
      "[step: 18399] loss: 0.005492696072906256\n",
      "[step: 18400] loss: 0.005322335287928581\n",
      "[step: 18401] loss: 0.005411280319094658\n",
      "[step: 18402] loss: 0.005303747486323118\n",
      "[step: 18403] loss: 0.005197999998927116\n",
      "[step: 18404] loss: 0.005224577616900206\n",
      "[step: 18405] loss: 0.0051721916534006596\n",
      "[step: 18406] loss: 0.005215210374444723\n",
      "[step: 18407] loss: 0.005024943966418505\n",
      "[step: 18408] loss: 0.0051618535071611404\n",
      "[step: 18409] loss: 0.0049844952300190926\n",
      "[step: 18410] loss: 0.005113514140248299\n",
      "[step: 18411] loss: 0.0050140987150371075\n",
      "[step: 18412] loss: 0.005091754719614983\n",
      "[step: 18413] loss: 0.004936979617923498\n",
      "[step: 18414] loss: 0.005058990325778723\n",
      "[step: 18415] loss: 0.005014657508581877\n",
      "[step: 18416] loss: 0.005007538013160229\n",
      "[step: 18417] loss: 0.00498106237500906\n",
      "[step: 18418] loss: 0.00496320566162467\n",
      "[step: 18419] loss: 0.004915324971079826\n",
      "[step: 18420] loss: 0.004992714151740074\n",
      "[step: 18421] loss: 0.004924127832055092\n",
      "[step: 18422] loss: 0.004928605165332556\n",
      "[step: 18423] loss: 0.004910535179078579\n",
      "[step: 18424] loss: 0.004887158516794443\n",
      "[step: 18425] loss: 0.00489452201873064\n",
      "[step: 18426] loss: 0.004898674786090851\n",
      "[step: 18427] loss: 0.0048563540913164616\n",
      "[step: 18428] loss: 0.0048563820309937\n",
      "[step: 18429] loss: 0.004870903212577105\n",
      "[step: 18430] loss: 0.004834156949073076\n",
      "[step: 18431] loss: 0.004844933748245239\n",
      "[step: 18432] loss: 0.004828710108995438\n",
      "[step: 18433] loss: 0.004825713112950325\n",
      "[step: 18434] loss: 0.004807651974260807\n",
      "[step: 18435] loss: 0.004830658435821533\n",
      "[step: 18436] loss: 0.0048050121404230595\n",
      "[step: 18437] loss: 0.004797023721039295\n",
      "[step: 18438] loss: 0.004793641623109579\n",
      "[step: 18439] loss: 0.004784774500876665\n",
      "[step: 18440] loss: 0.004775061737746\n",
      "[step: 18441] loss: 0.00478300591930747\n",
      "[step: 18442] loss: 0.004786898382008076\n",
      "[step: 18443] loss: 0.0047719404101371765\n",
      "[step: 18444] loss: 0.004783375654369593\n",
      "[step: 18445] loss: 0.004778676200658083\n",
      "[step: 18446] loss: 0.004786948207765818\n",
      "[step: 18447] loss: 0.00477533508092165\n",
      "[step: 18448] loss: 0.00476885074749589\n",
      "[step: 18449] loss: 0.004758056718856096\n",
      "[step: 18450] loss: 0.0047433627769351006\n",
      "[step: 18451] loss: 0.004737246315926313\n",
      "[step: 18452] loss: 0.004727095365524292\n",
      "[step: 18453] loss: 0.00472328532487154\n",
      "[step: 18454] loss: 0.004716820083558559\n",
      "[step: 18455] loss: 0.004712768364697695\n",
      "[step: 18456] loss: 0.004713377915322781\n",
      "[step: 18457] loss: 0.00471468735486269\n",
      "[step: 18458] loss: 0.004732314497232437\n",
      "[step: 18459] loss: 0.004773423541337252\n",
      "[step: 18460] loss: 0.004946369212120771\n",
      "[step: 18461] loss: 0.005155907943844795\n",
      "[step: 18462] loss: 0.005894310772418976\n",
      "[step: 18463] loss: 0.005068440921604633\n",
      "[step: 18464] loss: 0.005054187029600143\n",
      "[step: 18465] loss: 0.0055467309430241585\n",
      "[step: 18466] loss: 0.004919440485537052\n",
      "[step: 18467] loss: 0.00500694802030921\n",
      "[step: 18468] loss: 0.005394909530878067\n",
      "[step: 18469] loss: 0.004791948478668928\n",
      "[step: 18470] loss: 0.005016796290874481\n",
      "[step: 18471] loss: 0.005586278159171343\n",
      "[step: 18472] loss: 0.004846140276640654\n",
      "[step: 18473] loss: 0.005106011871248484\n",
      "[step: 18474] loss: 0.0059402077458798885\n",
      "[step: 18475] loss: 0.00484026363119483\n",
      "[step: 18476] loss: 0.0054981582798063755\n",
      "[step: 18477] loss: 0.006587108597159386\n",
      "[step: 18478] loss: 0.005064580589532852\n",
      "[step: 18479] loss: 0.007498244754970074\n",
      "[step: 18480] loss: 0.008398126810789108\n",
      "[step: 18481] loss: 0.006037496030330658\n",
      "[step: 18482] loss: 0.008001268841326237\n",
      "[step: 18483] loss: 0.0053992546163499355\n",
      "[step: 18484] loss: 0.006589147262275219\n",
      "[step: 18485] loss: 0.005305053666234016\n",
      "[step: 18486] loss: 0.006395730189979076\n",
      "[step: 18487] loss: 0.005331835243850946\n",
      "[step: 18488] loss: 0.005982648581266403\n",
      "[step: 18489] loss: 0.005250468850135803\n",
      "[step: 18490] loss: 0.006202622316777706\n",
      "[step: 18491] loss: 0.005636550020426512\n",
      "[step: 18492] loss: 0.005905016325414181\n",
      "[step: 18493] loss: 0.005338186863809824\n",
      "[step: 18494] loss: 0.005700177513062954\n",
      "[step: 18495] loss: 0.005463967099785805\n",
      "[step: 18496] loss: 0.005410068202763796\n",
      "[step: 18497] loss: 0.0052748252637684345\n",
      "[step: 18498] loss: 0.005405263043940067\n",
      "[step: 18499] loss: 0.005271653179079294\n",
      "[step: 18500] loss: 0.005226148758083582\n",
      "[step: 18501] loss: 0.0052847969345748425\n",
      "[step: 18502] loss: 0.005257510580122471\n",
      "[step: 18503] loss: 0.005367257632315159\n",
      "[step: 18504] loss: 0.00501609081402421\n",
      "[step: 18505] loss: 0.005250150337815285\n",
      "[step: 18506] loss: 0.004895160906016827\n",
      "[step: 18507] loss: 0.0051964460872113705\n",
      "[step: 18508] loss: 0.004985828883945942\n",
      "[step: 18509] loss: 0.005055942572653294\n",
      "[step: 18510] loss: 0.004829388577491045\n",
      "[step: 18511] loss: 0.004937349818646908\n",
      "[step: 18512] loss: 0.004908775445073843\n",
      "[step: 18513] loss: 0.0047730873338878155\n",
      "[step: 18514] loss: 0.004833930637687445\n",
      "[step: 18515] loss: 0.004838316701352596\n",
      "[step: 18516] loss: 0.004785112105309963\n",
      "[step: 18517] loss: 0.004758384078741074\n",
      "[step: 18518] loss: 0.004774621222168207\n",
      "[step: 18519] loss: 0.004769701510667801\n",
      "[step: 18520] loss: 0.004778377711772919\n",
      "[step: 18521] loss: 0.004735051654279232\n",
      "[step: 18522] loss: 0.004769524559378624\n",
      "[step: 18523] loss: 0.004681546241044998\n",
      "[step: 18524] loss: 0.004704817198216915\n",
      "[step: 18525] loss: 0.004751367494463921\n",
      "[step: 18526] loss: 0.004716069437563419\n",
      "[step: 18527] loss: 0.004682394675910473\n",
      "[step: 18528] loss: 0.004663143306970596\n",
      "[step: 18529] loss: 0.004705988336354494\n",
      "[step: 18530] loss: 0.004672173876315355\n",
      "[step: 18531] loss: 0.0046649109572172165\n",
      "[step: 18532] loss: 0.004637860227376223\n",
      "[step: 18533] loss: 0.004668389447033405\n",
      "[step: 18534] loss: 0.004674086347222328\n",
      "[step: 18535] loss: 0.004670483060181141\n",
      "[step: 18536] loss: 0.0046490211971104145\n",
      "[step: 18537] loss: 0.004611554555594921\n",
      "[step: 18538] loss: 0.004634067416191101\n",
      "[step: 18539] loss: 0.004641890060156584\n",
      "[step: 18540] loss: 0.004669013898819685\n",
      "[step: 18541] loss: 0.0046401130966842175\n",
      "[step: 18542] loss: 0.0046101282350718975\n",
      "[step: 18543] loss: 0.004604408051818609\n",
      "[step: 18544] loss: 0.004596729297190905\n",
      "[step: 18545] loss: 0.00462311552837491\n",
      "[step: 18546] loss: 0.00462441286072135\n",
      "[step: 18547] loss: 0.004657192155718803\n",
      "[step: 18548] loss: 0.004668965004384518\n",
      "[step: 18549] loss: 0.004748647101223469\n",
      "[step: 18550] loss: 0.004743759520351887\n",
      "[step: 18551] loss: 0.004767527803778648\n",
      "[step: 18552] loss: 0.004728173837065697\n",
      "[step: 18553] loss: 0.0046486156061291695\n",
      "[step: 18554] loss: 0.004608270712196827\n",
      "[step: 18555] loss: 0.004607939627021551\n",
      "[step: 18556] loss: 0.004679626319557428\n",
      "[step: 18557] loss: 0.004800308495759964\n",
      "[step: 18558] loss: 0.005162285175174475\n",
      "[step: 18559] loss: 0.005555217619985342\n",
      "[step: 18560] loss: 0.007012506015598774\n",
      "[step: 18561] loss: 0.005327919498085976\n",
      "[step: 18562] loss: 0.0055740452371537685\n",
      "[step: 18563] loss: 0.0080277593806386\n",
      "[step: 18564] loss: 0.005814758129417896\n",
      "[step: 18565] loss: 0.00694535905495286\n",
      "[step: 18566] loss: 0.006944901775568724\n",
      "[step: 18567] loss: 0.0064171114936470985\n",
      "[step: 18568] loss: 0.006387802306562662\n",
      "[step: 18569] loss: 0.007870446890592575\n",
      "[step: 18570] loss: 0.007365915924310684\n",
      "[step: 18571] loss: 0.008353081531822681\n",
      "[step: 18572] loss: 0.010832862928509712\n",
      "[step: 18573] loss: 0.01197994127869606\n",
      "[step: 18574] loss: 0.006919601000845432\n",
      "[step: 18575] loss: 0.01118792686611414\n",
      "[step: 18576] loss: 0.008529308252036572\n",
      "[step: 18577] loss: 0.010256512090563774\n",
      "[step: 18578] loss: 0.008604992181062698\n",
      "[step: 18579] loss: 0.008308067917823792\n",
      "[step: 18580] loss: 0.007915596477687359\n",
      "[step: 18581] loss: 0.008263942785561085\n",
      "[step: 18582] loss: 0.007346293888986111\n",
      "[step: 18583] loss: 0.0076367915607988834\n",
      "[step: 18584] loss: 0.007518034894019365\n",
      "[step: 18585] loss: 0.007793405093252659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 18586] loss: 0.007639813236892223\n",
      "[step: 18587] loss: 0.007102555595338345\n",
      "[step: 18588] loss: 0.006872422993183136\n",
      "[step: 18589] loss: 0.0068177287466824055\n",
      "[step: 18590] loss: 0.0066063292324543\n",
      "[step: 18591] loss: 0.006532617844641209\n",
      "[step: 18592] loss: 0.006248193327337503\n",
      "[step: 18593] loss: 0.006100008729845285\n",
      "[step: 18594] loss: 0.006078800186514854\n",
      "[step: 18595] loss: 0.00617195712402463\n",
      "[step: 18596] loss: 0.005909800063818693\n",
      "[step: 18597] loss: 0.005973354447633028\n",
      "[step: 18598] loss: 0.005902374163269997\n",
      "[step: 18599] loss: 0.00568305142223835\n",
      "[step: 18600] loss: 0.005638969596475363\n",
      "[step: 18601] loss: 0.005593558773398399\n",
      "[step: 18602] loss: 0.005440869368612766\n",
      "[step: 18603] loss: 0.005367896519601345\n",
      "[step: 18604] loss: 0.0052897194400429726\n",
      "[step: 18605] loss: 0.00530994962900877\n",
      "[step: 18606] loss: 0.005091306287795305\n",
      "[step: 18607] loss: 0.005203180480748415\n",
      "[step: 18608] loss: 0.005215504206717014\n",
      "[step: 18609] loss: 0.005183075089007616\n",
      "[step: 18610] loss: 0.0051407101564109325\n",
      "[step: 18611] loss: 0.005050894804298878\n",
      "[step: 18612] loss: 0.00504006864503026\n",
      "[step: 18613] loss: 0.004950250964611769\n",
      "[step: 18614] loss: 0.005003615282475948\n",
      "[step: 18615] loss: 0.004970309790223837\n",
      "[step: 18616] loss: 0.0050009433180093765\n",
      "[step: 18617] loss: 0.004913154058158398\n",
      "[step: 18618] loss: 0.004929001443088055\n",
      "[step: 18619] loss: 0.004922132007777691\n",
      "[step: 18620] loss: 0.0048574539832770824\n",
      "[step: 18621] loss: 0.004880902357399464\n",
      "[step: 18622] loss: 0.0048558772541582584\n",
      "[step: 18623] loss: 0.0048810625448822975\n",
      "[step: 18624] loss: 0.004944973159581423\n",
      "[step: 18625] loss: 0.004915069323033094\n",
      "[step: 18626] loss: 0.004873636178672314\n",
      "[step: 18627] loss: 0.0047876304015517235\n",
      "[step: 18628] loss: 0.00482212146744132\n",
      "[step: 18629] loss: 0.004942235071212053\n",
      "[step: 18630] loss: 0.00479038804769516\n",
      "[step: 18631] loss: 0.004754035267978907\n",
      "[step: 18632] loss: 0.004786535631865263\n",
      "[step: 18633] loss: 0.0047635724768042564\n",
      "[step: 18634] loss: 0.004720809403806925\n",
      "[step: 18635] loss: 0.004718168638646603\n",
      "[step: 18636] loss: 0.004759486299008131\n",
      "[step: 18637] loss: 0.0049098702147603035\n",
      "[step: 18638] loss: 0.004808514378964901\n",
      "[step: 18639] loss: 0.0047578634694218636\n",
      "[step: 18640] loss: 0.004691950045526028\n",
      "[step: 18641] loss: 0.004732133354991674\n",
      "[step: 18642] loss: 0.0047914376482367516\n",
      "[step: 18643] loss: 0.004683377221226692\n",
      "[step: 18644] loss: 0.004663591738790274\n",
      "[step: 18645] loss: 0.004702602978795767\n",
      "[step: 18646] loss: 0.004670499358326197\n",
      "[step: 18647] loss: 0.004641165491193533\n",
      "[step: 18648] loss: 0.004629414528608322\n",
      "[step: 18649] loss: 0.004641639534384012\n",
      "[step: 18650] loss: 0.004709804896265268\n",
      "[step: 18651] loss: 0.004706954583525658\n",
      "[step: 18652] loss: 0.00476003997027874\n",
      "[step: 18653] loss: 0.004659620579332113\n",
      "[step: 18654] loss: 0.00460987351834774\n",
      "[step: 18655] loss: 0.0046271514147520065\n",
      "[step: 18656] loss: 0.00465273717418313\n",
      "[step: 18657] loss: 0.004681273829191923\n",
      "[step: 18658] loss: 0.004612574353814125\n",
      "[step: 18659] loss: 0.004575884435325861\n",
      "[step: 18660] loss: 0.004587406758219004\n",
      "[step: 18661] loss: 0.0046333856880664825\n",
      "[step: 18662] loss: 0.004861779976636171\n",
      "[step: 18663] loss: 0.004925187211483717\n",
      "[step: 18664] loss: 0.005414088722318411\n",
      "[step: 18665] loss: 0.004746020305901766\n",
      "[step: 18666] loss: 0.004892774857580662\n",
      "[step: 18667] loss: 0.005518491845577955\n",
      "[step: 18668] loss: 0.004699563607573509\n",
      "[step: 18669] loss: 0.005423056427389383\n",
      "[step: 18670] loss: 0.007660826202481985\n",
      "[step: 18671] loss: 0.005222804378718138\n",
      "[step: 18672] loss: 0.007443568669259548\n",
      "[step: 18673] loss: 0.006346319802105427\n",
      "[step: 18674] loss: 0.0066108666360378265\n",
      "[step: 18675] loss: 0.007150555960834026\n",
      "[step: 18676] loss: 0.005955940578132868\n",
      "[step: 18677] loss: 0.006547724828124046\n",
      "[step: 18678] loss: 0.006507148966193199\n",
      "[step: 18679] loss: 0.006486665923148394\n",
      "[step: 18680] loss: 0.006313586141914129\n",
      "[step: 18681] loss: 0.006204887293279171\n",
      "[step: 18682] loss: 0.007491640280932188\n",
      "[step: 18683] loss: 0.007827745750546455\n",
      "[step: 18684] loss: 0.0063119689002633095\n",
      "[step: 18685] loss: 0.008028311654925346\n",
      "[step: 18686] loss: 0.00697981147095561\n",
      "[step: 18687] loss: 0.008535370230674744\n",
      "[step: 18688] loss: 0.006393864285200834\n",
      "[step: 18689] loss: 0.010095550678670406\n",
      "[step: 18690] loss: 0.006038415711373091\n",
      "[step: 18691] loss: 0.007741536479443312\n",
      "[step: 18692] loss: 0.006651261821389198\n",
      "[step: 18693] loss: 0.006139699835330248\n",
      "[step: 18694] loss: 0.007717217318713665\n",
      "[step: 18695] loss: 0.0062034414149820805\n",
      "[step: 18696] loss: 0.006059484090656042\n",
      "[step: 18697] loss: 0.00732142711058259\n",
      "[step: 18698] loss: 0.005689379759132862\n",
      "[step: 18699] loss: 0.007056358270347118\n",
      "[step: 18700] loss: 0.006263545248657465\n",
      "[step: 18701] loss: 0.0056550707668066025\n",
      "[step: 18702] loss: 0.006549300625920296\n",
      "[step: 18703] loss: 0.005417108070105314\n",
      "[step: 18704] loss: 0.005736596882343292\n",
      "[step: 18705] loss: 0.005837569013237953\n",
      "[step: 18706] loss: 0.005273870192468166\n",
      "[step: 18707] loss: 0.00581828597933054\n",
      "[step: 18708] loss: 0.005581722129136324\n",
      "[step: 18709] loss: 0.005521072074770927\n",
      "[step: 18710] loss: 0.005389319732785225\n",
      "[step: 18711] loss: 0.005229053553193808\n",
      "[step: 18712] loss: 0.005380944814532995\n",
      "[step: 18713] loss: 0.00521068973466754\n",
      "[step: 18714] loss: 0.00498886639252305\n",
      "[step: 18715] loss: 0.005114943720400333\n",
      "[step: 18716] loss: 0.00514771044254303\n",
      "[step: 18717] loss: 0.004863384645432234\n",
      "[step: 18718] loss: 0.0049888514913618565\n",
      "[step: 18719] loss: 0.004874135367572308\n",
      "[step: 18720] loss: 0.004975926596671343\n",
      "[step: 18721] loss: 0.004863893613219261\n",
      "[step: 18722] loss: 0.004782760050147772\n",
      "[step: 18723] loss: 0.004897823091596365\n",
      "[step: 18724] loss: 0.004743895027786493\n",
      "[step: 18725] loss: 0.0047284201718866825\n",
      "[step: 18726] loss: 0.004797643981873989\n",
      "[step: 18727] loss: 0.004756671842187643\n",
      "[step: 18728] loss: 0.004710712470114231\n",
      "[step: 18729] loss: 0.004743798635900021\n",
      "[step: 18730] loss: 0.004696878604590893\n",
      "[step: 18731] loss: 0.004695771262049675\n",
      "[step: 18732] loss: 0.0046618967317044735\n",
      "[step: 18733] loss: 0.004695291165262461\n",
      "[step: 18734] loss: 0.004702343605458736\n",
      "[step: 18735] loss: 0.0046376329846680164\n",
      "[step: 18736] loss: 0.004625610541552305\n",
      "[step: 18737] loss: 0.004644043277949095\n",
      "[step: 18738] loss: 0.00463098706677556\n",
      "[step: 18739] loss: 0.004623272456228733\n",
      "[step: 18740] loss: 0.004586133640259504\n",
      "[step: 18741] loss: 0.004603932145982981\n",
      "[step: 18742] loss: 0.004594620782881975\n",
      "[step: 18743] loss: 0.004575757775455713\n",
      "[step: 18744] loss: 0.004577973857522011\n",
      "[step: 18745] loss: 0.0045704469084739685\n",
      "[step: 18746] loss: 0.004564282018691301\n",
      "[step: 18747] loss: 0.0045619565062224865\n",
      "[step: 18748] loss: 0.00455059390515089\n",
      "[step: 18749] loss: 0.004551989492028952\n",
      "[step: 18750] loss: 0.004540725611150265\n",
      "[step: 18751] loss: 0.004534917883574963\n",
      "[step: 18752] loss: 0.004528934136033058\n",
      "[step: 18753] loss: 0.004521282855421305\n",
      "[step: 18754] loss: 0.004522054456174374\n",
      "[step: 18755] loss: 0.004520895890891552\n",
      "[step: 18756] loss: 0.004512868355959654\n",
      "[step: 18757] loss: 0.004512379877269268\n",
      "[step: 18758] loss: 0.004506031982600689\n",
      "[step: 18759] loss: 0.00450561661273241\n",
      "[step: 18760] loss: 0.004497702699154615\n",
      "[step: 18761] loss: 0.004494243301451206\n",
      "[step: 18762] loss: 0.004493647255003452\n",
      "[step: 18763] loss: 0.004501920659095049\n",
      "[step: 18764] loss: 0.004522273316979408\n",
      "[step: 18765] loss: 0.004608917515724897\n",
      "[step: 18766] loss: 0.004598670639097691\n",
      "[step: 18767] loss: 0.004726756364107132\n",
      "[step: 18768] loss: 0.004538097884505987\n",
      "[step: 18769] loss: 0.004480330273509026\n",
      "[step: 18770] loss: 0.004518360830843449\n",
      "[step: 18771] loss: 0.004533443134278059\n",
      "[step: 18772] loss: 0.004573732148855925\n",
      "[step: 18773] loss: 0.004488818813115358\n",
      "[step: 18774] loss: 0.0044530946761369705\n",
      "[step: 18775] loss: 0.00445702625438571\n",
      "[step: 18776] loss: 0.004498658236116171\n",
      "[step: 18777] loss: 0.0046798475086688995\n",
      "[step: 18778] loss: 0.00469103641808033\n",
      "[step: 18779] loss: 0.0050529539585113525\n",
      "[step: 18780] loss: 0.0045763785019516945\n",
      "[step: 18781] loss: 0.004507644567638636\n",
      "[step: 18782] loss: 0.0046206009574234486\n",
      "[step: 18783] loss: 0.0044986046850681305\n",
      "[step: 18784] loss: 0.004483447875827551\n",
      "[step: 18785] loss: 0.00455325935035944\n",
      "[step: 18786] loss: 0.004513212479650974\n",
      "[step: 18787] loss: 0.004500233102589846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 18788] loss: 0.004467335529625416\n",
      "[step: 18789] loss: 0.0044807931408286095\n",
      "[step: 18790] loss: 0.004502824507653713\n",
      "[step: 18791] loss: 0.004695832263678312\n",
      "[step: 18792] loss: 0.004612765274941921\n",
      "[step: 18793] loss: 0.004801631905138493\n",
      "[step: 18794] loss: 0.0046591004356741905\n",
      "[step: 18795] loss: 0.004614488687366247\n",
      "[step: 18796] loss: 0.0045671965926885605\n",
      "[step: 18797] loss: 0.004468727391213179\n",
      "[step: 18798] loss: 0.004512289073318243\n",
      "[step: 18799] loss: 0.004649861250072718\n",
      "[step: 18800] loss: 0.004781606141477823\n",
      "[step: 18801] loss: 0.005051744636148214\n",
      "[step: 18802] loss: 0.006334652192890644\n",
      "[step: 18803] loss: 0.005150485318154097\n",
      "[step: 18804] loss: 0.005152535624802113\n",
      "[step: 18805] loss: 0.005126262083649635\n",
      "[step: 18806] loss: 0.0047925193794071674\n",
      "[step: 18807] loss: 0.0053900438360869884\n",
      "[step: 18808] loss: 0.004950681701302528\n",
      "[step: 18809] loss: 0.004746188875287771\n",
      "[step: 18810] loss: 0.005201251246035099\n",
      "[step: 18811] loss: 0.004959616344422102\n",
      "[step: 18812] loss: 0.0049528200179338455\n",
      "[step: 18813] loss: 0.006452619098126888\n",
      "[step: 18814] loss: 0.005538426339626312\n",
      "[step: 18815] loss: 0.005959006492048502\n",
      "[step: 18816] loss: 0.0064149703830480576\n",
      "[step: 18817] loss: 0.006165415979921818\n",
      "[step: 18818] loss: 0.005112736485898495\n",
      "[step: 18819] loss: 0.006060889922082424\n",
      "[step: 18820] loss: 0.0055154478177428246\n",
      "[step: 18821] loss: 0.005321703851222992\n",
      "[step: 18822] loss: 0.006791278254240751\n",
      "[step: 18823] loss: 0.0058555337600409985\n",
      "[step: 18824] loss: 0.005655535496771336\n",
      "[step: 18825] loss: 0.006058509927242994\n",
      "[step: 18826] loss: 0.0051920609548687935\n",
      "[step: 18827] loss: 0.005974640604108572\n",
      "[step: 18828] loss: 0.0050805010832846165\n",
      "[step: 18829] loss: 0.005619158037006855\n",
      "[step: 18830] loss: 0.00553520955145359\n",
      "[step: 18831] loss: 0.005701225716620684\n",
      "[step: 18832] loss: 0.0058240205980837345\n",
      "[step: 18833] loss: 0.0057194968685507774\n",
      "[step: 18834] loss: 0.005337935406714678\n",
      "[step: 18835] loss: 0.005671695340424776\n",
      "[step: 18836] loss: 0.005695686209946871\n",
      "[step: 18837] loss: 0.0057848417200148106\n",
      "[step: 18838] loss: 0.005825648084282875\n",
      "[step: 18839] loss: 0.005473965313285589\n",
      "[step: 18840] loss: 0.005333844106644392\n",
      "[step: 18841] loss: 0.0057162391021847725\n",
      "[step: 18842] loss: 0.005129179917275906\n",
      "[step: 18843] loss: 0.0055549731478095055\n",
      "[step: 18844] loss: 0.004982444923371077\n",
      "[step: 18845] loss: 0.005329979117959738\n",
      "[step: 18846] loss: 0.005139921326190233\n",
      "[step: 18847] loss: 0.005314428359270096\n",
      "[step: 18848] loss: 0.005484926514327526\n",
      "[step: 18849] loss: 0.005191047675907612\n",
      "[step: 18850] loss: 0.005824076011776924\n",
      "[step: 18851] loss: 0.005272303242236376\n",
      "[step: 18852] loss: 0.00548006733879447\n",
      "[step: 18853] loss: 0.004985947161912918\n",
      "[step: 18854] loss: 0.0055304537527263165\n",
      "[step: 18855] loss: 0.0058693657629191875\n",
      "[step: 18856] loss: 0.00533650116994977\n",
      "[step: 18857] loss: 0.005411363672465086\n",
      "[step: 18858] loss: 0.005187361966818571\n",
      "[step: 18859] loss: 0.0048958430998027325\n",
      "[step: 18860] loss: 0.005062248092144728\n",
      "[step: 18861] loss: 0.005015210714191198\n",
      "[step: 18862] loss: 0.004825187847018242\n",
      "[step: 18863] loss: 0.004960008431226015\n",
      "[step: 18864] loss: 0.004844523500651121\n",
      "[step: 18865] loss: 0.004778179340064526\n",
      "[step: 18866] loss: 0.004718366544693708\n",
      "[step: 18867] loss: 0.00473004998639226\n",
      "[step: 18868] loss: 0.004738249816000462\n",
      "[step: 18869] loss: 0.004642230924218893\n",
      "[step: 18870] loss: 0.004605687689036131\n",
      "[step: 18871] loss: 0.004681807942688465\n",
      "[step: 18872] loss: 0.0045960815623402596\n",
      "[step: 18873] loss: 0.004547940567135811\n",
      "[step: 18874] loss: 0.004582949448376894\n",
      "[step: 18875] loss: 0.004602262284606695\n",
      "[step: 18876] loss: 0.004618465900421143\n",
      "[step: 18877] loss: 0.00453865947201848\n",
      "[step: 18878] loss: 0.004513489548116922\n",
      "[step: 18879] loss: 0.004498799331486225\n",
      "[step: 18880] loss: 0.004540045280009508\n",
      "[step: 18881] loss: 0.00458332896232605\n",
      "[step: 18882] loss: 0.004508972633630037\n",
      "[step: 18883] loss: 0.004493605345487595\n",
      "[step: 18884] loss: 0.0045011392794549465\n",
      "[step: 18885] loss: 0.004461318254470825\n",
      "[step: 18886] loss: 0.004467745311558247\n",
      "[step: 18887] loss: 0.004458887968212366\n",
      "[step: 18888] loss: 0.0044187940657138824\n",
      "[step: 18889] loss: 0.004411339294165373\n",
      "[step: 18890] loss: 0.004427745006978512\n",
      "[step: 18891] loss: 0.00444955937564373\n",
      "[step: 18892] loss: 0.004458151292055845\n",
      "[step: 18893] loss: 0.004472872242331505\n",
      "[step: 18894] loss: 0.004622202832251787\n",
      "[step: 18895] loss: 0.0045230938121676445\n",
      "[step: 18896] loss: 0.004483362659811974\n",
      "[step: 18897] loss: 0.004415759816765785\n",
      "[step: 18898] loss: 0.004432935733348131\n",
      "[step: 18899] loss: 0.004523729905486107\n",
      "[step: 18900] loss: 0.0044376300647854805\n",
      "[step: 18901] loss: 0.004367539193481207\n",
      "[step: 18902] loss: 0.004395392257720232\n",
      "[step: 18903] loss: 0.004500638227909803\n",
      "[step: 18904] loss: 0.004914497956633568\n",
      "[step: 18905] loss: 0.004756854381412268\n",
      "[step: 18906] loss: 0.004767787177115679\n",
      "[step: 18907] loss: 0.004401205573230982\n",
      "[step: 18908] loss: 0.004558438900858164\n",
      "[step: 18909] loss: 0.004936107434332371\n",
      "[step: 18910] loss: 0.004434484988451004\n",
      "[step: 18911] loss: 0.005039461888372898\n",
      "[step: 18912] loss: 0.00664913933724165\n",
      "[step: 18913] loss: 0.0051167369820177555\n",
      "[step: 18914] loss: 0.008497573435306549\n",
      "[step: 18915] loss: 0.007914368063211441\n",
      "[step: 18916] loss: 0.005827686749398708\n",
      "[step: 18917] loss: 0.0074181645177304745\n",
      "[step: 18918] loss: 0.006108326371759176\n",
      "[step: 18919] loss: 0.006960949394851923\n",
      "[step: 18920] loss: 0.006333226803690195\n",
      "[step: 18921] loss: 0.00621468760073185\n",
      "[step: 18922] loss: 0.006647157017141581\n",
      "[step: 18923] loss: 0.00620146794244647\n",
      "[step: 18924] loss: 0.00617241719737649\n",
      "[step: 18925] loss: 0.006445523351430893\n",
      "[step: 18926] loss: 0.005958515219390392\n",
      "[step: 18927] loss: 0.0069137332029640675\n",
      "[step: 18928] loss: 0.004983795341104269\n",
      "[step: 18929] loss: 0.005970906466245651\n",
      "[step: 18930] loss: 0.005284612067043781\n",
      "[step: 18931] loss: 0.005293572787195444\n",
      "[step: 18932] loss: 0.005519283469766378\n",
      "[step: 18933] loss: 0.004867768846452236\n",
      "[step: 18934] loss: 0.00560643570497632\n",
      "[step: 18935] loss: 0.005458620842546225\n",
      "[step: 18936] loss: 0.005294937174767256\n",
      "[step: 18937] loss: 0.004853370599448681\n",
      "[step: 18938] loss: 0.004843630827963352\n",
      "[step: 18939] loss: 0.005124631803482771\n",
      "[step: 18940] loss: 0.004848883021622896\n",
      "[step: 18941] loss: 0.005015987437218428\n",
      "[step: 18942] loss: 0.004786628298461437\n",
      "[step: 18943] loss: 0.0052040936425328255\n",
      "[step: 18944] loss: 0.00476909289136529\n",
      "[step: 18945] loss: 0.004939846228808165\n",
      "[step: 18946] loss: 0.005098581314086914\n",
      "[step: 18947] loss: 0.004830913618206978\n",
      "[step: 18948] loss: 0.005118135362863541\n",
      "[step: 18949] loss: 0.004706499166786671\n",
      "[step: 18950] loss: 0.00500015402212739\n",
      "[step: 18951] loss: 0.0047034816816449165\n",
      "[step: 18952] loss: 0.004760688170790672\n",
      "[step: 18953] loss: 0.0049034771509468555\n",
      "[step: 18954] loss: 0.0046731093898415565\n",
      "[step: 18955] loss: 0.004967498127371073\n",
      "[step: 18956] loss: 0.005138080101460218\n",
      "[step: 18957] loss: 0.004608041141182184\n",
      "[step: 18958] loss: 0.0052050515078008175\n",
      "[step: 18959] loss: 0.005795060191303492\n",
      "[step: 18960] loss: 0.005587022751569748\n",
      "[step: 18961] loss: 0.005267394240945578\n",
      "[step: 18962] loss: 0.004941378720104694\n",
      "[step: 18963] loss: 0.005407912191003561\n",
      "[step: 18964] loss: 0.004689705092459917\n",
      "[step: 18965] loss: 0.005357200279831886\n",
      "[step: 18966] loss: 0.004708469845354557\n",
      "[step: 18967] loss: 0.004919153172522783\n",
      "[step: 18968] loss: 0.0049531953409314156\n",
      "[step: 18969] loss: 0.0050552827306091785\n",
      "[step: 18970] loss: 0.005799500271677971\n",
      "[step: 18971] loss: 0.0049166614189744\n",
      "[step: 18972] loss: 0.005617580376565456\n",
      "[step: 18973] loss: 0.004593809600919485\n",
      "[step: 18974] loss: 0.005147495307028294\n",
      "[step: 18975] loss: 0.004659796133637428\n",
      "[step: 18976] loss: 0.005201954860240221\n",
      "[step: 18977] loss: 0.0049290163442492485\n",
      "[step: 18978] loss: 0.005215057171881199\n",
      "[step: 18979] loss: 0.004778448957949877\n",
      "[step: 18980] loss: 0.005056802183389664\n",
      "[step: 18981] loss: 0.005063444375991821\n",
      "[step: 18982] loss: 0.005057241767644882\n",
      "[step: 18983] loss: 0.004881299566477537\n",
      "[step: 18984] loss: 0.004685800988227129\n",
      "[step: 18985] loss: 0.004762290045619011\n",
      "[step: 18986] loss: 0.004572620149701834\n",
      "[step: 18987] loss: 0.004711532965302467\n",
      "[step: 18988] loss: 0.004492935258895159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 18989] loss: 0.004593329969793558\n",
      "[step: 18990] loss: 0.0045057134702801704\n",
      "[step: 18991] loss: 0.004495920147746801\n",
      "[step: 18992] loss: 0.004508478567004204\n",
      "[step: 18993] loss: 0.0044906302355229855\n",
      "[step: 18994] loss: 0.004711706191301346\n",
      "[step: 18995] loss: 0.004841862712055445\n",
      "[step: 18996] loss: 0.004629738628864288\n",
      "[step: 18997] loss: 0.004961938597261906\n",
      "[step: 18998] loss: 0.004906774498522282\n",
      "[step: 18999] loss: 0.004726710729300976\n",
      "[step: 19000] loss: 0.004881271161139011\n",
      "[step: 19001] loss: 0.0047131869941949844\n",
      "[step: 19002] loss: 0.004952198825776577\n",
      "[step: 19003] loss: 0.004737427458167076\n",
      "[step: 19004] loss: 0.004494949243962765\n",
      "[step: 19005] loss: 0.004873364232480526\n",
      "[step: 19006] loss: 0.004458419047296047\n",
      "[step: 19007] loss: 0.0046956888400018215\n",
      "[step: 19008] loss: 0.004843733739107847\n",
      "[step: 19009] loss: 0.004587164148688316\n",
      "[step: 19010] loss: 0.004865423310548067\n",
      "[step: 19011] loss: 0.005105313379317522\n",
      "[step: 19012] loss: 0.004628825932741165\n",
      "[step: 19013] loss: 0.005261403974145651\n",
      "[step: 19014] loss: 0.004989920184016228\n",
      "[step: 19015] loss: 0.004992690868675709\n",
      "[step: 19016] loss: 0.0048456075601279736\n",
      "[step: 19017] loss: 0.004620273131877184\n",
      "[step: 19018] loss: 0.00503449747338891\n",
      "[step: 19019] loss: 0.004451736807823181\n",
      "[step: 19020] loss: 0.004989468026906252\n",
      "[step: 19021] loss: 0.005186927039176226\n",
      "[step: 19022] loss: 0.004458444658666849\n",
      "[step: 19023] loss: 0.005297870375216007\n",
      "[step: 19024] loss: 0.005183992441743612\n",
      "[step: 19025] loss: 0.004636130761355162\n",
      "[step: 19026] loss: 0.005618909373879433\n",
      "[step: 19027] loss: 0.004634900949895382\n",
      "[step: 19028] loss: 0.005009803455322981\n",
      "[step: 19029] loss: 0.004859241656959057\n",
      "[step: 19030] loss: 0.0051150270737707615\n",
      "[step: 19031] loss: 0.00569153530523181\n",
      "[step: 19032] loss: 0.0051938011310994625\n",
      "[step: 19033] loss: 0.004960658028721809\n",
      "[step: 19034] loss: 0.004695121664553881\n",
      "[step: 19035] loss: 0.004927623551338911\n",
      "[step: 19036] loss: 0.004711950197815895\n",
      "[step: 19037] loss: 0.0048443651758134365\n",
      "[step: 19038] loss: 0.0044888765551149845\n",
      "[step: 19039] loss: 0.0046440791338682175\n",
      "[step: 19040] loss: 0.004555675201117992\n",
      "[step: 19041] loss: 0.004641771782189608\n",
      "[step: 19042] loss: 0.004995821509510279\n",
      "[step: 19043] loss: 0.004601161926984787\n",
      "[step: 19044] loss: 0.005354170221835375\n",
      "[step: 19045] loss: 0.006149218883365393\n",
      "[step: 19046] loss: 0.00537888566032052\n",
      "[step: 19047] loss: 0.006974671967327595\n",
      "[step: 19048] loss: 0.009347247891128063\n",
      "[step: 19049] loss: 0.008909516036510468\n",
      "[step: 19050] loss: 0.007233165670186281\n",
      "[step: 19051] loss: 0.007833882234990597\n",
      "[step: 19052] loss: 0.006009937729686499\n",
      "[step: 19053] loss: 0.006304004229605198\n",
      "[step: 19054] loss: 0.005970591679215431\n",
      "[step: 19055] loss: 0.006074738223105669\n",
      "[step: 19056] loss: 0.005755802616477013\n",
      "[step: 19057] loss: 0.006157597992569208\n",
      "[step: 19058] loss: 0.005785536952316761\n",
      "[step: 19059] loss: 0.005139433778822422\n",
      "[step: 19060] loss: 0.005214768927544355\n",
      "[step: 19061] loss: 0.005433604121208191\n",
      "[step: 19062] loss: 0.005196099169552326\n",
      "[step: 19063] loss: 0.0050673289224505424\n",
      "[step: 19064] loss: 0.005052829161286354\n",
      "[step: 19065] loss: 0.004851922392845154\n",
      "[step: 19066] loss: 0.004829164128750563\n",
      "[step: 19067] loss: 0.004831305705010891\n",
      "[step: 19068] loss: 0.0047554802149534225\n",
      "[step: 19069] loss: 0.004764558747410774\n",
      "[step: 19070] loss: 0.00460934778675437\n",
      "[step: 19071] loss: 0.0045213657431304455\n",
      "[step: 19072] loss: 0.004750732332468033\n",
      "[step: 19073] loss: 0.004629814997315407\n",
      "[step: 19074] loss: 0.004518907982856035\n",
      "[step: 19075] loss: 0.0046070534735918045\n",
      "[step: 19076] loss: 0.004582864698022604\n",
      "[step: 19077] loss: 0.004431190434843302\n",
      "[step: 19078] loss: 0.004548839759081602\n",
      "[step: 19079] loss: 0.004643104504793882\n",
      "[step: 19080] loss: 0.004466540180146694\n",
      "[step: 19081] loss: 0.004613639786839485\n",
      "[step: 19082] loss: 0.004759497474879026\n",
      "[step: 19083] loss: 0.004548782017081976\n",
      "[step: 19084] loss: 0.004941452294588089\n",
      "[step: 19085] loss: 0.0047714728862047195\n",
      "[step: 19086] loss: 0.004607815761119127\n",
      "[step: 19087] loss: 0.004849386867135763\n",
      "[step: 19088] loss: 0.004722091369330883\n",
      "[step: 19089] loss: 0.004790483508259058\n",
      "[step: 19090] loss: 0.004777337424457073\n",
      "[step: 19091] loss: 0.004461644683033228\n",
      "[step: 19092] loss: 0.004719235934317112\n",
      "[step: 19093] loss: 0.004389225039631128\n",
      "[step: 19094] loss: 0.004604044836014509\n",
      "[step: 19095] loss: 0.0047604599967598915\n",
      "[step: 19096] loss: 0.00438655074685812\n",
      "[step: 19097] loss: 0.004824098665267229\n",
      "[step: 19098] loss: 0.004783151671290398\n",
      "[step: 19099] loss: 0.004475060850381851\n",
      "[step: 19100] loss: 0.004858764819800854\n",
      "[step: 19101] loss: 0.00454613147303462\n",
      "[step: 19102] loss: 0.004610093310475349\n",
      "[step: 19103] loss: 0.00458257831633091\n",
      "[step: 19104] loss: 0.004356739576905966\n",
      "[step: 19105] loss: 0.004537889268249273\n",
      "[step: 19106] loss: 0.004324242938309908\n",
      "[step: 19107] loss: 0.00464195292443037\n",
      "[step: 19108] loss: 0.004993300419300795\n",
      "[step: 19109] loss: 0.004531808663159609\n",
      "[step: 19110] loss: 0.00555279478430748\n",
      "[step: 19111] loss: 0.006566579919308424\n",
      "[step: 19112] loss: 0.0055605145171284676\n",
      "[step: 19113] loss: 0.006066753063350916\n",
      "[step: 19114] loss: 0.00547247426584363\n",
      "[step: 19115] loss: 0.005741485394537449\n",
      "[step: 19116] loss: 0.005314503330737352\n",
      "[step: 19117] loss: 0.005338072311133146\n",
      "[step: 19118] loss: 0.0047366200014948845\n",
      "[step: 19119] loss: 0.005606740713119507\n",
      "[step: 19120] loss: 0.0045264707878232\n",
      "[step: 19121] loss: 0.005778355058282614\n",
      "[step: 19122] loss: 0.0050804573111236095\n",
      "[step: 19123] loss: 0.0052745407447218895\n",
      "[step: 19124] loss: 0.005389068275690079\n",
      "[step: 19125] loss: 0.005473378114402294\n",
      "[step: 19126] loss: 0.006304817274212837\n",
      "[step: 19127] loss: 0.0054452684707939625\n",
      "[step: 19128] loss: 0.005304443184286356\n",
      "[step: 19129] loss: 0.0050709545612335205\n",
      "[step: 19130] loss: 0.005096142180263996\n",
      "[step: 19131] loss: 0.005105515476316214\n",
      "[step: 19132] loss: 0.004814543295651674\n",
      "[step: 19133] loss: 0.005117236170917749\n",
      "[step: 19134] loss: 0.004986714106053114\n",
      "[step: 19135] loss: 0.004866170696914196\n",
      "[step: 19136] loss: 0.005165823735296726\n",
      "[step: 19137] loss: 0.004543232265859842\n",
      "[step: 19138] loss: 0.004814709071069956\n",
      "[step: 19139] loss: 0.004559315741062164\n",
      "[step: 19140] loss: 0.00491699855774641\n",
      "[step: 19141] loss: 0.0048960247077047825\n",
      "[step: 19142] loss: 0.004761975724250078\n",
      "[step: 19143] loss: 0.0048236120492219925\n",
      "[step: 19144] loss: 0.004617792554199696\n",
      "[step: 19145] loss: 0.004816641565412283\n",
      "[step: 19146] loss: 0.004648069851100445\n",
      "[step: 19147] loss: 0.004623475484549999\n",
      "[step: 19148] loss: 0.0044725071638822556\n",
      "[step: 19149] loss: 0.00457811588421464\n",
      "[step: 19150] loss: 0.0044211093336343765\n",
      "[step: 19151] loss: 0.004685860592871904\n",
      "[step: 19152] loss: 0.004655481781810522\n",
      "[step: 19153] loss: 0.004580244421958923\n",
      "[step: 19154] loss: 0.0047625647857785225\n",
      "[step: 19155] loss: 0.004385000094771385\n",
      "[step: 19156] loss: 0.004610319621860981\n",
      "[step: 19157] loss: 0.004408464301377535\n",
      "[step: 19158] loss: 0.00447784224525094\n",
      "[step: 19159] loss: 0.004532079678028822\n",
      "[step: 19160] loss: 0.0043633850291371346\n",
      "[step: 19161] loss: 0.004453395493328571\n",
      "[step: 19162] loss: 0.004351763986051083\n",
      "[step: 19163] loss: 0.004348144866526127\n",
      "[step: 19164] loss: 0.0043181562796235085\n",
      "[step: 19165] loss: 0.004308362491428852\n",
      "[step: 19166] loss: 0.004294680897146463\n",
      "[step: 19167] loss: 0.004318582359701395\n",
      "[step: 19168] loss: 0.004408028442412615\n",
      "[step: 19169] loss: 0.0042361728847026825\n",
      "[step: 19170] loss: 0.004287262447178364\n",
      "[step: 19171] loss: 0.004410746973007917\n",
      "[step: 19172] loss: 0.004239984322339296\n",
      "[step: 19173] loss: 0.004500371869653463\n",
      "[step: 19174] loss: 0.004893892910331488\n",
      "[step: 19175] loss: 0.0043970937840640545\n",
      "[step: 19176] loss: 0.005642095115035772\n",
      "[step: 19177] loss: 0.007904810830950737\n",
      "[step: 19178] loss: 0.006111963652074337\n",
      "[step: 19179] loss: 0.007473166100680828\n",
      "[step: 19180] loss: 0.010414846241474152\n",
      "[step: 19181] loss: 0.008937708102166653\n",
      "[step: 19182] loss: 0.008696626871824265\n",
      "[step: 19183] loss: 0.008524969220161438\n",
      "[step: 19184] loss: 0.008109183050692081\n",
      "[step: 19185] loss: 0.006537497043609619\n",
      "[step: 19186] loss: 0.008771423250436783\n",
      "[step: 19187] loss: 0.005098125897347927\n",
      "[step: 19188] loss: 0.009646127000451088\n",
      "[step: 19189] loss: 0.006503954529762268\n",
      "[step: 19190] loss: 0.008393578231334686\n",
      "[step: 19191] loss: 0.006490803323686123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 19192] loss: 0.007031739689409733\n",
      "[step: 19193] loss: 0.006904056295752525\n",
      "[step: 19194] loss: 0.006333671510219574\n",
      "[step: 19195] loss: 0.006895849481225014\n",
      "[step: 19196] loss: 0.006136201322078705\n",
      "[step: 19197] loss: 0.007387727033346891\n",
      "[step: 19198] loss: 0.005340289324522018\n",
      "[step: 19199] loss: 0.006368529517203569\n",
      "[step: 19200] loss: 0.0054861437529325485\n",
      "[step: 19201] loss: 0.006272610742598772\n",
      "[step: 19202] loss: 0.005092266481369734\n",
      "[step: 19203] loss: 0.0066528781317174435\n",
      "[step: 19204] loss: 0.006564077455550432\n",
      "[step: 19205] loss: 0.006576781161129475\n",
      "[step: 19206] loss: 0.005964786745607853\n",
      "[step: 19207] loss: 0.006051173433661461\n",
      "[step: 19208] loss: 0.005718276370316744\n",
      "[step: 19209] loss: 0.005733461584895849\n",
      "[step: 19210] loss: 0.005921774078160524\n",
      "[step: 19211] loss: 0.005345409270375967\n",
      "[step: 19212] loss: 0.005700106266885996\n",
      "[step: 19213] loss: 0.005436197854578495\n",
      "[step: 19214] loss: 0.005665973294526339\n",
      "[step: 19215] loss: 0.005292500369250774\n",
      "[step: 19216] loss: 0.005309599451720715\n",
      "[step: 19217] loss: 0.004935110919177532\n",
      "[step: 19218] loss: 0.005418617278337479\n",
      "[step: 19219] loss: 0.005055921617895365\n",
      "[step: 19220] loss: 0.0049863047897815704\n",
      "[step: 19221] loss: 0.00512911519035697\n",
      "[step: 19222] loss: 0.005194748751819134\n",
      "[step: 19223] loss: 0.004781321156769991\n",
      "[step: 19224] loss: 0.004913106560707092\n",
      "[step: 19225] loss: 0.004870026838034391\n",
      "[step: 19226] loss: 0.004856768064200878\n",
      "[step: 19227] loss: 0.004677236545830965\n",
      "[step: 19228] loss: 0.004787767305970192\n",
      "[step: 19229] loss: 0.004601615946739912\n",
      "[step: 19230] loss: 0.004800168797373772\n",
      "[step: 19231] loss: 0.004545631352812052\n",
      "[step: 19232] loss: 0.00465248292312026\n",
      "[step: 19233] loss: 0.004612284246832132\n",
      "[step: 19234] loss: 0.004538335837423801\n",
      "[step: 19235] loss: 0.004508915822952986\n",
      "[step: 19236] loss: 0.004643118940293789\n",
      "[step: 19237] loss: 0.004663648083806038\n",
      "[step: 19238] loss: 0.00451700109988451\n",
      "[step: 19239] loss: 0.004763914737850428\n",
      "[step: 19240] loss: 0.0046218037605285645\n",
      "[step: 19241] loss: 0.004666022956371307\n",
      "[step: 19242] loss: 0.004489070735871792\n",
      "[step: 19243] loss: 0.004550766199827194\n",
      "[step: 19244] loss: 0.0046221003867685795\n",
      "[step: 19245] loss: 0.004590002819895744\n",
      "[step: 19246] loss: 0.004446460399776697\n",
      "[step: 19247] loss: 0.004435951821506023\n",
      "[step: 19248] loss: 0.004503360949456692\n",
      "[step: 19249] loss: 0.004407448694109917\n",
      "[step: 19250] loss: 0.004556260537356138\n",
      "[step: 19251] loss: 0.004442083183676004\n",
      "[step: 19252] loss: 0.0044478788040578365\n",
      "[step: 19253] loss: 0.004426714964210987\n",
      "[step: 19254] loss: 0.004318363964557648\n",
      "[step: 19255] loss: 0.004370291251689196\n",
      "[step: 19256] loss: 0.004307256080210209\n",
      "[step: 19257] loss: 0.004357075318694115\n",
      "[step: 19258] loss: 0.004322499968111515\n",
      "[step: 19259] loss: 0.004323925357311964\n",
      "[step: 19260] loss: 0.00432000495493412\n",
      "[step: 19261] loss: 0.004281467758119106\n",
      "[step: 19262] loss: 0.004286245908588171\n",
      "[step: 19263] loss: 0.004293122794479132\n",
      "[step: 19264] loss: 0.004327127709984779\n",
      "[step: 19265] loss: 0.004264417104423046\n",
      "[step: 19266] loss: 0.004283503163605928\n",
      "[step: 19267] loss: 0.004409621469676495\n",
      "[step: 19268] loss: 0.004234593827277422\n",
      "[step: 19269] loss: 0.004398445598781109\n",
      "[step: 19270] loss: 0.004797669593244791\n",
      "[step: 19271] loss: 0.004364329855889082\n",
      "[step: 19272] loss: 0.005238831974565983\n",
      "[step: 19273] loss: 0.006876521278172731\n",
      "[step: 19274] loss: 0.00553515087813139\n",
      "[step: 19275] loss: 0.005784153006970882\n",
      "[step: 19276] loss: 0.006995033472776413\n",
      "[step: 19277] loss: 0.0052618542686104774\n",
      "[step: 19278] loss: 0.007256599608808756\n",
      "[step: 19279] loss: 0.005647649988532066\n",
      "[step: 19280] loss: 0.006233823485672474\n",
      "[step: 19281] loss: 0.005618967115879059\n",
      "[step: 19282] loss: 0.005523175001144409\n",
      "[step: 19283] loss: 0.005941080395132303\n",
      "[step: 19284] loss: 0.005108823534101248\n",
      "[step: 19285] loss: 0.006487817037850618\n",
      "[step: 19286] loss: 0.005345435347408056\n",
      "[step: 19287] loss: 0.006273675709962845\n",
      "[step: 19288] loss: 0.005519463215023279\n",
      "[step: 19289] loss: 0.00542728416621685\n",
      "[step: 19290] loss: 0.005777817685157061\n",
      "[step: 19291] loss: 0.005523628555238247\n",
      "[step: 19292] loss: 0.005738868378102779\n",
      "[step: 19293] loss: 0.0056200879625976086\n",
      "[step: 19294] loss: 0.005763475317507982\n",
      "[step: 19295] loss: 0.005226306151598692\n",
      "[step: 19296] loss: 0.005790876690298319\n",
      "[step: 19297] loss: 0.0068746088072657585\n",
      "[step: 19298] loss: 0.005463243927806616\n",
      "[step: 19299] loss: 0.006553065497428179\n",
      "[step: 19300] loss: 0.005478017032146454\n",
      "[step: 19301] loss: 0.005886767990887165\n",
      "[step: 19302] loss: 0.0051977005787193775\n",
      "[step: 19303] loss: 0.005493066273629665\n",
      "[step: 19304] loss: 0.005453305318951607\n",
      "[step: 19305] loss: 0.00528333755210042\n",
      "[step: 19306] loss: 0.005340063013136387\n",
      "[step: 19307] loss: 0.0050329710356891155\n",
      "[step: 19308] loss: 0.0048599326983094215\n",
      "[step: 19309] loss: 0.005075801629573107\n",
      "[step: 19310] loss: 0.00487534049898386\n",
      "[step: 19311] loss: 0.004881436470896006\n",
      "[step: 19312] loss: 0.004857186693698168\n",
      "[step: 19313] loss: 0.004655924160033464\n",
      "[step: 19314] loss: 0.004978124052286148\n",
      "[step: 19315] loss: 0.004630426876246929\n",
      "[step: 19316] loss: 0.0049568405374884605\n",
      "[step: 19317] loss: 0.004541020840406418\n",
      "[step: 19318] loss: 0.00476714177057147\n",
      "[step: 19319] loss: 0.004667169414460659\n",
      "[step: 19320] loss: 0.0045824781991541386\n",
      "[step: 19321] loss: 0.004654133226722479\n",
      "[step: 19322] loss: 0.00450318306684494\n",
      "[step: 19323] loss: 0.004573632497340441\n",
      "[step: 19324] loss: 0.00444633886218071\n",
      "[step: 19325] loss: 0.004515457898378372\n",
      "[step: 19326] loss: 0.004374989308416843\n",
      "[step: 19327] loss: 0.00451359897851944\n",
      "[step: 19328] loss: 0.004452643450349569\n",
      "[step: 19329] loss: 0.004364667925983667\n",
      "[step: 19330] loss: 0.004508147016167641\n",
      "[step: 19331] loss: 0.004384563770145178\n",
      "[step: 19332] loss: 0.0043088337406516075\n",
      "[step: 19333] loss: 0.00444093719124794\n",
      "[step: 19334] loss: 0.004323182161897421\n",
      "[step: 19335] loss: 0.004363455343991518\n",
      "[step: 19336] loss: 0.004322122782468796\n",
      "[step: 19337] loss: 0.004256779793649912\n",
      "[step: 19338] loss: 0.004327396862208843\n",
      "[step: 19339] loss: 0.004216570407152176\n",
      "[step: 19340] loss: 0.004262659698724747\n",
      "[step: 19341] loss: 0.004264933057129383\n",
      "[step: 19342] loss: 0.0042052543722093105\n",
      "[step: 19343] loss: 0.004263334441930056\n",
      "[step: 19344] loss: 0.004226094577461481\n",
      "[step: 19345] loss: 0.004194585606455803\n",
      "[step: 19346] loss: 0.004267657175660133\n",
      "[step: 19347] loss: 0.0042142788879573345\n",
      "[step: 19348] loss: 0.004187860991805792\n",
      "[step: 19349] loss: 0.004194263834506273\n",
      "[step: 19350] loss: 0.004157670307904482\n",
      "[step: 19351] loss: 0.004165201913565397\n",
      "[step: 19352] loss: 0.0042035230435431\n",
      "[step: 19353] loss: 0.004288980271667242\n",
      "[step: 19354] loss: 0.00430903397500515\n",
      "[step: 19355] loss: 0.004578518681228161\n",
      "[step: 19356] loss: 0.004168873652815819\n",
      "[step: 19357] loss: 0.004224931821227074\n",
      "[step: 19358] loss: 0.004660600330680609\n",
      "[step: 19359] loss: 0.0041454327292740345\n",
      "[step: 19360] loss: 0.00451238127425313\n",
      "[step: 19361] loss: 0.005674528889358044\n",
      "[step: 19362] loss: 0.0048720152117311954\n",
      "[step: 19363] loss: 0.00764939421787858\n",
      "[step: 19364] loss: 0.008683900348842144\n",
      "[step: 19365] loss: 0.009429460391402245\n",
      "[step: 19366] loss: 0.0070306542329490185\n",
      "[step: 19367] loss: 0.008029649965465069\n",
      "[step: 19368] loss: 0.007538683246821165\n",
      "[step: 19369] loss: 0.006652847398072481\n",
      "[step: 19370] loss: 0.009085027500987053\n",
      "[step: 19371] loss: 0.006936816032975912\n",
      "[step: 19372] loss: 0.007757233921438456\n",
      "[step: 19373] loss: 0.005839201621711254\n",
      "[step: 19374] loss: 0.006296661216765642\n",
      "[step: 19375] loss: 0.00636379374191165\n",
      "[step: 19376] loss: 0.005588047206401825\n",
      "[step: 19377] loss: 0.006158128380775452\n",
      "[step: 19378] loss: 0.006164682097733021\n",
      "[step: 19379] loss: 0.005306055303663015\n",
      "[step: 19380] loss: 0.005383054725825787\n",
      "[step: 19381] loss: 0.005542129743844271\n",
      "[step: 19382] loss: 0.004909161012619734\n",
      "[step: 19383] loss: 0.00518600782379508\n",
      "[step: 19384] loss: 0.005002225283533335\n",
      "[step: 19385] loss: 0.004685109481215477\n",
      "[step: 19386] loss: 0.0048384699039161205\n",
      "[step: 19387] loss: 0.004862040746957064\n",
      "[step: 19388] loss: 0.004664631560444832\n",
      "[step: 19389] loss: 0.004796640947461128\n",
      "[step: 19390] loss: 0.004722557961940765\n",
      "[step: 19391] loss: 0.004507026635110378\n",
      "[step: 19392] loss: 0.004553989507257938\n",
      "[step: 19393] loss: 0.004686209373176098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 19394] loss: 0.004572305362671614\n",
      "[step: 19395] loss: 0.004449483007192612\n",
      "[step: 19396] loss: 0.004557400476187468\n",
      "[step: 19397] loss: 0.004480042960494757\n",
      "[step: 19398] loss: 0.004444306716322899\n",
      "[step: 19399] loss: 0.004413960967212915\n",
      "[step: 19400] loss: 0.0043309829197824\n",
      "[step: 19401] loss: 0.004375037737190723\n",
      "[step: 19402] loss: 0.004376270342618227\n",
      "[step: 19403] loss: 0.004315110854804516\n",
      "[step: 19404] loss: 0.004319130443036556\n",
      "[step: 19405] loss: 0.004305913578718901\n",
      "[step: 19406] loss: 0.004291093442589045\n",
      "[step: 19407] loss: 0.004253638442605734\n",
      "[step: 19408] loss: 0.004234806634485722\n",
      "[step: 19409] loss: 0.004236168228089809\n",
      "[step: 19410] loss: 0.004242983181029558\n",
      "[step: 19411] loss: 0.004236683249473572\n",
      "[step: 19412] loss: 0.004226223565638065\n",
      "[step: 19413] loss: 0.004222199320793152\n",
      "[step: 19414] loss: 0.0042082020081579685\n",
      "[step: 19415] loss: 0.00422617606818676\n",
      "[step: 19416] loss: 0.004197507631033659\n",
      "[step: 19417] loss: 0.004200404044240713\n",
      "[step: 19418] loss: 0.004170896019786596\n",
      "[step: 19419] loss: 0.004156892187893391\n",
      "[step: 19420] loss: 0.004167338367551565\n",
      "[step: 19421] loss: 0.004182356875389814\n",
      "[step: 19422] loss: 0.004246323835104704\n",
      "[step: 19423] loss: 0.004234896972775459\n",
      "[step: 19424] loss: 0.004302815068513155\n",
      "[step: 19425] loss: 0.004195616114884615\n",
      "[step: 19426] loss: 0.004213906358927488\n",
      "[step: 19427] loss: 0.004190446343272924\n",
      "[step: 19428] loss: 0.0042854477651417255\n",
      "[step: 19429] loss: 0.004155290778726339\n",
      "[step: 19430] loss: 0.004101459868252277\n",
      "[step: 19431] loss: 0.004113184753805399\n",
      "[step: 19432] loss: 0.004174019210040569\n",
      "[step: 19433] loss: 0.004405574407428503\n",
      "[step: 19434] loss: 0.0042228964157402515\n",
      "[step: 19435] loss: 0.004179570823907852\n",
      "[step: 19436] loss: 0.004107510205358267\n",
      "[step: 19437] loss: 0.004082496277987957\n",
      "[step: 19438] loss: 0.004090397153049707\n",
      "[step: 19439] loss: 0.0040874476544559\n",
      "[step: 19440] loss: 0.004064019303768873\n",
      "[step: 19441] loss: 0.00406670942902565\n",
      "[step: 19442] loss: 0.004102526232600212\n",
      "[step: 19443] loss: 0.004186226520687342\n",
      "[step: 19444] loss: 0.004740705713629723\n",
      "[step: 19445] loss: 0.004189357161521912\n",
      "[step: 19446] loss: 0.004105592146515846\n",
      "[step: 19447] loss: 0.004230086226016283\n",
      "[step: 19448] loss: 0.004249443765729666\n",
      "[step: 19449] loss: 0.0046730623580515385\n",
      "[step: 19450] loss: 0.004215261898934841\n",
      "[step: 19451] loss: 0.004467894323170185\n",
      "[step: 19452] loss: 0.005658450536429882\n",
      "[step: 19453] loss: 0.00479082902893424\n",
      "[step: 19454] loss: 0.00847418513149023\n",
      "[step: 19455] loss: 0.009567542932927608\n",
      "[step: 19456] loss: 0.009875180199742317\n",
      "[step: 19457] loss: 0.00722721079364419\n",
      "[step: 19458] loss: 0.0097312917932868\n",
      "[step: 19459] loss: 0.007895157672464848\n",
      "[step: 19460] loss: 0.00671860296279192\n",
      "[step: 19461] loss: 0.007807300891727209\n",
      "[step: 19462] loss: 0.006369698327034712\n",
      "[step: 19463] loss: 0.008359763771295547\n",
      "[step: 19464] loss: 0.007961410097777843\n",
      "[step: 19465] loss: 0.010008218698203564\n",
      "[step: 19466] loss: 0.008146870881319046\n",
      "[step: 19467] loss: 0.007297420408576727\n",
      "[step: 19468] loss: 0.009364258497953415\n",
      "[step: 19469] loss: 0.007596958894282579\n",
      "[step: 19470] loss: 0.006685324478894472\n",
      "[step: 19471] loss: 0.007590210065245628\n",
      "[step: 19472] loss: 0.0060659050941467285\n",
      "[step: 19473] loss: 0.006045076064765453\n",
      "[step: 19474] loss: 0.007248180918395519\n",
      "[step: 19475] loss: 0.006372896954417229\n",
      "[step: 19476] loss: 0.005913261324167252\n",
      "[step: 19477] loss: 0.005674736108630896\n",
      "[step: 19478] loss: 0.00557407271116972\n",
      "[step: 19479] loss: 0.005669542122632265\n",
      "[step: 19480] loss: 0.005631183739751577\n",
      "[step: 19481] loss: 0.005266690626740456\n",
      "[step: 19482] loss: 0.005559575278311968\n",
      "[step: 19483] loss: 0.005108737852424383\n",
      "[step: 19484] loss: 0.005519389640539885\n",
      "[step: 19485] loss: 0.004848713055253029\n",
      "[step: 19486] loss: 0.004938669968396425\n",
      "[step: 19487] loss: 0.004913956858217716\n",
      "[step: 19488] loss: 0.005115806125104427\n",
      "[step: 19489] loss: 0.004903408698737621\n",
      "[step: 19490] loss: 0.004897179547697306\n",
      "[step: 19491] loss: 0.004628988914191723\n",
      "[step: 19492] loss: 0.004777552559971809\n",
      "[step: 19493] loss: 0.004739758558571339\n",
      "[step: 19494] loss: 0.00472463620826602\n",
      "[step: 19495] loss: 0.004582258872687817\n",
      "[step: 19496] loss: 0.004535675514489412\n",
      "[step: 19497] loss: 0.004531813785433769\n",
      "[step: 19498] loss: 0.004516302142292261\n",
      "[step: 19499] loss: 0.004574547987431288\n",
      "[step: 19500] loss: 0.004455156158655882\n",
      "[step: 19501] loss: 0.004466797225177288\n",
      "[step: 19502] loss: 0.004389652982354164\n",
      "[step: 19503] loss: 0.004460694268345833\n",
      "[step: 19504] loss: 0.004412083886563778\n",
      "[step: 19505] loss: 0.004378390032798052\n",
      "[step: 19506] loss: 0.004393070004880428\n",
      "[step: 19507] loss: 0.004337833728641272\n",
      "[step: 19508] loss: 0.00434641120955348\n",
      "[step: 19509] loss: 0.004367670509964228\n",
      "[step: 19510] loss: 0.004304868634790182\n",
      "[step: 19511] loss: 0.0042840102687478065\n",
      "[step: 19512] loss: 0.004307946655899286\n",
      "[step: 19513] loss: 0.004261773079633713\n",
      "[step: 19514] loss: 0.004271373152732849\n",
      "[step: 19515] loss: 0.004249916411936283\n",
      "[step: 19516] loss: 0.004222806543111801\n",
      "[step: 19517] loss: 0.004231462720781565\n",
      "[step: 19518] loss: 0.00420986907556653\n",
      "[step: 19519] loss: 0.004195245914161205\n",
      "[step: 19520] loss: 0.004195589106529951\n",
      "[step: 19521] loss: 0.004188349936157465\n",
      "[step: 19522] loss: 0.004173790104687214\n",
      "[step: 19523] loss: 0.004162650555372238\n",
      "[step: 19524] loss: 0.004152740817517042\n",
      "[step: 19525] loss: 0.004148117266595364\n",
      "[step: 19526] loss: 0.00414961576461792\n",
      "[step: 19527] loss: 0.004153511952608824\n",
      "[step: 19528] loss: 0.004140571691095829\n",
      "[step: 19529] loss: 0.004138593561947346\n",
      "[step: 19530] loss: 0.004127884283661842\n",
      "[step: 19531] loss: 0.004123705439269543\n",
      "[step: 19532] loss: 0.0041076322086155415\n",
      "[step: 19533] loss: 0.004102260805666447\n",
      "[step: 19534] loss: 0.004105381201952696\n",
      "[step: 19535] loss: 0.004146045073866844\n",
      "[step: 19536] loss: 0.004203420598059893\n",
      "[step: 19537] loss: 0.004571237601339817\n",
      "[step: 19538] loss: 0.0041646407917141914\n",
      "[step: 19539] loss: 0.004088814835995436\n",
      "[step: 19540] loss: 0.00420778151601553\n",
      "[step: 19541] loss: 0.00424202298745513\n",
      "[step: 19542] loss: 0.004508323036134243\n",
      "[step: 19543] loss: 0.004115080926567316\n",
      "[step: 19544] loss: 0.004455112386494875\n",
      "[step: 19545] loss: 0.005571850575506687\n",
      "[step: 19546] loss: 0.004897670354694128\n",
      "[step: 19547] loss: 0.005424673203378916\n",
      "[step: 19548] loss: 0.005293503403663635\n",
      "[step: 19549] loss: 0.005312933120876551\n",
      "[step: 19550] loss: 0.004779208451509476\n",
      "[step: 19551] loss: 0.004925848450511694\n",
      "[step: 19552] loss: 0.0050526149570941925\n",
      "[step: 19553] loss: 0.0049929264932870865\n",
      "[step: 19554] loss: 0.004620024468749762\n",
      "[step: 19555] loss: 0.004790939390659332\n",
      "[step: 19556] loss: 0.004724306985735893\n",
      "[step: 19557] loss: 0.004892708733677864\n",
      "[step: 19558] loss: 0.004366646520793438\n",
      "[step: 19559] loss: 0.005309129133820534\n",
      "[step: 19560] loss: 0.005991055630147457\n",
      "[step: 19561] loss: 0.005406494718044996\n",
      "[step: 19562] loss: 0.004923652857542038\n",
      "[step: 19563] loss: 0.004706897307187319\n",
      "[step: 19564] loss: 0.00497496547177434\n",
      "[step: 19565] loss: 0.00520068034529686\n",
      "[step: 19566] loss: 0.004647460300475359\n",
      "[step: 19567] loss: 0.0053365277126431465\n",
      "[step: 19568] loss: 0.0062394277192652225\n",
      "[step: 19569] loss: 0.005822074133902788\n",
      "[step: 19570] loss: 0.00453532487154007\n",
      "[step: 19571] loss: 0.006104019470512867\n",
      "[step: 19572] loss: 0.00513482466340065\n",
      "[step: 19573] loss: 0.00619420874863863\n",
      "[step: 19574] loss: 0.004862583242356777\n",
      "[step: 19575] loss: 0.00562559999525547\n",
      "[step: 19576] loss: 0.004791164770722389\n",
      "[step: 19577] loss: 0.004993157461285591\n",
      "[step: 19578] loss: 0.004802168812602758\n",
      "[step: 19579] loss: 0.004825487267225981\n",
      "[step: 19580] loss: 0.004610278178006411\n",
      "[step: 19581] loss: 0.004783128388226032\n",
      "[step: 19582] loss: 0.004553504753857851\n",
      "[step: 19583] loss: 0.004494041670113802\n",
      "[step: 19584] loss: 0.0045918058604002\n",
      "[step: 19585] loss: 0.004466441925615072\n",
      "[step: 19586] loss: 0.004686584230512381\n",
      "[step: 19587] loss: 0.0043307882733643055\n",
      "[step: 19588] loss: 0.004672394134104252\n",
      "[step: 19589] loss: 0.004450357519090176\n",
      "[step: 19590] loss: 0.004546866752207279\n",
      "[step: 19591] loss: 0.00448235496878624\n",
      "[step: 19592] loss: 0.004292983096092939\n",
      "[step: 19593] loss: 0.004500622395426035\n",
      "[step: 19594] loss: 0.004361471626907587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 19595] loss: 0.004480708856135607\n",
      "[step: 19596] loss: 0.004227214027196169\n",
      "[step: 19597] loss: 0.00433032913133502\n",
      "[step: 19598] loss: 0.00425372552126646\n",
      "[step: 19599] loss: 0.004285212140530348\n",
      "[step: 19600] loss: 0.004181739408522844\n",
      "[step: 19601] loss: 0.004229698795825243\n",
      "[step: 19602] loss: 0.004186295438557863\n",
      "[step: 19603] loss: 0.004176781512796879\n",
      "[step: 19604] loss: 0.004134576767683029\n",
      "[step: 19605] loss: 0.004110926296561956\n",
      "[step: 19606] loss: 0.0041514309123158455\n",
      "[step: 19607] loss: 0.004090408328920603\n",
      "[step: 19608] loss: 0.004099932033568621\n",
      "[step: 19609] loss: 0.004066553432494402\n",
      "[step: 19610] loss: 0.004114186391234398\n",
      "[step: 19611] loss: 0.004090607166290283\n",
      "[step: 19612] loss: 0.004084402695298195\n",
      "[step: 19613] loss: 0.00406337296590209\n",
      "[step: 19614] loss: 0.004048603121191263\n",
      "[step: 19615] loss: 0.0040191602893173695\n",
      "[step: 19616] loss: 0.004047903697937727\n",
      "[step: 19617] loss: 0.004100519232451916\n",
      "[step: 19618] loss: 0.0042279185727238655\n",
      "[step: 19619] loss: 0.004229438491165638\n",
      "[step: 19620] loss: 0.004691266920417547\n",
      "[step: 19621] loss: 0.004069148097187281\n",
      "[step: 19622] loss: 0.005035544745624065\n",
      "[step: 19623] loss: 0.007701973896473646\n",
      "[step: 19624] loss: 0.0065973312593996525\n",
      "[step: 19625] loss: 0.005561334080994129\n",
      "[step: 19626] loss: 0.005791160278022289\n",
      "[step: 19627] loss: 0.005290666129440069\n",
      "[step: 19628] loss: 0.005804097279906273\n",
      "[step: 19629] loss: 0.004857288673520088\n",
      "[step: 19630] loss: 0.005595040507614613\n",
      "[step: 19631] loss: 0.004659272264689207\n",
      "[step: 19632] loss: 0.004938328638672829\n",
      "[step: 19633] loss: 0.004668709821999073\n",
      "[step: 19634] loss: 0.004746918100863695\n",
      "[step: 19635] loss: 0.004845717456191778\n",
      "[step: 19636] loss: 0.00444420799612999\n",
      "[step: 19637] loss: 0.004990952089428902\n",
      "[step: 19638] loss: 0.004763482604175806\n",
      "[step: 19639] loss: 0.004527955781668425\n",
      "[step: 19640] loss: 0.004589031916111708\n",
      "[step: 19641] loss: 0.004474668297916651\n",
      "[step: 19642] loss: 0.004417102783918381\n",
      "[step: 19643] loss: 0.00439872220158577\n",
      "[step: 19644] loss: 0.004352457821369171\n",
      "[step: 19645] loss: 0.004283262882381678\n",
      "[step: 19646] loss: 0.004218833055347204\n",
      "[step: 19647] loss: 0.0042686546221375465\n",
      "[step: 19648] loss: 0.004213373642414808\n",
      "[step: 19649] loss: 0.0042143287137150764\n",
      "[step: 19650] loss: 0.004234207794070244\n",
      "[step: 19651] loss: 0.004344291519373655\n",
      "[step: 19652] loss: 0.004209775477647781\n",
      "[step: 19653] loss: 0.00429659616202116\n",
      "[step: 19654] loss: 0.004543145652860403\n",
      "[step: 19655] loss: 0.0042479136027395725\n",
      "[step: 19656] loss: 0.005255953408777714\n",
      "[step: 19657] loss: 0.009759992361068726\n",
      "[step: 19658] loss: 0.008024963550269604\n",
      "[step: 19659] loss: 0.007309102453291416\n",
      "[step: 19660] loss: 0.007784370798617601\n",
      "[step: 19661] loss: 0.005505669396370649\n",
      "[step: 19662] loss: 0.006691485643386841\n",
      "[step: 19663] loss: 0.00671906303614378\n",
      "[step: 19664] loss: 0.006060056854039431\n",
      "[step: 19665] loss: 0.006753697991371155\n",
      "[step: 19666] loss: 0.005444705486297607\n",
      "[step: 19667] loss: 0.006034500431269407\n",
      "[step: 19668] loss: 0.005074703600257635\n",
      "[step: 19669] loss: 0.00606734212487936\n",
      "[step: 19670] loss: 0.005369013175368309\n",
      "[step: 19671] loss: 0.005369927734136581\n",
      "[step: 19672] loss: 0.004771718755364418\n",
      "[step: 19673] loss: 0.004774733446538448\n",
      "[step: 19674] loss: 0.0050791543908417225\n",
      "[step: 19675] loss: 0.0049997735768556595\n",
      "[step: 19676] loss: 0.004698781296610832\n",
      "[step: 19677] loss: 0.00470315758138895\n",
      "[step: 19678] loss: 0.004633686505258083\n",
      "[step: 19679] loss: 0.004653412848711014\n",
      "[step: 19680] loss: 0.004506390076130629\n",
      "[step: 19681] loss: 0.004353420343250036\n",
      "[step: 19682] loss: 0.0045772395096719265\n",
      "[step: 19683] loss: 0.004451698623597622\n",
      "[step: 19684] loss: 0.0043677352368831635\n",
      "[step: 19685] loss: 0.004310101270675659\n",
      "[step: 19686] loss: 0.004362524952739477\n",
      "[step: 19687] loss: 0.004307952709496021\n",
      "[step: 19688] loss: 0.004190186504274607\n",
      "[step: 19689] loss: 0.004260790999978781\n",
      "[step: 19690] loss: 0.004276401828974485\n",
      "[step: 19691] loss: 0.004196986556053162\n",
      "[step: 19692] loss: 0.004185027442872524\n",
      "[step: 19693] loss: 0.00415672454982996\n",
      "[step: 19694] loss: 0.004184005782008171\n",
      "[step: 19695] loss: 0.004138752352446318\n",
      "[step: 19696] loss: 0.004085896071046591\n",
      "[step: 19697] loss: 0.004123933147639036\n",
      "[step: 19698] loss: 0.004102987237274647\n",
      "[step: 19699] loss: 0.004105046857148409\n",
      "[step: 19700] loss: 0.004168529994785786\n",
      "[step: 19701] loss: 0.004260614514350891\n",
      "[step: 19702] loss: 0.005304692778736353\n",
      "[step: 19703] loss: 0.00460863346233964\n",
      "[step: 19704] loss: 0.005703382659703493\n",
      "[step: 19705] loss: 0.006946443580091\n",
      "[step: 19706] loss: 0.006573617458343506\n",
      "[step: 19707] loss: 0.006498635280877352\n",
      "[step: 19708] loss: 0.007895587012171745\n",
      "[step: 19709] loss: 0.0067867315374314785\n",
      "[step: 19710] loss: 0.007074461318552494\n",
      "[step: 19711] loss: 0.007844449020922184\n",
      "[step: 19712] loss: 0.0060749477706849575\n",
      "[step: 19713] loss: 0.007615470327436924\n",
      "[step: 19714] loss: 0.005241652950644493\n",
      "[step: 19715] loss: 0.007326559163630009\n",
      "[step: 19716] loss: 0.005114284344017506\n",
      "[step: 19717] loss: 0.006614497862756252\n",
      "[step: 19718] loss: 0.005378637928515673\n",
      "[step: 19719] loss: 0.005922215525060892\n",
      "[step: 19720] loss: 0.00565751176327467\n",
      "[step: 19721] loss: 0.005094051361083984\n",
      "[step: 19722] loss: 0.005462433211505413\n",
      "[step: 19723] loss: 0.004951927810907364\n",
      "[step: 19724] loss: 0.005248012486845255\n",
      "[step: 19725] loss: 0.004721096716821194\n",
      "[step: 19726] loss: 0.004916565027087927\n",
      "[step: 19727] loss: 0.0047673191875219345\n",
      "[step: 19728] loss: 0.004824905656278133\n",
      "[step: 19729] loss: 0.0045335013419389725\n",
      "[step: 19730] loss: 0.0047034891322255135\n",
      "[step: 19731] loss: 0.004548524506390095\n",
      "[step: 19732] loss: 0.00451152678579092\n",
      "[step: 19733] loss: 0.004433863330632448\n",
      "[step: 19734] loss: 0.004338704980909824\n",
      "[step: 19735] loss: 0.004351803567260504\n",
      "[step: 19736] loss: 0.0043260264210402966\n",
      "[step: 19737] loss: 0.004283092450350523\n",
      "[step: 19738] loss: 0.0042339228093624115\n",
      "[step: 19739] loss: 0.004262035246938467\n",
      "[step: 19740] loss: 0.004247080534696579\n",
      "[step: 19741] loss: 0.004126735497266054\n",
      "[step: 19742] loss: 0.004159285221248865\n",
      "[step: 19743] loss: 0.0042443787679076195\n",
      "[step: 19744] loss: 0.004135445225983858\n",
      "[step: 19745] loss: 0.004171372391283512\n",
      "[step: 19746] loss: 0.004259745590388775\n",
      "[step: 19747] loss: 0.004092984367161989\n",
      "[step: 19748] loss: 0.0041849506087601185\n",
      "[step: 19749] loss: 0.004406666848808527\n",
      "[step: 19750] loss: 0.004078799858689308\n",
      "[step: 19751] loss: 0.004669806454330683\n",
      "[step: 19752] loss: 0.00536930188536644\n",
      "[step: 19753] loss: 0.005493736360222101\n",
      "[step: 19754] loss: 0.004777068272233009\n",
      "[step: 19755] loss: 0.00652991933748126\n",
      "[step: 19756] loss: 0.007403933443129063\n",
      "[step: 19757] loss: 0.007425343617796898\n",
      "[step: 19758] loss: 0.005973659921437502\n",
      "[step: 19759] loss: 0.006810960825532675\n",
      "[step: 19760] loss: 0.005249100737273693\n",
      "[step: 19761] loss: 0.007399057038128376\n",
      "[step: 19762] loss: 0.005605969112366438\n",
      "[step: 19763] loss: 0.006811696104705334\n",
      "[step: 19764] loss: 0.005717028398066759\n",
      "[step: 19765] loss: 0.007386448327451944\n",
      "[step: 19766] loss: 0.005577860865741968\n",
      "[step: 19767] loss: 0.006678382400423288\n",
      "[step: 19768] loss: 0.004999613389372826\n",
      "[step: 19769] loss: 0.005865576211363077\n",
      "[step: 19770] loss: 0.005487903021275997\n",
      "[step: 19771] loss: 0.0050972830504179\n",
      "[step: 19772] loss: 0.00522573059424758\n",
      "[step: 19773] loss: 0.004786946810781956\n",
      "[step: 19774] loss: 0.005226116627454758\n",
      "[step: 19775] loss: 0.004830466583371162\n",
      "[step: 19776] loss: 0.004806677810847759\n",
      "[step: 19777] loss: 0.004599961917847395\n",
      "[step: 19778] loss: 0.0047610169276595116\n",
      "[step: 19779] loss: 0.004677587188780308\n",
      "[step: 19780] loss: 0.004664700012654066\n",
      "[step: 19781] loss: 0.004364416003227234\n",
      "[step: 19782] loss: 0.004532914608716965\n",
      "[step: 19783] loss: 0.004460560157895088\n",
      "[step: 19784] loss: 0.004512887913733721\n",
      "[step: 19785] loss: 0.004284475464373827\n",
      "[step: 19786] loss: 0.004330419469624758\n",
      "[step: 19787] loss: 0.004312228411436081\n",
      "[step: 19788] loss: 0.00438536424189806\n",
      "[step: 19789] loss: 0.004315590485930443\n",
      "[step: 19790] loss: 0.004207551013678312\n",
      "[step: 19791] loss: 0.004241902381181717\n",
      "[step: 19792] loss: 0.004213911015540361\n",
      "[step: 19793] loss: 0.004255257081240416\n",
      "[step: 19794] loss: 0.0041461060754954815\n",
      "[step: 19795] loss: 0.004201487638056278\n",
      "[step: 19796] loss: 0.004138922318816185\n",
      "[step: 19797] loss: 0.004147708415985107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 19798] loss: 0.004147704225033522\n",
      "[step: 19799] loss: 0.004090486094355583\n",
      "[step: 19800] loss: 0.0041114953346550465\n",
      "[step: 19801] loss: 0.004054368007928133\n",
      "[step: 19802] loss: 0.004077766556292772\n",
      "[step: 19803] loss: 0.004122932907193899\n",
      "[step: 19804] loss: 0.004180385265499353\n",
      "[step: 19805] loss: 0.0041256980039179325\n",
      "[step: 19806] loss: 0.004089890979230404\n",
      "[step: 19807] loss: 0.004007322713732719\n",
      "[step: 19808] loss: 0.004015031736344099\n",
      "[step: 19809] loss: 0.004033499862998724\n",
      "[step: 19810] loss: 0.004017814993858337\n",
      "[step: 19811] loss: 0.004015258047729731\n",
      "[step: 19812] loss: 0.003970005549490452\n",
      "[step: 19813] loss: 0.0039626676589250565\n",
      "[step: 19814] loss: 0.003972036298364401\n",
      "[step: 19815] loss: 0.003965815994888544\n",
      "[step: 19816] loss: 0.004061624873429537\n",
      "[step: 19817] loss: 0.004286930896341801\n",
      "[step: 19818] loss: 0.005057985894382\n",
      "[step: 19819] loss: 0.004290388897061348\n",
      "[step: 19820] loss: 0.006209966726601124\n",
      "[step: 19821] loss: 0.006662233732640743\n",
      "[step: 19822] loss: 0.006041635759174824\n",
      "[step: 19823] loss: 0.00543171726167202\n",
      "[step: 19824] loss: 0.005837661679834127\n",
      "[step: 19825] loss: 0.006258800160139799\n",
      "[step: 19826] loss: 0.0050788684748113155\n",
      "[step: 19827] loss: 0.005640353541821241\n",
      "[step: 19828] loss: 0.004903379827737808\n",
      "[step: 19829] loss: 0.005060078110545874\n",
      "[step: 19830] loss: 0.004707382991909981\n",
      "[step: 19831] loss: 0.004980101250112057\n",
      "[step: 19832] loss: 0.005087757017463446\n",
      "[step: 19833] loss: 0.00484864367172122\n",
      "[step: 19834] loss: 0.004628065973520279\n",
      "[step: 19835] loss: 0.004623153246939182\n",
      "[step: 19836] loss: 0.005125594325363636\n",
      "[step: 19837] loss: 0.00484425388276577\n",
      "[step: 19838] loss: 0.004565793555229902\n",
      "[step: 19839] loss: 0.004598922561854124\n",
      "[step: 19840] loss: 0.004698675125837326\n",
      "[step: 19841] loss: 0.0048779333010315895\n",
      "[step: 19842] loss: 0.004655463621020317\n",
      "[step: 19843] loss: 0.004740867763757706\n",
      "[step: 19844] loss: 0.004466348327696323\n",
      "[step: 19845] loss: 0.004472716711461544\n",
      "[step: 19846] loss: 0.004426638595759869\n",
      "[step: 19847] loss: 0.004391626920551062\n",
      "[step: 19848] loss: 0.004226878751069307\n",
      "[step: 19849] loss: 0.00420651538297534\n",
      "[step: 19850] loss: 0.004281324800103903\n",
      "[step: 19851] loss: 0.004348657093942165\n",
      "[step: 19852] loss: 0.0041666338220238686\n",
      "[step: 19853] loss: 0.004255091305822134\n",
      "[step: 19854] loss: 0.004196254536509514\n",
      "[step: 19855] loss: 0.004179138224571943\n",
      "[step: 19856] loss: 0.004178664647042751\n",
      "[step: 19857] loss: 0.0040879142470657825\n",
      "[step: 19858] loss: 0.00409987848252058\n",
      "[step: 19859] loss: 0.004132903181016445\n",
      "[step: 19860] loss: 0.004039215389639139\n",
      "[step: 19861] loss: 0.004036217462271452\n",
      "[step: 19862] loss: 0.004071940202265978\n",
      "[step: 19863] loss: 0.004010725766420364\n",
      "[step: 19864] loss: 0.003992252517491579\n",
      "[step: 19865] loss: 0.004020912107080221\n",
      "[step: 19866] loss: 0.004046882037073374\n",
      "[step: 19867] loss: 0.0039893812499940395\n",
      "[step: 19868] loss: 0.003945437725633383\n",
      "[step: 19869] loss: 0.003941404167562723\n",
      "[step: 19870] loss: 0.003952889237552881\n",
      "[step: 19871] loss: 0.003972628619521856\n",
      "[step: 19872] loss: 0.0039616539143025875\n",
      "[step: 19873] loss: 0.003984522074460983\n",
      "[step: 19874] loss: 0.003980899695307016\n",
      "[step: 19875] loss: 0.004003740381449461\n",
      "[step: 19876] loss: 0.0039675310254096985\n",
      "[step: 19877] loss: 0.0039668926037848\n",
      "[step: 19878] loss: 0.003944518975913525\n",
      "[step: 19879] loss: 0.003984867595136166\n",
      "[step: 19880] loss: 0.004023693967610598\n",
      "[step: 19881] loss: 0.004259906709194183\n",
      "[step: 19882] loss: 0.004021842032670975\n",
      "[step: 19883] loss: 0.0039003738202154636\n",
      "[step: 19884] loss: 0.0038883984088897705\n",
      "[step: 19885] loss: 0.0039917961694300175\n",
      "[step: 19886] loss: 0.004341308493167162\n",
      "[step: 19887] loss: 0.00394847709685564\n",
      "[step: 19888] loss: 0.00393229816108942\n",
      "[step: 19889] loss: 0.004325400106608868\n",
      "[step: 19890] loss: 0.0039490400813519955\n",
      "[step: 19891] loss: 0.003959733061492443\n",
      "[step: 19892] loss: 0.004288715310394764\n",
      "[step: 19893] loss: 0.0039319540373981\n",
      "[step: 19894] loss: 0.0040126703679561615\n",
      "[step: 19895] loss: 0.0045481594279408455\n",
      "[step: 19896] loss: 0.004056716337800026\n",
      "[step: 19897] loss: 0.005321475211530924\n",
      "[step: 19898] loss: 0.006476729642599821\n",
      "[step: 19899] loss: 0.005415544379502535\n",
      "[step: 19900] loss: 0.0083734430372715\n",
      "[step: 19901] loss: 0.006552292965352535\n",
      "[step: 19902] loss: 0.00575038930401206\n",
      "[step: 19903] loss: 0.006221018265932798\n",
      "[step: 19904] loss: 0.005047934595495462\n",
      "[step: 19905] loss: 0.005962615367025137\n",
      "[step: 19906] loss: 0.005221440922468901\n",
      "[step: 19907] loss: 0.0058310190215706825\n",
      "[step: 19908] loss: 0.009096373803913593\n",
      "[step: 19909] loss: 0.007189973723143339\n",
      "[step: 19910] loss: 0.007622229401022196\n",
      "[step: 19911] loss: 0.007779576815664768\n",
      "[step: 19912] loss: 0.0063439323566854\n",
      "[step: 19913] loss: 0.006689004134386778\n",
      "[step: 19914] loss: 0.006941697560250759\n",
      "[step: 19915] loss: 0.006768152583390474\n",
      "[step: 19916] loss: 0.006368883885443211\n",
      "[step: 19917] loss: 0.006479655858129263\n",
      "[step: 19918] loss: 0.0061100623570382595\n",
      "[step: 19919] loss: 0.00601154612377286\n",
      "[step: 19920] loss: 0.005316064227372408\n",
      "[step: 19921] loss: 0.006339463405311108\n",
      "[step: 19922] loss: 0.005263985600322485\n",
      "[step: 19923] loss: 0.00492364726960659\n",
      "[step: 19924] loss: 0.005229960661381483\n",
      "[step: 19925] loss: 0.005022452678531408\n",
      "[step: 19926] loss: 0.005097552202641964\n",
      "[step: 19927] loss: 0.005055594723671675\n",
      "[step: 19928] loss: 0.00465708551928401\n",
      "[step: 19929] loss: 0.005081652197986841\n",
      "[step: 19930] loss: 0.004714567679911852\n",
      "[step: 19931] loss: 0.0045675733126699924\n",
      "[step: 19932] loss: 0.004518816713243723\n",
      "[step: 19933] loss: 0.004643029998987913\n",
      "[step: 19934] loss: 0.004513929132372141\n",
      "[step: 19935] loss: 0.004447155632078648\n",
      "[step: 19936] loss: 0.00426297215744853\n",
      "[step: 19937] loss: 0.004476750735193491\n",
      "[step: 19938] loss: 0.004262252245098352\n",
      "[step: 19939] loss: 0.004300757776945829\n",
      "[step: 19940] loss: 0.004261266905814409\n",
      "[step: 19941] loss: 0.004209212027490139\n",
      "[step: 19942] loss: 0.004200878087431192\n",
      "[step: 19943] loss: 0.004156795796006918\n",
      "[step: 19944] loss: 0.004133475478738546\n",
      "[step: 19945] loss: 0.004138122778385878\n",
      "[step: 19946] loss: 0.004121896345168352\n",
      "[step: 19947] loss: 0.004066189751029015\n",
      "[step: 19948] loss: 0.004086599685251713\n",
      "[step: 19949] loss: 0.004097749944776297\n",
      "[step: 19950] loss: 0.004032635595649481\n",
      "[step: 19951] loss: 0.004042971413582563\n",
      "[step: 19952] loss: 0.0039880648255348206\n",
      "[step: 19953] loss: 0.004001234658062458\n",
      "[step: 19954] loss: 0.004050195217132568\n",
      "[step: 19955] loss: 0.00399122666567564\n",
      "[step: 19956] loss: 0.00401264289394021\n",
      "[step: 19957] loss: 0.003951292484998703\n",
      "[step: 19958] loss: 0.003985526040196419\n",
      "[step: 19959] loss: 0.004026247654110193\n",
      "[step: 19960] loss: 0.003966827876865864\n",
      "[step: 19961] loss: 0.0039486996829509735\n",
      "[step: 19962] loss: 0.003908829763531685\n",
      "[step: 19963] loss: 0.0039282250218093395\n",
      "[step: 19964] loss: 0.003976288717240095\n",
      "[step: 19965] loss: 0.00413128174841404\n",
      "[step: 19966] loss: 0.004529237747192383\n",
      "[step: 19967] loss: 0.004105433821678162\n",
      "[step: 19968] loss: 0.005487233400344849\n",
      "[step: 19969] loss: 0.006384570617228746\n",
      "[step: 19970] loss: 0.005904240068048239\n",
      "[step: 19971] loss: 0.005672793835401535\n",
      "[step: 19972] loss: 0.005906601902097464\n",
      "[step: 19973] loss: 0.005454979371279478\n",
      "[step: 19974] loss: 0.005971265025436878\n",
      "[step: 19975] loss: 0.004795142915099859\n",
      "[step: 19976] loss: 0.005554057192057371\n",
      "[step: 19977] loss: 0.00487099215388298\n",
      "[step: 19978] loss: 0.00650510611012578\n",
      "[step: 19979] loss: 0.009344835765659809\n",
      "[step: 19980] loss: 0.009586823172867298\n",
      "[step: 19981] loss: 0.007381971925497055\n",
      "[step: 19982] loss: 0.009861075319349766\n",
      "[step: 19983] loss: 0.007253954187035561\n",
      "[step: 19984] loss: 0.006468906998634338\n",
      "[step: 19985] loss: 0.008031651377677917\n",
      "[step: 19986] loss: 0.0054830461740493774\n",
      "[step: 19987] loss: 0.007330548949539661\n",
      "[step: 19988] loss: 0.00648375554010272\n",
      "[step: 19989] loss: 0.008529657498002052\n",
      "[step: 19990] loss: 0.007535185664892197\n",
      "[step: 19991] loss: 0.005869768094271421\n",
      "[step: 19992] loss: 0.006636377424001694\n",
      "[step: 19993] loss: 0.00626907916739583\n",
      "[step: 19994] loss: 0.005857647396624088\n",
      "[step: 19995] loss: 0.006514041684567928\n",
      "[step: 19996] loss: 0.0050400132313370705\n",
      "[step: 19997] loss: 0.005684230476617813\n",
      "[step: 19998] loss: 0.004868058022111654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 19999] loss: 0.005205965600907803\n",
      "RMSE: 0.18516530096530914\n",
      "Model saved in path: /tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "#     saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "    \n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "    \n",
    "    save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIQAAAHVCAYAAACAOCDDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOWOLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x262abdfe6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "# Plot predictions\n",
    "plt.plot(testY, label='testY')\n",
    "plt.plot(test_predict, label='predict')\n",
    "plt.legend()\n",
    "#plt.savefig('./plot/rnn_3000_result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_testY = scalerY.inverse_transform(testY)\n",
    "inv_test_predict = scalerY.inverse_transform(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 131114.9885780877\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKAAAAHVCAYAAADGjoNpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRESkqhRAiYiIiIiIiIhIVf0P6sqpGY/zZ1sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x262a9cf7550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rmse_test = np.sqrt(mean_squared_error(inv_testY, inv_test_predict))\n",
    "\n",
    "print('RMSE: {}'.format(rmse_test))\n",
    "\n",
    "slices = slice(60, 100)\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(inv_testY[slices], label='testY')\n",
    "plt.plot(inv_test_predict[slices], label='predict')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
